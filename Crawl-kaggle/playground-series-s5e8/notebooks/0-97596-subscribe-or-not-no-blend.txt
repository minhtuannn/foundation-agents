Bank Scheme: Subscribe or Not?
If you appreciate the notebook, kindly remember to upvote it. The sole impetus for creating such notebooks is the appreciation of the effort expended by others.
Importing Libraries
In [1]:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

sns.set_theme(style="whitegrid", palette="husl")
plt.rcParams['figure.facecolor'] = 'white'
plt.rcParams['axes.grid'] = True
plt.rcParams['grid.alpha'] = 0.3
Importing Data
In [2]:
train = pd.read_csv("/kaggle/input/playground-series-s5e8/train.csv", index_col='id')
test = pd.read_csv("/kaggle/input/playground-series-s5e8/test.csv", index_col='id')
In [3]:
orig = pd.read_csv("/kaggle/input/bank-marketing-dataset-full/bank-full.csv", sep=';')
orig['y'] = orig['y'].map({'no': 0, 'yes': 1})
In [4]:
train = pd.concat([train, orig], ignore_index=True)
train = train.drop_duplicates()
In [5]:
train.head()
Out[5]:
age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y
0 42 technician married secondary no 7 no no cellular 25 aug 117 3 -1 0 unknown 0
1 38 blue-collar married secondary no 514 no no unknown 18 jun 185 1 -1 0 unknown 0
2 36 blue-collar married secondary no 602 yes no unknown 14 may 111 2 -1 0 unknown 0
3 27 student single secondary no 34 yes no unknown 28 may 10 2 -1 0 unknown 0
4 26 technician married secondary no 889 yes no cellular 3 feb 902 1 -1 0 unknown 1
In [6]:
train.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 795211 entries, 0 to 795210
Data columns (total 17 columns):
 #   Column     Non-Null Count   Dtype 
---  ------     --------------   ----- 
 0   age        795211 non-null  int64 
 1   job        795211 non-null  object
 2   marital    795211 non-null  object
 3   education  795211 non-null  object
 4   default    795211 non-null  object
 5   balance    795211 non-null  int64 
 6   housing    795211 non-null  object
 7   loan       795211 non-null  object
 8   contact    795211 non-null  object
 9   day        795211 non-null  int64 
 10  month      795211 non-null  object
 11  duration   795211 non-null  int64 
 12  campaign   795211 non-null  int64 
 13  pdays      795211 non-null  int64 
 14  previous   795211 non-null  int64 
 15  poutcome   795211 non-null  object
 16  y          795211 non-null  int64 
dtypes: int64(8), object(9)
memory usage: 103.1+ MB
Exploratory Data Analysis
In [7]:
def custom_describe(df):
    df_ = df.select_dtypes(include=np.number)
    des = df_.describe().T
    des['skewness'] = df_.skew()
    des['kurtosis'] = df_.kurtosis()
    des['count'] = des['count'].astype('int')
    return des
In [8]:
features = test.columns.tolist()
print(features)
['age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome']
In [9]:
numerical_features = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']
categorical_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'day', 'month', 'poutcome']
target = 'y'
Target Analysis
In [10]:
fig, axes = plt.subplots(1, 2, figsize=(14, 6))
sns.countplot(x=target, data=train, ax=axes[0])
axes[0].set_title('Distribution of Target Variable (Subscribed)', fontweight='bold', size=20)
axes[0].set_xticks(ticks=[0, 1],labels=['No', 'Yes'])
axes[0].set_xlabel("subscribed")

train[target].value_counts().plot(kind='pie', ax=axes[1], explode=(0.0, 0.1), autopct="%.2f%%", labels=['No', 'Yes'], pctdistance=0.75)
axes[1].add_artist(plt.Circle((0, 0), 0.5, fc='w'))
axes[1].set_title('Pie Chart of Target Variable', fontweight='bold', size=20)
axes[1].set_ylabel("")

plt.tight_layout()
plt.show()
Numerical Feature Analysis
In [11]:
custom_describe(train[numerical_features])
Out[11]:
count mean std min 25% 50% 75% max skewness kurtosis
age 795211 40.926953 10.129098 18.0 33.0 39.0 48.0 95.0 0.592744 -0.041077
balance 795211 1213.061980 2848.603881 -8019.0 0.0 624.0 1390.0 102127.0 12.034149 259.577376
duration 795211 256.339096 271.723766 0.0 91.0 135.0 347.5 4918.0 2.102151 6.978781
campaign 795211 2.587630 2.741838 1.0 1.0 2.0 3.0 63.0 4.827106 37.859523
pdays 795211 23.423889 78.901465 -1.0 -1.0 -1.0 -1.0 871.0 3.545487 13.020497
previous 795211 0.314566 1.410369 0.0 0.0 0.0 0.0 275.0 21.442389 2453.485828
In [12]:
def numerical_features_plot(df, feature, target):
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    fig.suptitle(f'Analysis of {feature}', fontsize=16, fontweight='bold')

    # Boxplot
    sns.boxplot(data=df, x=feature, y=target, hue=target, orient='h', ax=axes[0])
    axes[0].set_title(f'Boxplot of {feature}')
    axes[0].legend_.remove()  # Turn off legend

    # Violinplot
    sns.violinplot(data=df, x=feature, y=target, hue=target, orient='h', ax=axes[1])
    axes[1].set_title(f'Violinplot of {feature}')
    axes[1].legend_.remove()  # Turn off legend

    # Histogram with KDE
    sns.histplot(data=df, x=feature, hue=target, kde=True, ax=axes[2], alpha=0.6)
    axes[2].set_title(f'Distribution of {feature}')

    plt.tight_layout(rect=[0, 0, 1, 0.98])
    plt.show()
In [13]:
for feature in numerical_features:
    numerical_features_plot(train, feature, target)
Categorical Feature Analysis
In [14]:
def categorical_features_plot(df, feature):
    value_counts = df[feature].value_counts()
    
    top_n = min(10, len(value_counts))
    top_categories = value_counts.nlargest(top_n)
    
    df_plot = df[df[feature].isin(top_categories.index)]

    top_percentages = (top_categories / len(df)) * 100

    plt.figure(figsize=(25, 6))

    plt.subplot(1, 2, 1)
    sns.countplot(df_plot, x=feature, palette=sns.color_palette('viridis'))
    plt.title(f"Count Plot of{(' Top ' + str(top_n)) if len(value_counts) > 10 else ''} Categories of {feature}", size=16, fontweight='bold')

    plt.subplot(1, 2, 2)
    plt.pie(
        top_percentages,
        labels=top_percentages.index,
        autopct=lambda pct: f"{pct:.2f}%",
        pctdistance=0.75
    )
    plt.gca().add_artist(plt.Circle((0, 0), 0.5, fc='w'))  # Donut hole
    plt.title(
        f"{('Top ' + str(top_n)) if len(value_counts) > 10 else ''} {feature} Categories as % of Full Dataset",
        size=16,
        fontweight='bold'
    )
    plt.ylabel("")

    plt.tight_layout()
    plt.show()
In [15]:
for feature in categorical_features:
    if feature == 'day':
        continue
    categorical_features_plot(train, feature)
In [16]:
df = train.copy()

g = sns.FacetGrid(df[df['job'].isin(df['job'].value_counts().head(6).index)], 
                  col='job', col_wrap=3, height=4, aspect=1.2)
g.map_dataframe(sns.boxplot, x='y', y='age', palette='viridis')
g.set_titles("{col_name}")
g.fig.suptitle('Age Distribution by Job and Subscription Status', y=1.05)
plt.show()
In [17]:
def plot_categorical_heatmap(feature1, feature2):
    cross_tab = pd.crosstab(df[feature1], df[feature2], normalize='index') * 100
    plt.figure(figsize=(10, 6))
    sns.heatmap(cross_tab, annot=True, fmt='.1f', cmap='YlGnBu', linewidths=.5)
    plt.title(f'Percentage of Subscription by {feature1} and {feature2}')
    plt.ylabel(feature1)
    plt.xlabel(feature2)
    plt.show()
In [18]:
plot_categorical_heatmap('job', 'education')
In [19]:
plot_categorical_heatmap('marital', 'education')
In [20]:
plot_categorical_heatmap('poutcome', 'contact')
Model Training
In [21]:
def data_process(df): # Source: https://www.kaggle.com/code/haohuanchen/ps-s5e8-lightgb-model-a-simple-starter
    df = df.copy()
    
    def many_no(x):
        if x['default']=='no' and x['housing']=='no' and x['loan']=='no':
            return 21
        if x['default']=='no' and x['housing']=='no'\
        or x['default']=='no' and x['loan']=='no'\
        or x['housing']=='no' and x['loan']=='no':
            return 7
        if x['default']=='no' or x['housing']=='no' or x['loan']=='no':
            return 3
        return 0
    
    df['many_no']  = df.apply(lambda x: many_no(x), axis=1)
    
    return df
In [22]:
from sklearn.model_selection import train_test_split,StratifiedKFold

X = train.drop('y', axis=1)
y = train['y']

X_str = data_process(X).astype('str')
test_str = data_process(test).astype('str')
In [23]:
from catboost import CatBoostClassifier
from sklearn.metrics import roc_auc_score
from sklearn.base import clone

cat_clf = CatBoostClassifier(
    allow_writing_files=False,
    verbose=False,
    task_type='GPU',
    loss_function='CrossEntropy',
    use_best_model=True,
    cat_features=X_str.columns.to_list(),
    n_estimators=10000,
    learning_rate=0.1,
)

N_SPLITS = 5
skfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=0)
test_pred = np.zeros(len(test_str))
roc_scores = []

for fold, (train_idx, test_idx) in enumerate(skfold.split(X_str, y), 1):
    X_train, X_test = X_str.iloc[train_idx], X_str.iloc[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    
    model = clone(cat_clf)
    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=200, verbose=500)
    
    y_pred = model.predict_proba(X_test)[:, 1]
    roc_score = roc_auc_score(y_test, y_pred)
    roc_scores.append(roc_score)

    test_pred += model.predict_proba(test_str)[:, 1]
    print(f"Fold {fold} -> ROC-AUC: {roc_score:.5f}")

print(f"Average Fold ROC-AUC: {np.mean(roc_scores):.5f} \xb1 {np.std(roc_scores):.5f}")

test_pred = test_pred / N_SPLITS
0: learn: 0.5494372 test: 0.5482823 best: 0.5482823 (0) total: 2.83s remaining: 7h 51m 33s
500: learn: 0.1386466 test: 0.1369534 best: 0.1369527 (489) total: 31.4s remaining: 9m 55s
1000: learn: 0.1345384 test: 0.1356640 best: 0.1356618 (998) total: 59.9s remaining: 8m 58s
1500: learn: 0.1312745 test: 0.1351327 best: 0.1351300 (1489) total: 1m 28s remaining: 8m 21s
2000: learn: 0.1284355 test: 0.1349090 best: 0.1349014 (1982) total: 1m 56s remaining: 7m 47s
2500: learn: 0.1257106 test: 0.1347593 best: 0.1347581 (2497) total: 2m 25s remaining: 7m 17s
3000: learn: 0.1231534 test: 0.1346613 best: 0.1346402 (2912) total: 2m 54s remaining: 6m 48s
bestTest = 0.1346401676
bestIteration = 2912
Shrink model to first 2913 iterations.
Fold 1 -> ROC-AUC: 0.97228
0: learn: 0.5548725 test: 0.5537951 best: 0.5537951 (0) total: 70ms remaining: 11m 39s
500: learn: 0.1386541 test: 0.1368179 best: 0.1368179 (500) total: 29.2s remaining: 9m 12s
1000: learn: 0.1346209 test: 0.1356167 best: 0.1356167 (1000) total: 58.4s remaining: 8m 44s
1500: learn: 0.1314565 test: 0.1350563 best: 0.1350563 (1500) total: 1m 27s remaining: 8m 16s
2000: learn: 0.1285324 test: 0.1346477 best: 0.1346440 (1997) total: 1m 57s remaining: 7m 47s
2500: learn: 0.1258135 test: 0.1344952 best: 0.1344915 (2475) total: 2m 26s remaining: 7m 19s
3000: learn: 0.1232909 test: 0.1343806 best: 0.1343704 (2959) total: 2m 55s remaining: 6m 50s
bestTest = 0.1342972076
bestIteration = 3289
Shrink model to first 3290 iterations.
Fold 2 -> ROC-AUC: 0.97249
0: learn: 0.5283834 test: 0.5271977 best: 0.5271977 (0) total: 60.9ms remaining: 10m 8s
500: learn: 0.1383817 test: 0.1370632 best: 0.1370632 (500) total: 29.1s remaining: 9m 11s
1000: learn: 0.1344255 test: 0.1358173 best: 0.1358160 (999) total: 58.4s remaining: 8m 45s
1500: learn: 0.1312044 test: 0.1351822 best: 0.1351771 (1498) total: 1m 27s remaining: 8m 15s
2000: learn: 0.1283233 test: 0.1348903 best: 0.1348903 (2000) total: 1m 56s remaining: 7m 47s
2500: learn: 0.1257034 test: 0.1347089 best: 0.1347011 (2476) total: 2m 26s remaining: 7m 18s
3000: learn: 0.1232927 test: 0.1346088 best: 0.1346033 (2982) total: 2m 55s remaining: 6m 49s
3500: learn: 0.1208673 test: 0.1345185 best: 0.1345155 (3498) total: 3m 25s remaining: 6m 20s
bestTest = 0.1345139963
bestIteration = 3502
Shrink model to first 3503 iterations.
Fold 3 -> ROC-AUC: 0.97228
0: learn: 0.5308997 test: 0.5305820 best: 0.5305820 (0) total: 61ms remaining: 10m 9s
500: learn: 0.1381515 test: 0.1382827 best: 0.1382827 (500) total: 29s remaining: 9m 10s
1000: learn: 0.1341448 test: 0.1369839 best: 0.1369839 (1000) total: 58.7s remaining: 8m 47s
1500: learn: 0.1310263 test: 0.1365436 best: 0.1365411 (1495) total: 1m 27s remaining: 8m 17s
2000: learn: 0.1282001 test: 0.1362742 best: 0.1362720 (1995) total: 1m 57s remaining: 7m 48s
2500: learn: 0.1255490 test: 0.1361527 best: 0.1361281 (2389) total: 2m 26s remaining: 7m 19s
bestTest = 0.136128055
bestIteration = 2389
Shrink model to first 2390 iterations.
Fold 4 -> ROC-AUC: 0.97177
0: learn: 0.5489634 test: 0.5478096 best: 0.5478096 (0) total: 70.2ms remaining: 11m 42s
500: learn: 0.1382948 test: 0.1379695 best: 0.1379695 (500) total: 29.4s remaining: 9m 17s
1000: learn: 0.1344486 test: 0.1369118 best: 0.1369118 (1000) total: 58.7s remaining: 8m 47s
1500: learn: 0.1311745 test: 0.1364285 best: 0.1364157 (1491) total: 1m 27s remaining: 8m 16s
2000: learn: 0.1282604 test: 0.1361723 best: 0.1361698 (1999) total: 1m 56s remaining: 7m 47s
bestTest = 0.136152874
bestIteration = 2174
Shrink model to first 2175 iterations.
Fold 5 -> ROC-AUC: 0.97136
Average Fold ROC-AUC: 0.97203 Â± 0.00041
Submission
In [24]:
sub = pd.read_csv("/kaggle/input/playground-series-s5e8/sample_submission.csv")
sub['y'] = test_pred
sub.to_csv("submission.csv", index=False)
sub.head()
Out[24]:
id y
0 750000 0.006904
1 750001 0.192296
2 750002 0.000358
3 750003 0.000105
4 750004 0.016439