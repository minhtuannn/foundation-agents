unfold_moreShow hidden code
Out[1]:
üè¶ Introduction
In today‚Äôs data-driven financial world, understanding a borrower‚Äôs credit behavior is crucial for reducing lending risk. This dataset captures the complete financial snapshot of thousands of loan applicants ‚Äî from their income and credit scores to employment history and loan purpose ‚Äî along with the final outcome of whether they successfully repaid their loans.
With nearly 600,000 records and a blend of demographic, financial, and behavioral features, this dataset provides a perfect ground to explore credit risk modeling, customer profiling, and loan default prediction. The goal is to leverage these insights to build an intelligent system that can predict loan repayment likelihood and help financial institutions make smarter, data-backed lending decisions. </div
Import Libraries
In [2]:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score


import warnings
warnings.filterwarnings('ignore')
Load The Data
In [3]:
df_train = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')
df_test = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')
Quick Overview Of The Data
In [4]:
df_train.head()
Out[4]:
id annual_income debt_to_income_ratio credit_score loan_amount interest_rate gender marital_status education_level employment_status loan_purpose grade_subgrade loan_paid_back
0 0 29367.99 0.084 736 2528.42 13.67 Female Single High School Self-employed Other C3 1.0
1 1 22108.02 0.166 636 4593.10 12.92 Male Married Master's Employed Debt consolidation D3 0.0
2 2 49566.20 0.097 694 17005.15 9.76 Male Single High School Employed Debt consolidation C5 1.0
3 3 46858.25 0.065 533 4682.48 16.10 Female Single High School Employed Debt consolidation F1 1.0
4 4 25496.70 0.053 665 12184.43 10.21 Male Married High School Employed Other D1 1.0
Rows & Columns In Dataset
In [5]:
print('Train Shape',df_train.shape)
print('Test Shape',df_test.shape)
Train Shape (593994, 13)
Test Shape (254569, 12)
Checking Columns Names & Data Types
In [6]:
df_train.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 593994 entries, 0 to 593993
Data columns (total 13 columns):
 #   Column                Non-Null Count   Dtype  
---  ------                --------------   -----  
 0   id                    593994 non-null  int64  
 1   annual_income         593994 non-null  float64
 2   debt_to_income_ratio  593994 non-null  float64
 3   credit_score          593994 non-null  int64  
 4   loan_amount           593994 non-null  float64
 5   interest_rate         593994 non-null  float64
 6   gender                593994 non-null  object 
 7   marital_status        593994 non-null  object 
 8   education_level       593994 non-null  object 
 9   employment_status     593994 non-null  object 
 10  loan_purpose          593994 non-null  object 
 11  grade_subgrade        593994 non-null  object 
 12  loan_paid_back        593994 non-null  float64
dtypes: float64(5), int64(2), object(6)
memory usage: 58.9+ MB
Descriptive Statistics
In [7]:
df_train.describe()
Out[7]:
id annual_income debt_to_income_ratio credit_score loan_amount interest_rate loan_paid_back
count 593994.000000 593994.000000 593994.000000 593994.000000 593994.000000 593994.000000 593994.000000
mean 296996.500000 48212.202976 0.120696 680.916009 15020.297629 12.356345 0.798820
std 171471.442235 26711.942078 0.068573 55.424956 6926.530568 2.008959 0.400883
min 0.000000 6002.430000 0.011000 395.000000 500.090000 3.200000 0.000000
25% 148498.250000 27934.400000 0.072000 646.000000 10279.620000 10.990000 1.000000
50% 296996.500000 46557.680000 0.096000 682.000000 15000.220000 12.370000 1.000000
75% 445494.750000 60981.320000 0.156000 719.000000 18858.580000 13.680000 1.000000
max 593993.000000 393381.740000 0.627000 849.000000 48959.950000 20.990000 1.000000
Data Visualization
Loan Paid Back(Target Column) Distribution
In [9]:
plt.pie(df_train['loan_paid_back'].value_counts(),labels=df_train['loan_paid_back'].value_counts().index,autopct='%1.1f%%',colors=['#5C4033'])
plt.title('Loan Paid Back Distribution')
plt.show()
Numeric Columns Distribution
In [10]:
num_cols = df_train.select_dtypes(include=np.number).columns

plt.figure(figsize=(10,8))
for col in num_cols:
  plt.subplot(2,4,num_cols.get_loc(col)+1)
  sns.histplot(df_train[col], color='#5C4033')
plt.tight_layout()
plt.show()
Categorical Columns Distribution
In [11]:
cat_cols = ['gender','marital_status','education_level','employment_status','loan_purpose']
plt.figure(figsize=(10,8))
for col in cat_cols:
  plt.subplot(2,3,cat_cols.index(col)+1)
  plt.pie(df_train[col].value_counts(),labels=df_train[col].value_counts().index,autopct='%1.1f%%',colors=['#4B2E05','#C4A484','#FFF8F0','#F5DEB3'])
  plt.title(col)
plt.tight_layout()
plt.show()
Gender Distribution
In [12]:
sns.countplot(x='gender', hue='loan_paid_back' , data=df_train, color='#5C4033')
plt.title('Gender Distribution')
plt.show()
Annual Income vs Loan Amount
In [13]:
plt.figure(figsize=(8,5))
sns.scatterplot(data=df_train, x='annual_income', y='loan_amount', hue='interest_rate', color='#5C4033')
plt.title("Annual Income vs Loan Amount (colored by Interest Rate)")
plt.xlabel("Annual Income")
plt.ylabel("Loan Amount")
plt.show()
Credit Score vs Interest Rate
In [14]:
plt.figure(figsize=(8,5))
sns.scatterplot(data=df_train, x='credit_score', y='interest_rate', color='#5C4033')
plt.title("Credit Score vs Interest Rate")
plt.xlabel("Credit Score")
plt.ylabel("Interest Rate (%)")
plt.show()
Debt-to-Income Ratio vs Interest Rate
In [15]:
plt.figure(figsize=(8,5))
sns.scatterplot(data=df_train, x='debt_to_income_ratio', y='interest_rate', color='#5C4033')
plt.title("Debt-to-Income Ratio vs Interest Rate")
plt.xlabel("Debt-to-Income Ratio")
plt.ylabel("Interest Rate (%)")
plt.show()
Correlation Heatmap
In [16]:
from matplotlib.colors import LinearSegmentedColormap
brown_cmap = LinearSegmentedColormap.from_list(
    "brown_cmap", ["#FFF8F0", "#F5DEB3", "#C4A484", "#5C4033"]
)

plt.figure(figsize=(8,5))
sns.heatmap(df_train.corr(numeric_only=True), annot=True, cmap=brown_cmap, fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()
Feture Engineering
In [17]:
df_train = pd.get_dummies(df_train, columns=['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose'], drop_first=True,dtype=int)
df_test = pd.get_dummies(df_test, columns=['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose'], drop_first=True,dtype=int)
In [18]:
def feature_engineer(df):
    df = df.copy()

    # --- grade & subgrade
    def extract_grade_subgrade(s):
        if pd.isna(s): return (np.nan, np.nan)
        s = str(s).strip()
        if len(s)==0: return (np.nan, np.nan)
        return (s[0], s[1:])

    g = df['grade_subgrade'].fillna("").astype(str).apply(extract_grade_subgrade)
    df['grade'] = g.apply(lambda x: x[0]).replace("", np.nan)
    df['subgrade'] = pd.to_numeric(g.apply(lambda x: x[1]), errors='coerce')

    # --- numeric
    num_cols = ['annual_income','loan_amount','debt_to_income_ratio','interest_rate','credit_score']
    for c in num_cols:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors='coerce')

    # --- interactions
    df['loan_to_income'] = df['loan_amount'] / df['annual_income'].replace(0, np.nan)
    df['income_to_loan'] = df['annual_income'] / df['loan_amount'].replace(0, np.nan)
    df['log_annual_income'] = np.log1p(df['annual_income'].clip(lower=0))
    df['log_loan_amount'] = np.log1p(df['loan_amount'].clip(lower=0))
    df['interest_x_loan'] = df['interest_rate'] * df['loan_amount']
    df['interest_x_credit'] = df['interest_rate'] * df['credit_score']
    df['dti_x_interest'] = df['debt_to_income_ratio'] * df['interest_rate']

     # --- credit bucket
    if 'credit_score' in df.columns:
        df['credit_score_bucket'] = pd.cut(
            df['credit_score'],
            bins=[0,580,670,740,800,900],
            labels=['poor','fair','good','very_good','excellent']
        ).astype(object)

    # --- freq encoding
    cat_cols = ['loan_purpose','employment_status','education_level','marital_status','gender']
    for c in cat_cols:
        if c in df.columns:
            freq = df[c].fillna('NA').value_counts(normalize=True)
            df[f'{c}_freq'] = df[c].fillna('NA').map(freq).astype(float)

    df['missing_count'] = df.isna().sum(axis=1)

    # --- grade one-hot
    df['grade'] = df['grade'].astype(object)
    grade_dummies = pd.get_dummies(df['grade'], prefix='grade', dummy_na=True)
    df = pd.concat([df, grade_dummies], axis=1)

    return df
In [19]:
train_fe = feature_engineer(df_train)
test_fe  = feature_engineer(df_test)
target = 'loan_paid_back'
FEATURES = [c for c in train_fe.columns if c not in ['id', target, 'grade_subgrade']]

train_X = train_fe[FEATURES].copy()
train_y = train_fe[target].copy()
test_X  = test_fe[FEATURES].copy()
In [20]:
# Fill missing values
for c in train_X.columns:
    if train_X[c].dtype.kind in 'biufc':  # numeric
        med = train_X[c].median()
        train_X[c] = train_X[c].fillna(med)
        test_X[c]  = test_X[c].fillna(med)
    else:
        train_X[c] = train_X[c].fillna('NA')
        test_X[c]  = test_X[c].fillna('NA')

# Convert object columns to categorical codes
for c in train_X.columns:
    if train_X[c].dtype == 'object':
        train_X[c] = train_X[c].astype('category').cat.codes
        test_X[c]  = test_X[c].astype('category').cat.codes
Optuna Objective
In [21]:
def objective(trial):
    param = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'verbosity': -1,
        'seed': 42,
        'n_jobs': -1,
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),
        'num_leaves': trial.suggest_int('num_leaves', 30, 300),
        'max_depth': trial.suggest_int('max_depth', 4, 12),
        'min_child_samples': trial.suggest_int('min_child_samples', 10, 200),
        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),
        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),
        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),
    }

    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    aucs = []

    for tr_idx, val_idx in folds.split(train_X, train_y):
        X_tr, X_val = train_X.iloc[tr_idx], train_X.iloc[val_idx]
        y_tr, y_val = train_y.iloc[tr_idx], train_y.iloc[val_idx]


        dtrain = lgb.Dataset(X_tr, label=y_tr)
        dval   = lgb.Dataset(X_val, label=y_val, reference=dtrain)

        bst = lgb.train(
            param,
            dtrain,
            valid_sets=[dval],
            num_boost_round=5000,
            callbacks=[
                lgb.early_stopping(100),
                lgb.log_evaluation(period=0)
            ]
        )

        pred = bst.predict(X_val, num_iteration=bst.best_iteration)
        aucs.append(roc_auc_score(y_val, pred))

    return np.mean(aucs)
Run Optuna Optimization
In [22]:
import optuna
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=4, show_progress_bar=True)

print("Best AUC:", study.best_value)
print("Best params:", study.best_params)
[I 2025-11-14 16:37:56,366] A new study created in memory with name: no-name-53b22e74-c523-465f-a8e3-5558383dc3f6
Best‚Äátrial:‚Äá1.‚ÄáBest‚Äávalue:‚Äá0.922646:‚Äá100%
4/4‚Äá[14:09<00:00,‚Äá213.34s/it]
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[513] valid_0's auc: 0.922845
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[596] valid_0's auc: 0.922271
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[656] valid_0's auc: 0.921121
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[501] valid_0's auc: 0.921948
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[563] valid_0's auc: 0.921221
[I 2025-11-14 16:40:40,491] Trial 0 finished with value: 0.9218811595541426 and parameters: {'learning_rate': 0.061608776841792764, 'num_leaves': 263, 'max_depth': 7, 'min_child_samples': 62, 'subsample': 0.9022544216831023, 'colsample_bytree': 0.7724412221549775, 'reg_alpha': 0.042028194172385604, 'reg_lambda': 0.0014021344354067807}. Best is trial 0 with value: 0.9218811595541426.
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[849] valid_0's auc: 0.923534
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[773] valid_0's auc: 0.923133
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[1029] valid_0's auc: 0.921883
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[876] valid_0's auc: 0.922639
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[786] valid_0's auc: 0.922039
[I 2025-11-14 16:43:50,729] Trial 1 finished with value: 0.9226456670001962 and parameters: {'learning_rate': 0.08457397540654793, 'num_leaves': 136, 'max_depth': 5, 'min_child_samples': 10, 'subsample': 0.7837044037748188, 'colsample_bytree': 0.6642040308973245, 'reg_alpha': 0.002650505600804862, 'reg_lambda': 0.47331690141233634}. Best is trial 1 with value: 0.9226456670001962.
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[1211] valid_0's auc: 0.923001
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[1357] valid_0's auc: 0.922842
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[1391] valid_0's auc: 0.92153
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[1252] valid_0's auc: 0.922263
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[1144] valid_0's auc: 0.921304
[I 2025-11-14 16:49:37,471] Trial 2 finished with value: 0.9221879041686458 and parameters: {'learning_rate': 0.029737290156702276, 'num_leaves': 122, 'max_depth': 7, 'min_child_samples': 33, 'subsample': 0.6885101858334616, 'colsample_bytree': 0.6399372872421483, 'reg_alpha': 0.037411682804172744, 'reg_lambda': 0.009577971326032829}. Best is trial 1 with value: 0.9226456670001962.
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[576] valid_0's auc: 0.923432
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[728] valid_0's auc: 0.923069
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[495] valid_0's auc: 0.921769
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[525] valid_0's auc: 0.922432
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[518] valid_0's auc: 0.922055
[I 2025-11-14 16:52:05,633] Trial 3 finished with value: 0.9225511911654302 and parameters: {'learning_rate': 0.07634737275436315, 'num_leaves': 49, 'max_depth': 9, 'min_child_samples': 91, 'subsample': 0.7631554948993723, 'colsample_bytree': 0.6429169648088917, 'reg_alpha': 2.9222215914959326, 'reg_lambda': 0.017682388794196313}. Best is trial 1 with value: 0.9226456670001962.
Best AUC: 0.9226456670001962
Best params: {'learning_rate': 0.08457397540654793, 'num_leaves': 136, 'max_depth': 5, 'min_child_samples': 10, 'subsample': 0.7837044037748188, 'colsample_bytree': 0.6642040308973245, 'reg_alpha': 0.002650505600804862, 'reg_lambda': 0.47331690141233634}
Train Final Model
In [23]:
best_params = {
    **study.best_params,
    'objective':'binary',
    'metric':'auc',
    'boosting_type':'gbdt',
    'verbosity':-1,
    'seed':42,
    'n_jobs':-1
}

dtrain = lgb.Dataset(train_X, label=train_y)

final_model = lgb.train(
    best_params,
    dtrain,
    num_boost_round=100,
    valid_sets=[dtrain],
    callbacks=[
        lgb.early_stopping(5),
        lgb.log_evaluation(period=0)
    ]
)

print(" Done Training")
 Done Training
Submission
In [24]:
test_X = test_fe[FEATURES].copy()

# Fill missing values (same logic as training)
for c in train_X.columns:
    if train_X[c].dtype.kind in 'biufc':  # numeric
        med = train_X[c].median()
        test_X[c] = test_X[c].fillna(med)
    else:
        test_X[c] = test_X[c].fillna('NA')

# Convert any remaining object columns to category codes
for c in test_X.columns:
    if test_X[c].dtype == 'object':
        test_X[c] = test_X[c].astype('category').cat.codes

# --- Predict using your trained model ---
y_pred = final_model.predict(test_X, num_iteration=final_model.best_iteration)

# If the target is binary (0/1), convert probabilities to classes
submission = df_test[['id']].copy()
submission['loan_paid_back'] = (y_pred > 0.5).astype(int)

# --- Save to CSV for Kaggle submission ---
submission.to_csv('submission.csv', index=False)

print("submission.csv file is ready!")
submission.head()
submission.csv file is ready!
Out[24]:
id loan_paid_back
0 593994 1
1 593995 1
2 593996 0
3 593997 1
4 593998 1
In [ ]:
 