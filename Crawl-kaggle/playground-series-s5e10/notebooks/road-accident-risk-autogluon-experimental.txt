Predicting Road Accident Risk
On Version 12, we added external data (OOF) from other Notebook. a Credit were given to :
Notebook Author Public Leaderboard
S5E10 | Single TabM - Tuned Masaya Kawamata 0.05545
XGB Boosting Over Residuals - [CV 0.05595] Chris Deotte 0.05548
S5E10 | XGB +OrigCol - 20seeds Masaya Kawamata 0.05549
PG S5E10 - RealMLP - [CV 0.055936 LB 0.05549] Mahog 0.05549
Feature-Rich Single XGB | CV = 0.05594 Meta Models 0.05547
S5E10 | TabM over Residuals Masaya Kawamata 0.05544
Road Risk | Single YDF Mikhail Naumov 0.05541
S5E10 | NN Stacking - Baseline Masaya Kawamata 0.05541
üöë [Accident Prediction]üö¶- Stacking Model üç∞ Aliffa Agnur 0.05540
üîé TL;DR
Version 1: AutoGluon best quality 11H Bayes Optimal model only uses catboost with various hyperparameter variations. Best model = 0.055946, Public Leaderboard = 0.05545
Version 3: AutoGluon best quality 11H Bayes Optimal model XGBoost only with various different hyperparameter. Best Models = 0.055953, Public Leaderboard = 0.05547
Version 7: AutoGluon (Predict Residual) best quality 11H Bayes Optimal Model. Catboost only with Various Hyperparameter. Best Models = ? , Public Leaderboard = 0.05545
Version 8: AutoGluon GPU 5H 50Mins (Predict Residual) best quality Bayes Optimal Model. TabM only with Various Hyperparameter. Best Models = 0.055979 , Public Leaderboard = 0.0550
Version 10: AutoGluon CPU 10H Best Quality. used AutoGluon as Features. Best Models = 0.055979, Public Leaderboard = 0.05550
Version 11: AutoGluon CPU 10H Best Quality. used 1 AutoGluon OOF as features with the best lb score (0.05543). Best Models = 0.055790, Public Leaderboard = 0.05547
Version 12: AutoGluon CPU 11H Best Quality. used 1 AutoGluon OOF (best oof) + Use 7+ Other OOF from other notebook. Best Models = 0.055823 , Public Leaderboard = 0.05541
Version 14: AutoGluon CPU 11H Best Quality. same as version 12, but we add YDF + NN_stacking from other notebook. Using CatBoost Only. Best Models = 0.055865, Public Leaderboard = 0.05539
Version 16: AutoGluon CPU 11H (Actual only 1Hour) Best Quality. same as version 14, but LightGBM only with various Hyperparameter. Best Models = 0.55868, Public Leaderboard = 0.05540
Version 21: AutoGluon CPU 11H Best Quality. no model specific (use all different model) and 7 Other OOF. Best Models = 0.055821, Public Leaderboard = 0.05540, Hold-out 20% = 0.055851
Version 22: AutoGluon GPU 11H Best Quality. same as version 14 (catboost only), but this time using GPU. Best Models = 0.055871, Public Leaderboard = 0.05540
Version 23: AutoGluon GPU 11H Best Quality. catboost only. using 8 OOF (Tabm Residual,RealMlp,XGB Residual, Single TabM, XGB*20, XGB Rich, NN Stacking, Single YDF). Best Models = ?, Public Leaderboard = ?
Load Data
In [1]:
%%capture
!pip install autogluon.tabular scikit-learn==1.5.2
#!pip install "autogluon.tabular[tabpfn]==1.4.0"
In [2]:
import os
import scipy
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from autogluon.tabular import TabularPredictor
from sklearn.metrics import mean_squared_error, r2_score

import warnings
warnings.filterwarnings('ignore')
In [3]:
# LOAD DATA

train_df = pd.read_csv(r'/kaggle/input/playground-series-s5e10/train.csv').drop(columns = 'id')
test_df  = pd.read_csv(r'/kaggle/input/playground-series-s5e10/test.csv').drop(columns = 'id')

print(f'Train : {train_df.shape[0]}')
print(f'Test  : {test_df.shape[0]}')
Train : 517754
Test  : 172585
unfold_moreShow hidden code
Success Load Tabm Residual!
Success Load RealMLP!
Success Load XGB Residual!
Success Load Single Tabm!
Success Load XGB * 20 Seed!
Success Load XGB Rich Features!
Success Load NN Stacking!
Success Load Single YDF!
Out[4]:
((517754, 8), (172585, 8))
In [ ]:
 
 In [5]:
# MERGING TRAIN DATA WITH EXTERNAL DATA

merge_train = pd.concat((train_df, external_oof), axis = 1)
merge_test = pd.concat((test_df, external_test), axis = 1)

display(merge_train)
display(merge_test)
road_type num_lanes curvature speed_limit lighting weather road_signs_present public_road time_of_day holiday ... num_reported_accidents accident_risk tabm_residual realmlp xgb_residual single_tabm xgb_diff_seed20 xgb_rich nn_stacking single_ydf
0 urban 2 0.06 35 daylight rainy False True afternoon False ... 1 0.13 0.130670 0.128773 0.126662 0.128814 0.128963 0.129418 0.128434 0.130264
1 urban 4 0.99 35 daylight clear True False evening True ... 0 0.35 0.325544 0.327026 0.326985 0.323260 0.322940 0.322881 0.324563 0.325790
2 rural 4 0.63 70 dim clear False True morning True ... 2 0.30 0.388800 0.385355 0.389054 0.382161 0.387817 0.387941 0.386533 0.386329
3 highway 4 0.07 35 dim rainy True True morning False ... 1 0.21 0.134281 0.133874 0.128482 0.132775 0.129615 0.129980 0.132626 0.131700
4 rural 1 0.58 60 daylight foggy False False evening True ... 1 0.56 0.472425 0.472611 0.470489 0.471251 0.469951 0.471372 0.471103 0.468466
... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ...
517749 highway 4 0.10 70 daylight foggy True True afternoon False ... 2 0.32 0.316405 0.323327 0.320074 0.317979 0.320555 0.320493 0.319734 0.321161
517750 rural 4 0.47 35 daylight rainy True True morning False ... 1 0.26 0.240590 0.240129 0.240548 0.236662 0.239382 0.240492 0.239043 0.242210
517751 urban 4 0.62 25 daylight foggy False False afternoon False ... 0 0.19 0.294203 0.295854 0.292329 0.296087 0.293422 0.294688 0.294786 0.293608
517752 highway 3 0.63 25 night clear True False afternoon True ... 3 0.51 0.497290 0.495361 0.492313 0.495484 0.493777 0.492699 0.495922 0.496974
517753 highway 2 0.31 45 dim rainy False True afternoon True ... 2 0.22 0.189941 0.191042 0.192058 0.192340 0.191005 0.192192 0.189296 0.191728
517754 rows √ó 21 columns
road_type num_lanes curvature speed_limit lighting weather road_signs_present public_road time_of_day holiday school_season num_reported_accidents tabm_residual realmlp xgb_residual single_tabm xgb_diff_seed20 xgb_rich nn_stacking single_ydf
0 highway 2 0.34 45 night clear True True afternoon True True 1 0.295337 0.295693 0.294914 0.296001 0.296001 0.295182 0.295168 0.296523
1 urban 3 0.04 45 dim foggy True False afternoon True False 0 0.120559 0.118715 0.120085 0.117532 0.117532 0.121481 0.119272 0.123731
2 urban 2 0.59 35 dim clear True False afternoon True True 1 0.185113 0.178979 0.180709 0.180968 0.180968 0.182011 0.181912 0.182780
3 rural 4 0.95 35 daylight rainy False False afternoon False False 2 0.307005 0.303249 0.307187 0.309812 0.309812 0.315057 0.310136 0.306962
4 highway 2 0.86 35 daylight clear True False evening False True 3 0.396242 0.392223 0.396670 0.396790 0.396790 0.396305 0.396914 0.400744
... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ...
172580 rural 2 0.01 45 dim rainy False False afternoon True True 2 0.105337 0.103375 0.104611 0.106372 0.106372 0.101307 0.103797 0.102585
172581 rural 1 0.74 70 daylight foggy False True afternoon False False 2 0.517383 0.517045 0.517941 0.518917 0.518917 0.517864 0.518025 0.518740
172582 urban 2 0.14 70 dim clear False False evening True True 1 0.251603 0.248754 0.249541 0.249645 0.249645 0.249413 0.249822 0.250564
172583 urban 1 0.09 45 daylight foggy True True morning False True 0 0.127359 0.124852 0.125785 0.124998 0.124998 0.126752 0.124878 0.127078
172584 highway 1 0.63 35 night foggy True False evening False False 0 0.488765 0.485463 0.485016 0.488204 0.488204 0.485559 0.487610 0.486701
172585 rows √ó 20 columns
In [6]:
# CHECK TARGET DISTRIBUTION

sns.histplot(train_df['accident_risk'], kde = True, color = 'orange', bins = 50)
Out[6]:
<Axes: xlabel='accident_risk', ylabel='Count'>
In [7]:
# CHECK CORRELATION BETWEEN OOF

# GET ALL OOF AUTOGLUON COLUMNS
autogluon_features = merge_train.iloc[:, 13:].columns

# HEATMAP CORRELATION
plt.figure(figsize = (10, 8))
corr_matrix = merge_train[autogluon_features].corr(method = 'spearman')
sns.heatmap(corr_matrix, annot = False, cmap = 'coolwarm', fmt = '.2f')
Out[7]:
<Axes: >
Feature Engineering
üîé Feature Engineering :
This feature engineering method is inspired by the Bayesian optimal model discussed by @siuketin(Broccoli beef) .Link discussion
In [8]:
# FEATURE ENGINEERING 

def f(X):
    return \
    0.3 * X["curvature"] + \
    0.2 * (X["lighting"] == "night").astype(int) + \
    0.1 * (X["weather"] != "clear").astype(int) + \
    0.2 * (X["speed_limit"] >= 60).astype(int) + \
    0.1 * (X["num_reported_accidents"] > 2).astype(int)

def clip(f):
    def clip_f(X):
        sigma = 0.05
        mu = f(X)
        a, b = -mu/sigma, (1-mu)/sigma
        Phi_a, Phi_b = scipy.stats.norm.cdf(a), scipy.stats.norm.cdf(b)
        phi_a, phi_b = scipy.stats.norm.pdf(a), scipy.stats.norm.pdf(b)
        return mu*(Phi_b-Phi_a)+sigma*(phi_a-phi_b)+1-Phi_b
    return clip_f

train = clip(f)(merge_train)
test = clip(f)(merge_test)

merge_train['score'] = train
merge_test['score']  = test

display(merge_train.shape)
display(merge_test.shape)

merge_train
(517754, 22)
(172585, 21)
Out[8]:
road_type num_lanes curvature speed_limit lighting weather road_signs_present public_road time_of_day holiday ... accident_risk tabm_residual realmlp xgb_residual single_tabm xgb_diff_seed20 xgb_rich nn_stacking single_ydf score
0 urban 2 0.06 35 daylight rainy False True afternoon False ... 0.13 0.130670 0.128773 0.126662 0.128814 0.128963 0.129418 0.128434 0.130264 0.118153
1 urban 4 0.99 35 daylight clear True False evening True ... 0.35 0.325544 0.327026 0.326985 0.323260 0.322940 0.322881 0.324563 0.325790 0.297000
2 rural 4 0.63 70 dim clear False True morning True ... 0.30 0.388800 0.385355 0.389054 0.382161 0.387817 0.387941 0.386533 0.386329 0.389000
3 highway 4 0.07 35 dim rainy True True morning False ... 0.21 0.134281 0.133874 0.128482 0.132775 0.129615 0.129980 0.132626 0.131700 0.121128
4 rural 1 0.58 60 daylight foggy False False evening True ... 0.56 0.472425 0.472611 0.470489 0.471251 0.469951 0.471372 0.471103 0.468466 0.474000
... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ...
517749 highway 4 0.10 70 daylight foggy True True afternoon False ... 0.32 0.316405 0.323327 0.320074 0.317979 0.320555 0.320493 0.319734 0.321161 0.330000
517750 rural 4 0.47 35 daylight rainy True True morning False ... 0.26 0.240590 0.240129 0.240548 0.236662 0.239382 0.240492 0.239043 0.242210 0.241000
517751 urban 4 0.62 25 daylight foggy False False afternoon False ... 0.19 0.294203 0.295854 0.292329 0.296087 0.293422 0.294688 0.294786 0.293608 0.286000
517752 highway 3 0.63 25 night clear True False afternoon True ... 0.51 0.497290 0.495361 0.492313 0.495484 0.493777 0.492699 0.495922 0.496974 0.489000
517753 highway 2 0.31 45 dim rainy False True afternoon True ... 0.22 0.189941 0.191042 0.192058 0.192340 0.191005 0.192192 0.189296 0.191728 0.193001
517754 rows √ó 22 columns
AutoGluon
In [9]:
# AUTOGLUON

# DEFINE AUTOGLUON
predictor = TabularPredictor(label = 'accident_risk',
                             problem_type = 'regression',
                             eval_metric = 'rmse')

# TRAIN AUTOGLUON
predictor.fit(merge_train,
              presets = 'best_quality',
              time_limit = 3600 * 11,
              auto_stack = True,
              hyperparameters = {
    'CAT': [
        # 1Ô∏è‚É£ RMSE 
        {'learning_rate': 0.1, 'depth': 6, 'iterations': 8000, 'grow_policy': 'SymmetricTree',
         'loss_function': 'RMSE', 'bootstrap_type': 'Bayesian', 'leaf_estimation_method': 'Newton'},
        
        # 2Ô∏è‚É£ DEPTHWISE CAT + RMSE
        {'learning_rate': 0.05, 'depth': 8, 'iterations': 10000, 'grow_policy': 'Depthwise',
         'loss_function': 'RMSE', 'bootstrap_type': 'Bernoulli', 'leaf_estimation_method': 'Gradient'},
        
        # 3Ô∏è‚É£ LOSSGUIDE WITH RMSE
        {'learning_rate': 0.02, 'depth': 10, 'iterations': 15000, 'grow_policy': 'Lossguide',
         'loss_function': 'RMSE', 'bootstrap_type': 'Bayesian'},
        
        # 4Ô∏è‚É£ SYMMETRIC TREE WITH HUBER
        {'learning_rate': 0.01, 'depth': 12, 'iterations': 20000, 'grow_policy': 'SymmetricTree',
         'loss_function': 'Huber:delta=1.0', 'bootstrap_type': 'MVS', 'leaf_estimation_method': 'Gradient'},
        
        # 5Ô∏è‚É£ DEEP MODEL WITH QUANTILE
        {'learning_rate': 0.005, 'depth': 14, 'iterations': 25000, 'grow_policy': 'Depthwise',
         'loss_function': 'Quantile:alpha=0.5', 'bootstrap_type': 'Bayesian'},
        
        # 6Ô∏è‚É£ LOSS GUIDE BERNOULLI
        {'learning_rate': 0.02, 'depth': 10, 'iterations': 15000, 'grow_policy': 'Lossguide',
         'loss_function': 'RMSE', 'bootstrap_type': 'Bernoulli',
         'ag_args': {'name_suffix': 'cat_seed123', 'seed': 123}}
    ]
},
              #num_bag_folds = 5,
              #num_bag_sets = 3,
              num_cpus = 4,
              verbosity = 2,
              ag_args_fit={'early_stopping_rounds': 300, 'num_cpus': 4, 'num_gpus': 1},
              #ag_args_fit={'time_limit': 900}
             )
No path specified. Models will be saved in: "AutogluonModels/ag-20251026_201617"
Verbosity: 2 (Standard Logging)
=================== System Info ===================
AutoGluon Version:  1.4.0
Python Version:     3.11.13
Operating System:   Linux
Platform Machine:   x86_64
Platform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024
CPU Count:          4
Memory Avail:       29.82 GB / 31.35 GB (95.1%)
Disk Space Avail:   19.50 GB / 19.52 GB (99.9%)
===================================================
Presets specified: ['best_quality']
Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)
Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1
DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.
 This is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.
 Running DyStack for up to 9900s of the 39600s of remaining time (25%).
  Context path: "/kaggle/working/AutogluonModels/ag-20251026_201617/ds_sub_fit/sub_fit_ho"
Running DyStack sub-fit ...
Beginning AutoGluon training ... Time limit = 9899s
AutoGluon will save models to "/kaggle/working/AutogluonModels/ag-20251026_201617/ds_sub_fit/sub_fit_ho"
Train Data Rows:    460225
Train Data Columns: 21
Label Column:       accident_risk
Problem Type:       regression
Preprocessing data ...
Using Feature Generators to preprocess the data ...
Fitting AutoMLPipelineFeatureGenerator...
 Available Memory:                    30485.77 MB
 Train Data (Original)  Memory Usage: 157.85 MB (0.5% of available memory)
 Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.
 Stage 1 Generators:
  Fitting AsTypeFeatureGenerator...
   Note: Converting 4 features to boolean dtype as they only contain 2 unique values.
 Stage 2 Generators:
  Fitting FillNaFeatureGenerator...
 Stage 3 Generators:
  Fitting IdentityFeatureGenerator...
  Fitting CategoryFeatureGenerator...
   Fitting CategoryMemoryMinimizeFeatureGenerator...
 Stage 4 Generators:
  Fitting DropUniqueFeatureGenerator...
 Stage 5 Generators:
  Fitting DropDuplicatesFeatureGenerator...
 Types of features in original data (raw dtype, special dtypes):
  ('bool', [])   :  4 | ['road_signs_present', 'public_road', 'holiday', 'school_season']
  ('float', [])  : 10 | ['curvature', 'tabm_residual', 'realmlp', 'xgb_residual', 'single_tabm', ...]
  ('int', [])    :  3 | ['num_lanes', 'speed_limit', 'num_reported_accidents']
  ('object', []) :  4 | ['road_type', 'lighting', 'weather', 'time_of_day']
 Types of features in processed data (raw dtype, special dtypes):
  ('category', [])  :  4 | ['road_type', 'lighting', 'weather', 'time_of_day']
  ('float', [])     : 10 | ['curvature', 'tabm_residual', 'realmlp', 'xgb_residual', 'single_tabm', ...]
  ('int', [])       :  3 | ['num_lanes', 'speed_limit', 'num_reported_accidents']
  ('int', ['bool']) :  4 | ['road_signs_present', 'public_road', 'holiday', 'school_season']
 1.7s = Fit runtime
 21 features in original data used to generate 21 features in processed data.
 Train Data (Processed) Memory Usage: 49.16 MB (0.2% of available memory)
Data preprocessing and feature engineering runtime = 1.82s ...
AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'
 This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
 To change this, specify the eval_metric parameter of Predictor()
User-specified model hyperparameters to be fit:
{
 'CAT': [{'learning_rate': 0.1, 'depth': 6, 'iterations': 8000, 'grow_policy': 'SymmetricTree', 'loss_function': 'RMSE', 'bootstrap_type': 'Bayesian', 'leaf_estimation_method': 'Newton'}, {'learning_rate': 0.05, 'depth': 8, 'iterations': 10000, 'grow_policy': 'Depthwise', 'loss_function': 'RMSE', 'bootstrap_type': 'Bernoulli', 'leaf_estimation_method': 'Gradient'}, {'learning_rate': 0.02, 'depth': 10, 'iterations': 15000, 'grow_policy': 'Lossguide', 'loss_function': 'RMSE', 'bootstrap_type': 'Bayesian'}, {'learning_rate': 0.01, 'depth': 12, 'iterations': 20000, 'grow_policy': 'SymmetricTree', 'loss_function': 'Huber:delta=1.0', 'bootstrap_type': 'MVS', 'leaf_estimation_method': 'Gradient'}, {'learning_rate': 0.005, 'depth': 14, 'iterations': 25000, 'grow_policy': 'Depthwise', 'loss_function': 'Quantile:alpha=0.5', 'bootstrap_type': 'Bayesian'}, {'learning_rate': 0.02, 'depth': 10, 'iterations': 15000, 'grow_policy': 'Lossguide', 'loss_function': 'RMSE', 'bootstrap_type': 'Bernoulli', 'ag_args': {'name_suffix': 'cat_seed123', 'seed': 123}}],
}
AutoGluon will fit 2 stack levels (L1 to L2) ...
WARNING: Unknown ag_args key: seed
Fitting 6 L1 models, fit_strategy="sequential" ...
Fitting model: CatBoost_BAG_L1 ... Training model for up to 6596.57s of the 9897.32s of remaining time.
Will use sequential fold fitting strategy because import of ray failed. Reason: ray==2.47.1 detected. 2.10.0 <= ray < 2.45.0 is required. You can use pip to install certain version of ray `pip install "ray>=2.10.0,<2.45.0"`
 Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=1)
 Training S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 -0.056  = Validation score   (-root_mean_squared_error)
 94.1s  = Training   runtime
 0.21s  = Validation runtime
Fitting model: CatBoost_2_BAG_L1 ... Training model for up to 6501.74s of the 9802.49s of remaining time.
 Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=1)
 Training S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 -0.0559  = Validation score   (-root_mean_squared_error)
 43.8s  = Training   runtime
 1.07s  = Validation runtime
Fitting model: CatBoost_3_BAG_L1 ... Training model for up to 6456.37s of the 9757.13s of remaining time.
 Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=1)
 Training S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 -0.0559  = Validation score   (-root_mean_squared_error)
 62.77s  = Training   runtime
 2.51s  = Validation runtime
Fitting model: CatBoost_4_BAG_L1 ... Training model for up to 6390.60s of the 9691.36s of remaining time.
 Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=1)
 Training S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 -0.056  = Validation score   (-root_mean_squared_error)
 771.78s  = Training   runtime
 1.81s  = Validation runtime
Fitting model: CatBoost_5_BAG_L1 ... Training model for up to 5613.77s of the 8914.52s of remaining time.
 Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=1)
 Training S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 -0.0609  = Validation score   (-root_mean_squared_error)
 1547.19s  = Training   runtime
 15.11s  = Validation runtime
Fitting model: CatBoostcat_seed123_BAG_L1 ... Training model for up to 4038.67s of the 7339.43s of remaining time.
 Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=1)
 Training S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 -0.0559  = Validation score   (-root_mean_squared_error)
 54.94s  = Training   runtime
 2.08s  = Validation runtime
Fitting model: WeightedEnsemble_L2 ... Training model for up to 659.66s of the 7281.91s of remaining time.
 Ensemble Weights: {'CatBoost_2_BAG_L1': 0.4, 'CatBoost_3_BAG_L1': 0.4, 'CatBoost_BAG_L1': 0.2}
 -0.0559  = Validation score   (-root_mean_squared_error)
 0.39s  = Training   runtime
 0.01s  = Validation runtime
WARNING: Unknown ag_args key: seed
Fitting 6 L2 models, fit_strategy="sequential" ...
Fitting model: CatBoost_BAG_L2 ... Training model for up to 7281.47s of the 7281.43s of remaining time.
 Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=1)
 Training S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 -0.056  = Validation score   (-root_mean_squared_error)
 69.9s  = Training   runtime
 0.23s  = Validation runtime
Fitting model: CatBoost_2_BAG_L2 ... Training model for up to 7210.68s of the 7210.64s of remaining time.
 Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=1)
 Training S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 -0.0559  = Validation score   (-root_mean_squared_error)
 43.41s  = Training   runtime
 1.17s  = Validation runtime
Fitting model: CatBoost_3_BAG_L2 ... Training model for up to 7165.43s of the 7165.39s of remaining time.
 Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=1)
 Training S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 -0.0559  = Validation score   (-root_mean_squared_error)
 64.48s  = Training   runtime
 2.42s  = Validation runtime
Fitting model: CatBoost_4_BAG_L2 ... Training model for up to 7097.85s of the 7097.81s of remaining time.
 Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=1)
 Training S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 -0.056  = Validation score   (-root_mean_squared_error)
 771.2s  = Training   runtime
 1.81s  = Validation runtime
Fitting model: CatBoost_5_BAG_L2 ... Training model for up to 6321.33s of the 6321.29s of remaining time.
 Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=1)
 Training S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 -0.0561  = Validation score   (-root_mean_squared_error)
 1712.16s  = Training   runtime
 17.3s  = Validation runtime
Fitting model: CatBoostcat_seed123_BAG_L2 ... Training model for up to 4578.45s of the 4578.40s of remaining time.
 Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=1)
 Training S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 -0.0559  = Validation score   (-root_mean_squared_error)
 67.8s  = Training   runtime
 2.64s  = Validation runtime
Fitting model: WeightedEnsemble_L3 ... Training model for up to 728.15s of the 4507.21s of remaining time.
 Ensemble Weights: {'CatBoost_3_BAG_L1': 0.286, 'CatBoost_2_BAG_L2': 0.286, 'CatBoost_BAG_L1': 0.143, 'CatBoost_2_BAG_L1': 0.143, 'CatBoost_5_BAG_L2': 0.143}
 -0.0559  = Validation score   (-root_mean_squared_error)
 0.83s  = Training   runtime
 0.01s  = Validation runtime
AutoGluon training complete, total runtime = 5392.92s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1393.7 rows/s (57529 batch size)
TabularPredictor saved. To load, use: predictor = TabularPredictor.load("/kaggle/working/AutogluonModels/ag-20251026_201617/ds_sub_fit/sub_fit_ho")
Deleting DyStack predictor artifacts (clean_up_fits=True) ...
Leaderboard on holdout data (DyStack):
                         model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order
0          WeightedEnsemble_L2      -0.055797  -0.055904  root_mean_squared_error        3.195446       3.802931   201.061706                 0.003024                0.007779           0.394658            2       True          7
1   CatBoostcat_seed123_BAG_L1      -0.055800  -0.055911  root_mean_squared_error        1.801284       2.079180    54.940387                 1.801284                2.079180          54.940387            1       True          6
2            CatBoost_3_BAG_L1      -0.055800  -0.055910  root_mean_squared_error        2.124536       2.514497    62.765203                 2.124536                2.514497          62.765203            1       True          3
3          WeightedEnsemble_L3      -0.055801  -0.055886  root_mean_squared_error       46.576544      41.284425  4330.986381                 0.004273                0.008347           0.834344            3       True         14
4            CatBoost_2_BAG_L2      -0.055801  -0.055907  root_mean_squared_error       26.114064      23.973830  2617.992771                 0.964395                1.170913          43.414604            2       True          9
5            CatBoost_3_BAG_L2      -0.055803  -0.055913  root_mean_squared_error       27.227294      25.224621  2639.060510                 2.077626                2.421705          64.482343            2       True         10
6            CatBoost_2_BAG_L1      -0.055803  -0.055915  root_mean_squared_error        0.887497       1.068048    43.799121                 0.887497                1.068048          43.799121            1       True          2
7   CatBoostcat_seed123_BAG_L2      -0.055804  -0.055913  root_mean_squared_error       27.348055      25.442054  2642.378613                 2.198387                2.639138          67.800446            2       True         13
8              CatBoost_BAG_L2      -0.055809  -0.055962  root_mean_squared_error       25.335624      23.033610  2644.474869                 0.185955                0.230694          69.896703            2       True          8
9              CatBoost_BAG_L1      -0.055815  -0.055955  root_mean_squared_error        0.180389       0.212607    94.102724                 0.180389                0.212607          94.102724            1       True          1
10           CatBoost_4_BAG_L2      -0.055897  -0.056028  root_mean_squared_error       26.998634      24.612780  3345.781862                 1.848966                1.809864         771.203695            2       True         11
11           CatBoost_4_BAG_L1      -0.055900  -0.056020  root_mean_squared_error        1.947867       1.814829   771.778890                 1.947867                1.814829         771.778890            1       True          4
12           CatBoost_5_BAG_L2      -0.056051  -0.056121  root_mean_squared_error       45.607875      40.105165  4286.737433                20.458207               17.302249        1712.159266            2       True         12
13           CatBoost_5_BAG_L1      -0.056865  -0.060883  root_mean_squared_error       18.208096      15.113755  1547.191843                18.208096               15.113755        1547.191843            1       True          5
 0  = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)
 5449s  = DyStack   runtime | 34151s  = Remaining runtime
Starting main fit with num_stack_levels=0.
 For future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`
Beginning AutoGluon training ... Time limit = 34151s
AutoGluon will save models to "/kaggle/working/AutogluonModels/ag-20251026_201617"
Train Data Rows:    517754
Train Data Columns: 21
Label Column:       accident_risk
Problem Type:       regression
Preprocessing data ...
Using Feature Generators to preprocess the data ...
Fitting AutoMLPipelineFeatureGenerator...
 Available Memory:                    30160.46 MB
 Train Data (Original)  Memory Usage: 177.59 MB (0.6% of available memory)
 Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.
 Stage 1 Generators:
  Fitting AsTypeFeatureGenerator...
   Note: Converting 4 features to boolean dtype as they only contain 2 unique values.
 Stage 2 Generators:
  Fitting FillNaFeatureGenerator...
 Stage 3 Generators:
  Fitting IdentityFeatureGenerator...
  Fitting CategoryFeatureGenerator...
   Fitting CategoryMemoryMinimizeFeatureGenerator...
 Stage 4 Generators:
  Fitting DropUniqueFeatureGenerator...
 Stage 5 Generators:
  Fitting DropDuplicatesFeatureGenerator...
 Types of features in original data (raw dtype, special dtypes):
  ('bool', [])   :  4 | ['road_signs_present', 'public_road', 'holiday', 'school_season']
  ('float', [])  : 10 | ['curvature', 'tabm_residual', 'realmlp', 'xgb_residual', 'single_tabm', ...]
  ('int', [])    :  3 | ['num_lanes', 'speed_limit', 'num_reported_accidents']
  ('object', []) :  4 | ['road_type', 'lighting', 'weather', 'time_of_day']
 Types of features in processed data (raw dtype, special dtypes):
  ('category', [])  :  4 | ['road_type', 'lighting', 'weather', 'time_of_day']
  ('float', [])     : 10 | ['curvature', 'tabm_residual', 'realmlp', 'xgb_residual', 'single_tabm', ...]
  ('int', [])       :  3 | ['num_lanes', 'speed_limit', 'num_reported_accidents']
  ('int', ['bool']) :  4 | ['road_signs_present', 'public_road', 'holiday', 'school_season']
 1.9s = Fit runtime
 21 features in original data used to generate 21 features in processed data.
 Train Data (Processed) Memory Usage: 55.30 MB (0.2% of available memory)
Data preprocessing and feature engineering runtime = 2.03s ...
AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'
 This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
 To change this, specify the eval_metric parameter of Predictor()
User-specified model hyperparameters to be fit:
{
 'CAT': [{'learning_rate': 0.1, 'depth': 6, 'iterations': 8000, 'grow_policy': 'SymmetricTree', 'loss_function': 'RMSE', 'bootstrap_type': 'Bayesian', 'leaf_estimation_method': 'Newton'}, {'learning_rate': 0.05, 'depth': 8, 'iterations': 10000, 'grow_policy': 'Depthwise', 'loss_function': 'RMSE', 'bootstrap_type': 'Bernoulli', 'leaf_estimation_method': 'Gradient'}, {'learning_rate': 0.02, 'depth': 10, 'iterations': 15000, 'grow_policy': 'Lossguide', 'loss_function': 'RMSE', 'bootstrap_type': 'Bayesian'}, {'learning_rate': 0.01, 'depth': 12, 'iterations': 20000, 'grow_policy': 'SymmetricTree', 'loss_function': 'Huber:delta=1.0', 'bootstrap_type': 'MVS', 'leaf_estimation_method': 'Gradient'}, {'learning_rate': 0.005, 'depth': 14, 'iterations': 25000, 'grow_policy': 'Depthwise', 'loss_function': 'Quantile:alpha=0.5', 'bootstrap_type': 'Bayesian'}, {'learning_rate': 0.02, 'depth': 10, 'iterations': 15000, 'grow_policy': 'Lossguide', 'loss_function': 'RMSE', 'bootstrap_type': 'Bernoulli', 'ag_args': {'name_suffix': 'cat_seed123', 'seed': 123}}],
}
WARNING: Unknown ag_args key: seed
Fitting 6 L1 models, fit_strategy="sequential" ...
Fitting model: CatBoost_BAG_L1 ... Training model for up to 34148.86s of the 34148.86s of remaining time.
 Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=1)
 Training S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 -0.0559  = Validation score   (-root_mean_squared_error)
 81.0s  = Training   runtime
 0.25s  = Validation runtime
Fitting model: CatBoost_2_BAG_L1 ... Training model for up to 34067.03s of the 34067.03s of remaining time.
 Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=1)
 Training S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 -0.0559  = Validation score   (-root_mean_squared_error)
 42.61s  = Training   runtime
 1.22s  = Validation runtime
Fitting model: CatBoost_3_BAG_L1 ... Training model for up to 34022.57s of the 34022.57s of remaining time.
 Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=1)
 Training S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 -0.0559  = Validation score   (-root_mean_squared_error)
 60.7s  = Training   runtime
 2.4s  = Validation runtime
Fitting model: CatBoost_4_BAG_L1 ... Training model for up to 33958.92s of the 33958.92s of remaining time.
 Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=1)
 Training S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 -0.056  = Validation score   (-root_mean_squared_error)
 853.08s  = Training   runtime
 2.12s  = Validation runtime
Fitting model: CatBoost_5_BAG_L1 ... Training model for up to 33100.39s of the 33100.39s of remaining time.
 Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=1)
 Training S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 -0.0562  = Validation score   (-root_mean_squared_error)
 1863.85s  = Training   runtime
 20.1s  = Validation runtime
Fitting model: CatBoostcat_seed123_BAG_L1 ... Training model for up to 31200.72s of the 31200.71s of remaining time.
 Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=1)
 Training S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 Training S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.
 Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.
 -0.0559  = Validation score   (-root_mean_squared_error)
 59.68s  = Training   runtime
 2.4s  = Validation runtime
Fitting model: WeightedEnsemble_L2 ... Training model for up to 3414.89s of the 31137.96s of remaining time.
 Ensemble Weights: {'CatBoost_3_BAG_L1': 0.429, 'CatBoost_2_BAG_L1': 0.286, 'CatBoost_BAG_L1': 0.143, 'CatBoost_5_BAG_L1': 0.143}
 -0.0559  = Validation score   (-root_mean_squared_error)
 0.54s  = Training   runtime
 0.01s  = Validation runtime
AutoGluon training complete, total runtime = 3013.6s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2700.4 rows/s (64720 batch size)
TabularPredictor saved. To load, use: predictor = TabularPredictor.load("/kaggle/working/AutogluonModels/ag-20251026_201617")
Out[9]:
<autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f7822ff7690>
Evaluation
In [10]:
%%time
# COMPARE MODELS
predictor.leaderboard(silent = True)
CPU times: user 4.92 ms, sys: 0 ns, total: 4.92 ms
Wall time: 4.5 ms
Out[10]:
model score_val eval_metric pred_time_val fit_time pred_time_val_marginal fit_time_marginal stack_level can_infer fit_order
0 WeightedEnsemble_L2 -0.055886 root_mean_squared_error 23.974203 2048.703790 0.008374 0.541927 2 True 7
1 CatBoost_3_BAG_L1 -0.055895 root_mean_squared_error 2.397406 60.695524 2.397406 60.695524 1 True 3
2 CatBoostcat_seed123_BAG_L1 -0.055896 root_mean_squared_error 2.403250 59.682199 2.403250 59.682199 1 True 6
3 CatBoost_2_BAG_L1 -0.055899 root_mean_squared_error 1.218748 42.614581 1.218748 42.614581 1 True 2
4 CatBoost_BAG_L1 -0.055936 root_mean_squared_error 0.247192 80.999851 0.247192 80.999851 1 True 1
5 CatBoost_4_BAG_L1 -0.056029 root_mean_squared_error 2.117556 853.080281 2.117556 853.080281 1 True 4
6 CatBoost_5_BAG_L1 -0.056172 root_mean_squared_error 20.102483 1863.851906 20.102483 1863.851906 1 True 5
In [11]:
# CHECK FEATURE IMPORTANCES 

importance_df = predictor.feature_importance(merge_train[:500])

importance_df.style.background_gradient(subset=['importance', 'stddev'], cmap='Blues')
Computing feature importance via permutation shuffling for 21 features using 500 rows with 5 shuffle sets...
 431.6s = Expected runtime (86.32s per shuffle set)
 41.24s = Actual runtime (Completed 5 of 5 shuffle sets)
Out[11]:
  importance stddev p_value n p99_high p99_low
realmlp 0.017012 0.001228 0.000003 5 0.019541 0.014484
xgb_rich 0.016876 0.001166 0.000003 5 0.019277 0.014475
xgb_residual 0.016814 0.001108 0.000002 5 0.019096 0.014532
single_ydf 0.016779 0.001115 0.000002 5 0.019075 0.014484
single_tabm 0.013699 0.000859 0.000002 5 0.015469 0.011930
tabm_residual 0.012600 0.000749 0.000001 5 0.014142 0.011058
nn_stacking 0.012479 0.000975 0.000004 5 0.014486 0.010472
curvature 0.002329 0.000636 0.000606 5 0.003639 0.001020
score 0.002180 0.000301 0.000043 5 0.002800 0.001560
xgb_diff_seed20 0.001441 0.000453 0.001029 5 0.002372 0.000509
num_reported_accidents 0.000251 0.000104 0.002851 5 0.000466 0.000037
lighting 0.000180 0.000046 0.000487 5 0.000275 0.000084
num_lanes 0.000155 0.000012 0.000005 5 0.000181 0.000130
speed_limit 0.000154 0.000039 0.000445 5 0.000234 0.000074
time_of_day 0.000130 0.000015 0.000023 5 0.000161 0.000098
road_type 0.000119 0.000015 0.000030 5 0.000150 0.000088
weather 0.000102 0.000039 0.002046 5 0.000182 0.000023
holiday 0.000077 0.000019 0.000393 5 0.000116 0.000038
road_signs_present 0.000071 0.000006 0.000006 5 0.000084 0.000058
school_season 0.000070 0.000008 0.000022 5 0.000086 0.000053
public_road 0.000066 0.000007 0.000012 5 0.000080 0.000052
In [12]:
# PLOT FEATURE IMPORTANCE

imp = importance_df['importance'].sort_values(ascending=True)

plt.figure(figsize=(6, 8))
imp.plot(kind='barh', color='steelblue')
plt.title('Feature Importance (AutoGluon)')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.show()
In [13]:
# CHECKING BEST MODEL 

best_model = predictor.model_best

print(f'Best Model : {best_model}')
Best Model : WeightedEnsemble_L2
In [14]:
%%time
# CHECK SUBMISSION

# TEST DATA PREDICTION
y_test = predictor.predict(merge_test)

submission = pd.read_csv(r'/kaggle/input/playground-series-s5e10/sample_submission.csv')

submission['accident_risk'] = y_test

submission
CPU times: user 4min 3s, sys: 2.63 s, total: 4min 5s
Wall time: 1min 5s
Out[14]:
id accident_risk
0 517754 0.297764
1 517755 0.122818
2 517756 0.182081
3 517757 0.310411
4 517758 0.397881
... ... ...
172580 690334 0.102243
172581 690335 0.520736
172582 690336 0.252614
172583 690337 0.127006
172584 690338 0.488124
172585 rows √ó 2 columns
In [15]:
# GET OOF (OUT-OF-FOLD) PREDICTION

# GET OOF
oof_predictions = predictor.predict_oof()

# CONVERT TO DATAFRAME
y_pred = oof_predictions.to_frame(name = 'oof_prediction')  # ---> RETURN DATAFRAME
oof_df = pd.DataFrame(y_pred)

oof_df
Out[15]:
oof_prediction
0 0.131163
1 0.324436
2 0.388187
3 0.131673
4 0.469977
... ...
517749 0.320622
517750 0.242544
517751 0.296790
517752 0.493697
517753 0.190041
517754 rows √ó 1 columns
In [16]:
# EVALUATION

rmse = mean_squared_error(train_df['accident_risk'], oof_predictions, squared = False)
r2   = r2_score(train_df['accident_risk'], oof_predictions)

print(f'RMSE : {rmse}')
print(f'R2   : {r2}"')
RMSE : 0.05588621470809477
R2   : 0.8872246546591565"
In [17]:
# RESIDUAL PLOT

y_true = merge_train['accident_risk']

plt.figure(figsize = (12, 5))

# ACTUAL VS PREDICTED DATA
plt.subplot(1, 2, 1)
plt.scatter(x = y_true, y = y_pred, alpha = 0.6)
plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--', lw=2)

plt.xlabel('Predicted Values')
plt.ylabel('Actual Values')
plt.title('Actual vs Predicted Data')

# RESIDUAL PLOT
residual = y_true - y_pred['oof_prediction'].values

plt.subplot(1, 2, 2)
plt.scatter(x = y_pred, y = residual, alpha = 0.6)
plt.axhline(y=0, color='r', linestyle='--') 
plt.xlabel("Predicted values")
plt.ylabel("Residuals (y_true - y_pred)")
plt.title("Residual Plot")

plt.show()
In [18]:
# DISTRIBUTION COMPARISON

plt.figure(figsize = (12, 5))

plt.subplot(1, 2, 1)
sns.kdeplot(merge_train['accident_risk'], label = 'True Label', fill = False)
sns.kdeplot(oof_predictions, label = 'Predicted Label (OOF)', fill = False)
plt.title('True vs Predicted Accident Risk Distribution')
plt.legend()

plt.subplot(1, 2, 2)
sns.kdeplot(y_test, label = 'Test Acccident Risk', fill = False)
plt.title('Test Accident Risk Distribution')
plt.legend()

plt.show()
In [19]:
# SAVE SUBMISSION
submission.to_csv(r'autogluon_experiment11.csv', index = False)
oof_df.to_csv(r'oof_autogluon_experiment11.csv', index = False)