Ensemble (LightGBM + CatBoost + XGBoost | prediction for road accident
This notebook tackles Playground Series S5E10 – https://www.kaggle.com/competitions/playground-series-s5e10
EDA
Preprocessing
Feature engineering
Ensemble - LightGBM + CatBoost + XGBoost
(Stacking - Meta Model LGBM+CatBoost)
1. Import Libraries
In [1]:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
import pandas as pd
import numpy as np
from sklearn.model_selection import RandomizedSearchCV, KFold
from sklearn.metrics import make_scorer, mean_squared_error
import lightgbm as lgb
from scipy.stats import uniform, randint
from sklearn.model_selection import RandomizedSearchCV, KFold
from catboost import CatBoostRegressor
import xgboost as xgb
2. Import Train-Test-Submission_Sample Files
unfold_moreShow hidden cell
In [3]:
df = pd.read_csv("/kaggle/input/playground-series-s5e10/train.csv")
df_test = pd.read_csv("/kaggle/input/playground-series-s5e10/test.csv")
df_sub = pd.read_csv("/kaggle/input/playground-series-s5e10/sample_submission.csv")
3. EDA
In [4]:
print(f"df Train shape {df.shape}")
print(f"df Test shape {df_test.shape}")
df Train shape (517754, 14)
df Test shape (172585, 13)
In [5]:
df.columns
Out[5]:
Index(['id', 'road_type', 'num_lanes', 'curvature', 'speed_limit', 'lighting',
       'weather', 'road_signs_present', 'public_road', 'time_of_day',
       'holiday', 'school_season', 'num_reported_accidents', 'accident_risk'],
      dtype='object')
In [6]:
#Droping "id" column
df=df.drop("id", axis=1)
df_test=df_test.drop("id", axis=1)
In [7]:
df.head()
Out[7]:
road_type num_lanes curvature speed_limit lighting weather road_signs_present public_road time_of_day holiday school_season num_reported_accidents accident_risk
0 urban 2 0.06 35 daylight rainy False True afternoon False True 1 0.13
1 urban 4 0.99 35 daylight clear True False evening True True 0 0.35
2 rural 4 0.63 70 dim clear False True morning True False 2 0.30
3 highway 4 0.07 35 dim rainy True True morning False False 1 0.21
4 rural 1 0.58 60 daylight foggy False False evening True False 1 0.56
Check Null and Duplicate Values
In [8]:
#Check Null Value
df.isna().sum().sum()
Out[8]:
0
In [9]:
#Check Test Null Value 
df_test.isna().sum().sum()
Out[9]:
0
In [10]:
#Check Duplicate Rows
df.duplicated().sum()
Out[10]:
656
In [11]:
#Drop Duplicate Rows
df = df.drop_duplicates()
df.duplicated().sum()
Out[11]:
0
Divide Columns to Numerical and Categorical
In [12]:
num_cols =  df.select_dtypes(include="number").columns.tolist()
cat_cols = df.select_dtypes(exclude="number").columns.tolist()
num_cols.remove("accident_risk")

print(f"categorical columns : {cat_cols}")
print(f"numerical columns : {num_cols}")
categorical columns : ['road_type', 'lighting', 'weather', 'road_signs_present', 'public_road', 'time_of_day', 'holiday', 'school_season']
numerical columns : ['num_lanes', 'curvature', 'speed_limit', 'num_reported_accidents']
Numerical Features Analyse
In [13]:
df[num_cols].describe().T
Out[13]:
count mean std min 25% 50% 75% max
num_lanes 517098.0 2.491454 1.120404 1.0 1.00 2.00 3.00 4.0
curvature 517098.0 0.488749 0.272570 0.0 0.26 0.51 0.71 1.0
speed_limit 517098.0 46.113058 15.788804 25.0 35.00 45.00 60.00 70.0
num_reported_accidents 517098.0 1.188057 0.896117 0.0 1.00 1.00 2.00 7.0
In [14]:
# numerical features correlation
plt.figure(figsize=(8, 6))
correlation_matrix = df[num_cols + ['accident_risk']].corr()
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', 
            linewidths=1, color="blue")
plt.show()
Categorical Features Analyse
In [15]:
#Categorical Columns Unique
for c in cat_cols :
   print(f" {c} (Uniques): {df[c].unique()}")
 road_type (Uniques): ['urban' 'rural' 'highway']
 lighting (Uniques): ['daylight' 'dim' 'night']
 weather (Uniques): ['rainy' 'clear' 'foggy']
 road_signs_present (Uniques): [False  True]
 public_road (Uniques): [ True False]
 time_of_day (Uniques): ['afternoon' 'evening' 'morning']
 holiday (Uniques): [False  True]
 school_season (Uniques): [ True False]
Ploting Categorical Features vs Target
In [16]:
# Categorical features vs target

fig , axes = plt.subplots(2,4, figsize=(16,8))
axes = axes.flatten()
cmap = plt.get_cmap('magma')
colors = cmap([0.9,0.66,0.33])
target = 'accident_risk'
for i,col in enumerate(cat_cols) :
  
    grouped = df.groupby(col)[target].mean()
   
    axes[i].bar(grouped.index.astype(str), grouped.values , color=colors)  # .astype(str) to handle non-string indices
    
    axes[i].set_ylabel(f'Mean {target}')
    axes[i].set_title(f'{col} vs {target}')
    axes[i].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()
Accident Risk Distribution
In [17]:
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.hist(df['accident_risk'], bins=50, edgecolor='black', color='lightsalmon')
plt.title('Accident Risk Distribution')
plt.xlabel('Accident Risk')

plt.subplot(1, 2, 2)
df['accident_risk'].plot(kind='box', color='salmon')
plt.title('Accident Risk Box Plot')
plt.tight_layout()
plt.show()
4. Pre Processing
Feature Engineering
In [18]:
df['weather_lighting'] = df['weather'].astype(str) + '_' + df['lighting'].astype(str)
df_test['weather_lighting'] = df_test['weather'].astype(str) + '_' + df_test['lighting'].astype(str)

df['speed_squared'] = df['speed_limit'] ** 2
df_test['speed_squared'] = df_test['speed_limit'] ** 2

df['curvature_squared'] = df['curvature'] ** 2
df_test['curvature_squared'] = df_test['curvature'] ** 2

df['meta_curvature'] = 0.3 * df['curvature']
df['meta_night'] = 0.2 * (df['lighting'] == 'night').astype(int)
df['meta_weather'] = 0.1 * (df['weather'] != 'clear').astype(int)
df['meta_speed'] = 0.2 * (df['speed_limit'] >= 60).astype(int)
df['meta_accidents'] = 0.1 * (df['num_reported_accidents'] > 2).astype(int)

df_test['meta_curvature'] = 0.3 * df_test['curvature']
df_test['meta_night'] = 0.2 * (df_test['lighting'] == 'night').astype(int)
df_test['meta_weather'] = 0.1 * (df_test['weather'] != 'clear').astype(int)
df_test['meta_speed'] = 0.2 * (df_test['speed_limit'] >= 60).astype(int)
df_test['meta_accidents'] = 0.1 * (df_test['num_reported_accidents'] > 2).astype(int)
Log transforms to handle skewness
In [19]:
df['log_curvature'] = np.log1p(df['curvature'])
df_test['log_curvature'] = np.log1p(df_test['curvature'])
Meta Feature I see in many Competition Notebook
In [20]:
def f(X):
    return \
    0.3 * X["curvature"] + \
    0.2 * (X["lighting"] == "night").astype(int) + \
    0.1 * (X["weather"] != "clear").astype(int) + \
    0.2 * (X["speed_limit"] >= 60).astype(int) + \
    0.1 * (X["num_reported_accidents"] > 2).astype(int)


    
df['meta'] = f(df)
df_test['meta'] = f(df_test)

#Very good FE
In [21]:
df.head()
Out[21]:
road_type num_lanes curvature speed_limit lighting weather road_signs_present public_road time_of_day holiday ... weather_lighting speed_squared curvature_squared meta_curvature meta_night meta_weather meta_speed meta_accidents log_curvature meta
0 urban 2 0.06 35 daylight rainy False True afternoon False ... rainy_daylight 1225 0.0036 0.018 0.0 0.1 0.0 0.0 0.058269 0.118
1 urban 4 0.99 35 daylight clear True False evening True ... clear_daylight 1225 0.9801 0.297 0.0 0.0 0.0 0.0 0.688135 0.297
2 rural 4 0.63 70 dim clear False True morning True ... clear_dim 4900 0.3969 0.189 0.0 0.0 0.2 0.0 0.488580 0.389
3 highway 4 0.07 35 dim rainy True True morning False ... rainy_dim 1225 0.0049 0.021 0.0 0.1 0.0 0.0 0.067659 0.121
4 rural 1 0.58 60 daylight foggy False False evening True ... foggy_daylight 3600 0.3364 0.174 0.0 0.1 0.2 0.0 0.457425 0.474
5 rows × 23 columns
Handeling Categorical Columns
In [22]:
#convert Boolean columns

bool_cols = ["road_signs_present", "public_road","holiday", "school_season"]
for col in bool_cols :
    df[col]= df[col].astype(int)
    df_test[col]=df_test[col].astype(int)
In [23]:
#label encoding categorical feature
le = LabelEncoder()
cate_cols = df.select_dtypes(exclude="number").columns.tolist()
# cate_cols = ['road_type', 'lighting', 'weather', 'time_of_day']
for col in cate_cols :
    df[col]= le.fit_transform(df[col])
    df_test[col]=le.transform(df_test[col])
    
Final DataSet Shape
In [24]:
df_test.head()
Out[24]:
road_type num_lanes curvature speed_limit lighting weather road_signs_present public_road time_of_day holiday ... weather_lighting speed_squared curvature_squared meta_curvature meta_night meta_weather meta_speed meta_accidents log_curvature meta
0 0 2 0.34 45 2 0 1 1 0 1 ... 2 2025 0.1156 0.102 0.2 0.0 0.0 0.0 0.292670 0.302
1 2 3 0.04 45 1 1 1 0 0 1 ... 4 2025 0.0016 0.012 0.0 0.1 0.0 0.0 0.039221 0.112
2 2 2 0.59 35 1 0 1 0 0 1 ... 1 1225 0.3481 0.177 0.0 0.0 0.0 0.0 0.463734 0.177
3 1 4 0.95 35 0 2 0 0 0 0 ... 6 1225 0.9025 0.285 0.0 0.1 0.0 0.0 0.667829 0.385
4 0 2 0.86 35 0 0 1 0 1 0 ... 0 1225 0.7396 0.258 0.0 0.0 0.0 0.1 0.620576 0.358
5 rows × 22 columns
In [25]:
df.head()
Out[25]:
road_type num_lanes curvature speed_limit lighting weather road_signs_present public_road time_of_day holiday ... weather_lighting speed_squared curvature_squared meta_curvature meta_night meta_weather meta_speed meta_accidents log_curvature meta
0 2 2 0.06 35 0 2 0 1 0 0 ... 6 1225 0.0036 0.018 0.0 0.1 0.0 0.0 0.058269 0.118
1 2 4 0.99 35 0 0 1 0 1 1 ... 0 1225 0.9801 0.297 0.0 0.0 0.0 0.0 0.688135 0.297
2 1 4 0.63 70 1 0 0 1 2 1 ... 1 4900 0.3969 0.189 0.0 0.0 0.2 0.0 0.488580 0.389
3 0 4 0.07 35 1 2 1 1 2 0 ... 7 1225 0.0049 0.021 0.0 0.1 0.0 0.0 0.067659 0.121
4 1 1 0.58 60 0 1 0 0 1 1 ... 3 3600 0.3364 0.174 0.0 0.1 0.2 0.0 0.457425 0.474
5 rows × 23 columns
In [26]:
print("Train Shape :" , df.shape)
print("Test Shape :" , df_test.shape)
Train Shape : (517098, 23)
Test Shape : (172585, 22)
5. X , y (Train Test Split)
In [27]:
X= df.drop('accident_risk', axis =1)
y= df['accident_risk']
X_test = df_test

X_train= X
y_train= y
6. Model Training and Evaluation
Ensemble with LGB+CatBoost+XGBoost
In [28]:
# Lighgbm best param from random search cv
param_lgb = {
    
    'n_estimators': 2700,
    'learning_rate': 0.01,
    'num_leaves': 99,
    'max_depth': 13,
    'min_child_samples': 10,
    'min_child_weight': 0.002,
    'subsample': 0.60,
    'subsample_freq': 1,
    'colsample_bytree': 0.83,
    'reg_alpha': 0.01,
    'reg_lambda':  0.70,
    'min_split_gain':  0.004,
    'feature_fraction': 0.9 , 

 
}

# catboost best param from random search cv
param_cat = {
     'bagging_temperature' : 0.20,
     'border_count'        : 178,
     'depth'               : 8,
     'iterations'          : 1600,
     'l2_leaf_reg'         : 4,
     'learning_rate'       : 0.04,
     'random_strength'    : 0.32,
     
}

# xgboost best param from random search cv
param_xgb = {
              'n_estimators': 1251,
              'learning_rate': 0.0074,
              'max_depth': 9,
              'min_child_weight': 3,
              'subsample': 0.72,
              'colsample_bytree': 0.74,
              'colsample_bylevel': 0.94,
              'gamma': 0.0002,
              'reg_alpha': 0.61,
              'reg_lambda': 4.92}


print("\n" + "="*60)
print("Simple Average (90-10)")
print("="*60)

cat_model =  CatBoostRegressor(**param_cat,
                               loss_function='RMSE',
                               random_seed=42,
                               verbose=False,
                               thread_count=-1,)

lgb_model = lgb.LGBMRegressor(**param_lgb ,
                               objective='regression',
                               metric='rmse',
                               boosting_type='gbdt',
                               random_state=42,
                               n_jobs=-1,
                               verbose=-1    
                            )  

xgb_model = xgb.XGBRegressor(**param_xgb,
                              random_state = 42,
                              objective = 'reg:squarederror')


#Lets see CV Score
kfold = KFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = []

for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train), 1):
    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
    
    
    # Train cat
    cat_model.fit(X_tr, y_tr)
    cat_pred = cat_model.predict(X_val)
    
    # Train LightGBM
    lgb_model.fit(X_tr, y_tr)
    lgb_pred = lgb_model.predict(X_val)

    # Train XGBoost
    xgb_model.fit(X_tr, y_tr)
    xgb_pred = xgb_model.predict(X_val)
    # Simple average
    ensemble_pred = 0.3  * cat_pred + 0.3 * lgb_pred + 0.4 * xgb_pred
    # Simple average
       
    rmse = np.sqrt(mean_squared_error(y_val, ensemble_pred))
    cv_scores.append(rmse)
    print(f"Fold {fold}: {rmse:.5f}")

simple_avg_score = np.mean(cv_scores)
print(f"\nSimple Average CV Score: {simple_avg_score:.5f} (+/- {np.std(cv_scores):.5f})")
============================================================
Simple Average (90-10)
============================================================
Fold 1: 0.05586
Fold 2: 0.05614
Fold 3: 0.05601
Fold 4: 0.05589
Fold 5: 0.05592

Simple Average CV Score: 0.05597 (+/- 0.00010)
In [29]:
xgb_model.fit(X_train, y_train)
xgb_pred = xgb_model.predict(X_test)

lgb_model.fit(X_train, y_train)
lgb_pred = lgb_model.predict(X_test)

cat_model.fit(X_train,y_train,)
cat_pred = cat_model.predict(X_test)

ensemble_pred = 0.3  * cat_pred + 0.3 * lgb_pred + 0.4 * xgb_pred
Feature Importance
In [30]:
feature_importances = xgb_model.feature_importances_

importance_df = pd.DataFrame({
    'feature': X.columns, 
    'importance': feature_importances
})

importance_df = importance_df.sort_values('importance', ascending=False)

plt.style.use('fivethirtyeight')
plt.figure(figsize=(10, 8))
sns.barplot(x='importance', 
            y='feature', 
            data=importance_df.head(10)) 
plt.title('Feature Importance (XGB)')
plt.xlabel('Importance Score')
plt.ylabel('Features')
plt.tight_layout()
plt.show()
unfold_moreShow hidden markdown
unfold_moreShow hidden code
unfold_moreShow hidden code
7. Make Submission
In [33]:
df_sub['accident_risk'] = ensemble_pred

df_sub.to_csv('submission.csv', index=False)

df_sub.head()
Out[33]:
id accident_risk
0 517754 0.294450
1 517755 0.120764
2 517756 0.180657
3 517757 0.311562
4 517758 0.400211