Blend It Like a DJ: Turning Rhythm Features into Winning Predictions
The notebook begins by preparing the data: it reads the input files, defines the target, and builds new features by adding squared terms, pairwise products, ratios, and quantile-based bins. The target is stabilized with a Yeo–Johnson transform, which is later inverted to recover predictions in the original BPM scale. Three gradient-boosting models—LightGBM, XGBoost, and CatBoost—are trained with cross-validation, producing reliable out-of-fold predictions and averaged test predictions. Each model’s performance is logged with RMSE, and predictions are clipped to realistic tempo ranges derived from the training distribution. Calibration is explored through isotonic regression, and model outputs are blended with optimized weights chosen to minimize OOF error. The final blended predictions are calibrated, clipped, and saved to submission.csv in the required format.
Imports, Globals, and Configuration
This section sets up the working environment. It imports core libraries for data handling, modeling, preprocessing, and evaluation. Global settings are defined, including random seeds, file paths, column names, and print/display preferences. These configurations provide a consistent foundation for the rest of the workflow.
In [1]:
# -----------------------------
# Imports, Globals, and Configuration
# -----------------------------

import os, math, warnings
import numpy as np
import pandas as pd
from dataclasses import dataclass
from typing import List, Tuple

from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import PowerTransformer
from sklearn.isotonic import IsotonicRegression

import lightgbm as lgb
from xgboost import XGBRegressor
from catboost import CatBoostRegressor, Pool

warnings.filterwarnings("ignore")
np.set_printoptions(suppress=True)
Config
This section defines the core configuration. It sets the random seed, the number of cross-validation folds, and allows for multiple seeds if bagged CV is desired. It specifies the dataset directory and identifies the ID and target columns. Finally, it lists the nine base numeric features that form the foundation for further feature engineering and modeling.
In [2]:
# -----------------------------
# Config
# -----------------------------
SEED = 42
N_FOLDS = 5
CV_SEEDS = [42]  
DATA_DIR = "/kaggle/input/playground-series-s5e9"
ID_COL = "id"
TARGET_COL = "BeatsPerMinute"

BASE_NUM_COLS = [
    'RhythmScore','AudioLoudness','VocalContent','AcousticQuality',
    'InstrumentalScore','LivePerformanceLikelihood','MoodScore',
    'TrackDurationMs','Energy'
]
Utils
This utility section introduces two key components. First, it defines an rmse function for evaluating predictions with Root Mean Squared Error. Second, it creates a CVResult dataclass to neatly store cross-validation outcomes, including out-of-fold predictions, test predictions, fold-level scores, and the best iteration counts from each fold.
In [3]:
# -----------------------------
# Utils
# -----------------------------

def rmse(y_true, y_pred):
    return mean_squared_error(y_true, y_pred, squared=False)

@dataclass
class CVResult:
    oof: np.ndarray
    test_pred: np.ndarray
    fold_metrics: List[float]
    best_iters: List[int]
Feature Engineering
This section builds extra signal from the nine base numeric features without leaking information. It first computes train-only quantile edges and defines a helper to assign bin indices. Then it expands features by adding squared terms, all pairwise products, and safe two-way ratios (with a tiny epsilon to avoid divide-by-zero). It creates quartile and decile bins from the train distribution and applies the same bin edges to both train and test. Finally, it returns the engineered train/test data and the complete feature list (excluding id and the target).
In [4]:
# -----------------------------
# Feature Engineering
# -----------------------------

def compute_bin_edges(train_col: pd.Series, q: List[float]) -> np.ndarray:
    # unique quantiles, padded with -inf/inf
    qs = np.unique(np.clip(q, 0.0, 1.0))
    edges = np.quantile(train_col.values, qs)
    edges = np.unique(edges)
    # ensure strictly increasing edges; add tiny jitter if necessary
    for i in range(1, len(edges)):
        if edges[i] <= edges[i-1]:
            edges[i] = edges[i-1] + 1e-9
    return np.concatenate(([-np.inf], edges, [np.inf]))


def apply_bins(col: pd.Series, edges: np.ndarray) -> np.ndarray:
    # returns bin indices 0..len(edges)-2
    return np.digitize(col.values, edges) - 1


def build_features(train: pd.DataFrame, test: pd.DataFrame, base_cols: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame, List[str]]:
    """Create engineered features on train & test using train-only stats.
    - Pairwise products (including squares) and safe ratios (both directions)
    - Quantile-based bins (quartile & decile) computed from train only
    """
    tr = train.copy()
    te = test.copy()

    # Cast to float32 early for memory
    for c in base_cols:
        tr[c] = tr[c].astype(np.float32)
        te[c] = te[c].astype(np.float32)

    # Products & squares
    for i, c1 in enumerate(base_cols):
        v1_tr = tr[c1]
        v1_te = te[c1]
        tr[f"{c1}_sq"] = v1_tr * v1_tr
        te[f"{c1}_sq"] = v1_te * v1_te
        for j in range(i+1, len(base_cols)):
            c2 = base_cols[j]
            tr[f"{c1}_x_{c2}"] = v1_tr * tr[c2]
            te[f"{c1}_x_{c2}"] = v1_te * te[c2]

    # Safe ratios (both directions)
    eps = 1e-6
    for i, c1 in enumerate(base_cols):
        for j, c2 in enumerate(base_cols):
            if i == j:
                continue
            tr[f"{c1}_div_{c2}"] = tr[c1] / (tr[c2].abs() + eps)
            te[f"{c1}_div_{c2}"] = te[c1] / (te[c2].abs() + eps)

    # Quantile bins from train only
    quart_q = [0.25, 0.5, 0.75]
    dec_q   = [i/10 for i in range(1, 10)]
    for c in base_cols:
        edges4 = compute_bin_edges(tr[c], quart_q)
        edges10 = compute_bin_edges(tr[c], dec_q)
        tr[f"{c}_quartile"] = apply_bins(tr[c], edges4).astype(np.int8)
        te[f"{c}_quartile"] = apply_bins(te[c], edges4).astype(np.int8)
        tr[f"{c}_decile"] = apply_bins(tr[c], edges10).astype(np.int8)
        te[f"{c}_decile"] = apply_bins(te[c], edges10).astype(np.int8)

    # Final feature list: all except ID/TARGET
    feat_cols = [c for c in tr.columns if c not in [ID_COL, TARGET_COL]]
    return tr, te, feat_cols
Model Trainers
This block defines three cross-validated training routines—one each for LightGBM, XGBoost, and CatBoost. Each routine builds a simple numeric preprocessor (median imputation), runs KFold CV (optionally across multiple CV seeds), tracks the best iteration via early stopping or “best model,” and aggregates true OOF predictions, averaged test predictions, and fold RMSEs. At the end of each routine, it reports the overall CV RMSE and returns a compact CVResult bundle with the OOF vector, test predictions, per-fold scores, and iteration counts.
In [5]:
# -----------------------------
# Model Trainers
# -----------------------------

def fit_lgbm(X: pd.DataFrame, y: pd.Series, X_test: pd.DataFrame, num_cols: List[str]) -> CVResult:
    params = dict(
        n_estimators=6000,
        learning_rate=0.02,
        num_leaves=127,
        max_depth=-1,
        subsample=0.9,
        colsample_bytree=0.9,
        reg_lambda=1.0,
        min_child_samples=25,
        random_state=SEED,
        n_jobs=-1,
    )

    # Preprocessor: median imputation for numeric
    num_pipe = Pipeline([("imp", SimpleImputer(strategy="median"))])
    pre = ColumnTransformer([("num", num_pipe, num_cols)], remainder="drop")

    oof_sum = np.zeros(len(X), dtype=float)
    oof_cnt = np.zeros(len(X), dtype=float)
    test_pred = np.zeros(len(X_test), dtype=float)
    fold_metrics = []
    best_iters = []

    for cv_seed in CV_SEEDS:
        kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=cv_seed)
        for fold, (tr_idx, va_idx) in enumerate(kf.split(X, y), 1):
            print(f"\n[LGBM] Seed {cv_seed} | Fold {fold}")
            X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]
            y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]

            pre.fit(X_tr, y_tr)
            Xtr = pre.transform(X_tr)
            Xva = pre.transform(X_va)
            Xte = pre.transform(X_test)

            model = lgb.LGBMRegressor(**params)
            model.fit(
                Xtr, y_tr,
                eval_set=[(Xva, y_va)],
                eval_metric="rmse",
                callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=True)],
            )
            bi = int(model.best_iteration_)
            best_iters.append(bi)

            va_pred = model.predict(Xva, num_iteration=bi)
            oof_sum[va_idx] += va_pred
            oof_cnt[va_idx] += 1
            fold_rmse = rmse(y_va, va_pred)
            fold_metrics.append(fold_rmse)
            print(f"[LGBM] RMSE: {fold_rmse:.5f}")

            test_pred += model.predict(Xte, num_iteration=bi) / (N_FOLDS * len(CV_SEEDS))

    oof = oof_sum / np.maximum(oof_cnt, 1)
    print(f"[LGBM] CV RMSE: {rmse(y, oof):.5f}")
    return CVResult(oof, test_pred, fold_metrics, best_iters)


def fit_xgb(X: pd.DataFrame, y: pd.Series, X_test: pd.DataFrame, num_cols: List[str]) -> CVResult:
    params = dict(
        n_estimators=7000,
        learning_rate=0.02,
        max_depth=8,
        subsample=0.9,
        colsample_bytree=0.9,
        reg_lambda=1.0,
        reg_alpha=0.0,
        min_child_weight=1.0,
        tree_method="hist",
        random_state=SEED,
        n_jobs=-1,
    )

    num_pipe = Pipeline([("imp", SimpleImputer(strategy="median"))])
    pre = ColumnTransformer([("num", num_pipe, num_cols)], remainder="drop")

    oof_sum = np.zeros(len(X), dtype=float)
    oof_cnt = np.zeros(len(X), dtype=float)
    test_pred = np.zeros(len(X_test), dtype=float)
    fold_metrics = []
    best_iters = []

    for cv_seed in CV_SEEDS:
        kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=cv_seed)
        for fold, (tr_idx, va_idx) in enumerate(kf.split(X, y), 1):
            print(f"\n[XGB] Seed {cv_seed} | Fold {fold}")
            X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]
            y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]

            pre.fit(X_tr, y_tr)
            Xtr = pre.transform(X_tr)
            Xva = pre.transform(X_va)
            Xte = pre.transform(X_test)

            model = XGBRegressor(**params)
            model.fit(
                Xtr, y_tr,
                eval_set=[(Xva, y_va)],
                eval_metric="rmse",
                verbose=False,
                early_stopping_rounds=300,
            )
            best_iter = int(model.best_iteration) if hasattr(model, "best_iteration") else params["n_estimators"]
            best_iters.append(best_iter)

            va_pred = model.predict(Xva, iteration_range=(0, best_iter))
            oof_sum[va_idx] += va_pred
            oof_cnt[va_idx] += 1
            fold_rmse = rmse(y_va, va_pred)
            fold_metrics.append(fold_rmse)
            print(f"[XGB] RMSE: {fold_rmse:.5f}")

            test_pred += model.predict(Xte, iteration_range=(0, best_iter)) / (N_FOLDS * len(CV_SEEDS))

    oof = oof_sum / np.maximum(oof_cnt, 1)
    print(f"[XGB] CV RMSE: {rmse(y, oof):.5f}")
    return CVResult(oof, test_pred, fold_metrics, best_iters)


def fit_cat(X: pd.DataFrame, y: pd.Series, X_test: pd.DataFrame, num_cols: List[str]) -> CVResult:
    params = dict(
        depth=8,
        learning_rate=0.03,
        n_estimators=8000,
        loss_function="RMSE",
        random_seed=SEED,
        l2_leaf_reg=3.0,
        subsample=0.9,
        rsm=0.9,
        verbose=200,
        allow_const_label=True,
    )

    # CatBoost: explicit median imputation
    imp = SimpleImputer(strategy="median")
    X_num = pd.DataFrame(imp.fit_transform(X[num_cols]), columns=num_cols, index=X.index)
    Xte_num = pd.DataFrame(imp.transform(X_test[num_cols]), columns=num_cols, index=X_test.index)

    oof_sum = np.zeros(len(X), dtype=float)
    oof_cnt = np.zeros(len(X), dtype=float)
    test_pred = np.zeros(len(X_test), dtype=float)
    fold_metrics = []
    best_iters = []

    for cv_seed in CV_SEEDS:
        kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=cv_seed)
        for fold, (tr_idx, va_idx) in enumerate(kf.split(X_num, y), 1):
            print(f"\n[CAT] Seed {cv_seed} | Fold {fold}")
            X_tr, X_va = X_num.iloc[tr_idx], X_num.iloc[va_idx]
            y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]

            train_pool = Pool(X_tr, y_tr)
            valid_pool = Pool(X_va, y_va)

            model = CatBoostRegressor(**params)
            model.fit(train_pool, eval_set=valid_pool, use_best_model=True)

            best_iters.append(int(model.tree_count_))
            va_pred = model.predict(valid_pool)
            oof_sum[va_idx] += va_pred
            oof_cnt[va_idx] += 1
            fold_rmse = rmse(y_va, va_pred)
            fold_metrics.append(fold_rmse)
            print(f"[CAT] RMSE: {fold_rmse:.5f}")

            test_pred += model.predict(Xte_num) / (N_FOLDS * len(CV_SEEDS))

    oof = oof_sum / np.maximum(oof_cnt, 1)
    print(f"[CAT] CV RMSE: {rmse(y, oof):.5f}")
    return CVResult(oof, test_pred, fold_metrics, best_iters)
Blending & Calibration
This function looks for the best mix of three model predictions by testing different weight combinations. First, it does a broad search using a coarse grid to find a promising balance of weights that add up to one. Next, it zooms in around that balance with a finer grid to improve precision. In the end, it returns the lowest error score found and the set of weights that achieved it.
In [6]:
# -----------------------------
# Blending & Calibration
# -----------------------------

def two_stage_weight_search(y_true: np.ndarray, oof_mat: np.ndarray) -> Tuple[float, Tuple[float,float,float]]:
    # Stage 1: coarse grid
    best = (np.inf, (1.0, 0.0, 0.0))
    for a in np.linspace(0, 1, 21):
        for b in np.linspace(0, 1-a, int((1-a)/0.05)+1):
            c = 1.0 - a - b
            w = np.array([a, b, c])
            score = rmse(y_true, (oof_mat * w).sum(1))
            if score < best[0]:
                best = (score, (a, b, c))
    # Stage 2: fine grid around best
    a0, b0, c0 = best[1]
    fine = np.arange(-0.05, 0.0501, 0.005)
    best_f = best
    for da in fine:
        for db in fine:
            a = a0 + da; b = b0 + db; c = 1.0 - a - b
            if a < 0 or b < 0 or c < 0: 
                continue
            w = np.array([a, b, c])
            score = rmse(y_true, (oof_mat * w).sum(1))
            if score < best_f[0]:
                best_f = (score, (a, b, c))
    return best_f
Main
This main block runs the entire pipeline from data to submission. It loads the train and test files, checks required columns, and builds engineered features using only training statistics. It selects all engineered columns as inputs, identifies numeric types, and prints dataset shapes. It optionally stabilizes the target with a Yeo–Johnson transform and defines an inverse function to bring predictions back to BPM. It trains LightGBM, XGBoost, and CatBoost with cross-validation, then converts their out-of-fold and test predictions back to the original scale. It computes adaptive clipping bounds from the training target distribution to guard against unrealistic values. It prepares two calibration modes—no calibration and per-model isotonic—and for each mode finds the best blending weights that minimize out-of-fold error. It applies a final isotonic calibration on the blended out-of-fold predictions, picks the mode and weights with the lowest error, and uses that calibrator on the blended test predictions. It clips the final predictions within the adaptive bounds, writes them to submission.csv, and reports the chosen mode and weights.
In [7]:
# -----------------------------
# Main
# -----------------------------

def main():
    # Load
    train = pd.read_csv(os.path.join(DATA_DIR, "train.csv"))
    test  = pd.read_csv(os.path.join(DATA_DIR, "test.csv"))
    _ = pd.read_csv(os.path.join(DATA_DIR, "sample_submission.csv"))

    assert TARGET_COL in train.columns, f"Target column '{TARGET_COL}' not found in train.csv"
    assert ID_COL in train.columns and ID_COL in test.columns, "ID column not found in train/test"

    # Build features (train-only stats used for bins)
    train_fe, test_fe, feature_cols = build_features(train, test, BASE_NUM_COLS)

    X = train_fe[feature_cols].copy()
    y = train_fe[TARGET_COL].astype(float).copy()
    X_test = test_fe[feature_cols].copy()

    # Numeric/categorical split (should be all numeric now)
    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()
    print(f"Train shape: {X.shape} | Test shape: {X_test.shape} | Numeric features: {len(num_cols)}")

    # Optional target transform
    use_target_transform = True
    if use_target_transform:
        pt = PowerTransformer(method="yeo-johnson")
        y_t = pt.fit_transform(y.values.reshape(-1, 1)).ravel()
        y_fit = pd.Series(y_t, index=y.index)
        inv = lambda arr: pt.inverse_transform(arr.reshape(-1,1)).ravel()
    else:
        y_fit = y.copy()
        inv = lambda arr: arr

    # Train models (with CV seeds bagging if CV_SEEDS has >1 seed)
    lgb_cv = fit_lgbm(X, y_fit, X_test, num_cols)
    xgb_cv = fit_xgb(X, y_fit, X_test, num_cols)
    cat_cv = fit_cat(X, y_fit, X_test, num_cols)

    # Inverse-transform predictions back to BPM
    lgb_oof, lgb_test = inv(lgb_cv.oof), inv(lgb_cv.test_pred)
    xgb_oof, xgb_test = inv(xgb_cv.oof), inv(xgb_cv.test_pred)
    cat_oof, cat_test = inv(cat_cv.oof), inv(cat_cv.test_pred)

    print("\nModel CV RMSEs (post-inverse-transform):")
    print(f"  LGBM: {rmse(y, lgb_oof):.5f}")
    print(f"  XGB : {rmse(y, xgb_oof):.5f}")
    print(f"  CAT : {rmse(y, cat_oof):.5f}")

    # Adaptive clipping bounds from train target
    lo = float(np.quantile(y, 0.005))
    hi = float(np.quantile(y, 0.995))
    clip_lo = max(lo, 40.0)
    clip_hi = min(hi, 240.0)

    # Try three calibration modes and pick the best by OOF
    modes = []

    # Mode A: No isotonic
    A_oof = np.vstack([
        np.clip(lgb_oof, clip_lo, clip_hi),
        np.clip(xgb_oof, clip_lo, clip_hi),
        np.clip(cat_oof, clip_lo, clip_hi)
    ]).T
    A_test = np.vstack([
        np.clip(lgb_test, clip_lo, clip_hi),
        np.clip(xgb_test, clip_lo, clip_hi),
        np.clip(cat_test, clip_lo, clip_hi)
    ]).T
    modes.append(("none", A_oof, A_test))

    # Mode B: Per-model isotonic
    iso_lgb = IsotonicRegression(out_of_bounds="clip").fit(lgb_oof, y)
    iso_xgb = IsotonicRegression(out_of_bounds="clip").fit(xgb_oof, y)
    iso_cat = IsotonicRegression(out_of_bounds="clip").fit(cat_oof, y)
    B_oof = np.vstack([
        np.clip(iso_lgb.predict(lgb_oof), clip_lo, clip_hi),
        np.clip(iso_xgb.predict(xgb_oof), clip_lo, clip_hi),
        np.clip(iso_cat.predict(cat_oof), clip_lo, clip_hi)
    ]).T
    B_test = np.vstack([
        np.clip(iso_lgb.predict(lgb_test), clip_lo, clip_hi),
        np.clip(iso_xgb.predict(xgb_test), clip_lo, clip_hi),
        np.clip(iso_cat.predict(cat_test), clip_lo, clip_hi)
    ]).T
    modes.append(("per_model_iso", B_oof, B_test))

    # Evaluate modes with blending; keep the best
    best_global = (np.inf, None, None, None)  # (rmse, weights, test_mat, mode)
    for mode_name, oof_mat, test_mat in modes:
        score1, w1 = two_stage_weight_search(y.values, oof_mat)
        print(f"\n[{mode_name}] Best blend OOF RMSE: {score1:.5f} with weights LGBM={w1[0]:.3f}, XGB={w1[1]:.3f}, CAT={w1[2]:.3f}")

        # Final isotonic on blended predictions (often helps a bit)
        oof_blend = (oof_mat * np.array(w1)).sum(1)
        iso_final = IsotonicRegression(out_of_bounds="clip").fit(oof_blend, y.values)
        oof_blend_iso = iso_final.predict(oof_blend)
        score2 = rmse(y.values, oof_blend_iso)
        print(f"[{mode_name}] After final isotonic: OOF RMSE: {score2:.5f}")

        if score2 < best_global[0]:
            best_global = (score2, w1, (test_mat, iso_final), mode_name)

    best_rmse, best_w, (best_test_mat, best_iso), best_mode = best_global
    print(f"\nChosen mode: {best_mode} | OOF RMSE: {best_rmse:.5f} | Weights: LGBM={best_w[0]:.3f}, XGB={best_w[1]:.3f}, CAT={best_w[2]:.3f}")

    blended_test = (best_test_mat * np.array(best_w)).sum(1)
    blended_test = best_iso.predict(blended_test)
    blended_test = np.clip(blended_test, clip_lo, clip_hi)

    # Save submission
    submission = pd.DataFrame({ID_COL: test_fe[ID_COL], TARGET_COL: blended_test})
    submission.to_csv("submission.csv", index=False)
    print("Saved submission.csv")

if __name__ == "__main__":
    main()
Train shape: (524164, 144) | Test shape: (174722, 144) | Numeric features: 144

[LGBM] Seed 42 | Fold 1
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.485929 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 32252
[LightGBM] [Info] Number of data points in the train set: 419331, number of used features: 144
[LightGBM] [Info] Start training from score 0.000814
Training until validation scores don't improve for 300 rounds
Early stopping, best iteration is:
[37] valid_0's rmse: 0.999099 valid_0's l2: 0.998198
[LGBM] RMSE: 0.99910

[LGBM] Seed 42 | Fold 2
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.509574 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 32252
[LightGBM] [Info] Number of data points in the train set: 419331, number of used features: 144
[LightGBM] [Info] Start training from score 0.000162
Training until validation scores don't improve for 300 rounds
Early stopping, best iteration is:
[45] valid_0's rmse: 1.00058 valid_0's l2: 1.00116
[LGBM] RMSE: 1.00058

[LGBM] Seed 42 | Fold 3
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.444793 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 32252
[LightGBM] [Info] Number of data points in the train set: 419331, number of used features: 144
[LightGBM] [Info] Start training from score -0.000053
Training until validation scores don't improve for 300 rounds
Early stopping, best iteration is:
[99] valid_0's rmse: 1.00206 valid_0's l2: 1.00413
[LGBM] RMSE: 1.00206

[LGBM] Seed 42 | Fold 4
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.433743 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 32252
[LightGBM] [Info] Number of data points in the train set: 419331, number of used features: 144
[LightGBM] [Info] Start training from score -0.000416
Training until validation scores don't improve for 300 rounds
Early stopping, best iteration is:
[74] valid_0's rmse: 0.999001 valid_0's l2: 0.998002
[LGBM] RMSE: 0.99900

[LGBM] Seed 42 | Fold 5
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.489740 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 32252
[LightGBM] [Info] Number of data points in the train set: 419332, number of used features: 144
[LightGBM] [Info] Start training from score -0.000507
Training until validation scores don't improve for 300 rounds
Early stopping, best iteration is:
[52] valid_0's rmse: 0.997665 valid_0's l2: 0.995335
[LGBM] RMSE: 0.99766
[LGBM] CV RMSE: 0.99968

[XGB] Seed 42 | Fold 1
[XGB] RMSE: 0.99916

[XGB] Seed 42 | Fold 2
[XGB] RMSE: 1.00061

[XGB] Seed 42 | Fold 3
[XGB] RMSE: 1.00207

[XGB] Seed 42 | Fold 4
[XGB] RMSE: 0.99917

[XGB] Seed 42 | Fold 5
[XGB] RMSE: 0.99771
[XGB] CV RMSE: 0.99974

[CAT] Seed 42 | Fold 1
0: learn: 1.0001412 test: 0.9992975 best: 0.9992975 (0) total: 375ms remaining: 49m 56s
200: learn: 0.9968004 test: 0.9991411 best: 0.9990888 (85) total: 57s remaining: 36m 51s
400: learn: 0.9941035 test: 0.9992921 best: 0.9990888 (85) total: 1m 51s remaining: 35m 10s
600: learn: 0.9912177 test: 0.9993788 best: 0.9990888 (85) total: 2m 41s remaining: 33m 13s
800: learn: 0.9882570 test: 0.9995764 best: 0.9990888 (85) total: 3m 32s remaining: 31m 50s
1000: learn: 0.9853634 test: 0.9997652 best: 0.9990888 (85) total: 4m 23s remaining: 30m 44s
1200: learn: 0.9825526 test: 0.9999025 best: 0.9990888 (85) total: 5m 14s remaining: 29m 43s
1400: learn: 0.9797359 test: 1.0000640 best: 0.9990888 (85) total: 6m 7s remaining: 28m 49s
1600: learn: 0.9770404 test: 1.0002344 best: 0.9990888 (85) total: 6m 58s remaining: 27m 52s
1800: learn: 0.9743965 test: 1.0004215 best: 0.9990888 (85) total: 7m 50s remaining: 26m 59s
2000: learn: 0.9718145 test: 1.0006000 best: 0.9990888 (85) total: 8m 42s remaining: 26m 7s
2200: learn: 0.9691925 test: 1.0007873 best: 0.9990888 (85) total: 9m 35s remaining: 25m 16s
2400: learn: 0.9666444 test: 1.0009729 best: 0.9990888 (85) total: 10m 27s remaining: 24m 22s
2600: learn: 0.9641941 test: 1.0011303 best: 0.9990888 (85) total: 11m 19s remaining: 23m 30s
2800: learn: 0.9617259 test: 1.0013205 best: 0.9990888 (85) total: 12m 12s remaining: 22m 38s
3000: learn: 0.9592654 test: 1.0014747 best: 0.9990888 (85) total: 13m 6s remaining: 21m 50s
3200: learn: 0.9568231 test: 1.0016437 best: 0.9990888 (85) total: 14m remaining: 21m
3400: learn: 0.9543686 test: 1.0018129 best: 0.9990888 (85) total: 14m 54s remaining: 20m 8s
3600: learn: 0.9519197 test: 1.0019822 best: 0.9990888 (85) total: 15m 48s remaining: 19m 18s
3800: learn: 0.9495975 test: 1.0021237 best: 0.9990888 (85) total: 16m 42s remaining: 18m 27s
4000: learn: 0.9472615 test: 1.0022723 best: 0.9990888 (85) total: 17m 36s remaining: 17m 36s
4200: learn: 0.9449003 test: 1.0024711 best: 0.9990888 (85) total: 18m 30s remaining: 16m 44s
4400: learn: 0.9425688 test: 1.0026083 best: 0.9990888 (85) total: 19m 23s remaining: 15m 51s
4600: learn: 0.9403061 test: 1.0027437 best: 0.9990888 (85) total: 20m 18s remaining: 15m
4800: learn: 0.9380572 test: 1.0028827 best: 0.9990888 (85) total: 21m 12s remaining: 14m 8s
5000: learn: 0.9357630 test: 1.0031019 best: 0.9990888 (85) total: 22m 7s remaining: 13m 15s
5200: learn: 0.9335459 test: 1.0032726 best: 0.9990888 (85) total: 23m 1s remaining: 12m 23s
5400: learn: 0.9312987 test: 1.0034639 best: 0.9990888 (85) total: 23m 56s remaining: 11m 31s
5600: learn: 0.9291158 test: 1.0036185 best: 0.9990888 (85) total: 24m 50s remaining: 10m 38s
5800: learn: 0.9269559 test: 1.0037256 best: 0.9990888 (85) total: 25m 43s remaining: 9m 45s
6000: learn: 0.9248427 test: 1.0038976 best: 0.9990888 (85) total: 26m 36s remaining: 8m 51s
6200: learn: 0.9226645 test: 1.0040503 best: 0.9990888 (85) total: 27m 32s remaining: 7m 59s
6400: learn: 0.9205680 test: 1.0042145 best: 0.9990888 (85) total: 28m 27s remaining: 7m 6s
6600: learn: 0.9184523 test: 1.0043366 best: 0.9990888 (85) total: 29m 22s remaining: 6m 13s
6800: learn: 0.9163092 test: 1.0045176 best: 0.9990888 (85) total: 30m 17s remaining: 5m 20s
7000: learn: 0.9142128 test: 1.0047127 best: 0.9990888 (85) total: 31m 11s remaining: 4m 27s
7200: learn: 0.9121082 test: 1.0048932 best: 0.9990888 (85) total: 32m 6s remaining: 3m 33s
7400: learn: 0.9100610 test: 1.0050571 best: 0.9990888 (85) total: 33m 1s remaining: 2m 40s
7600: learn: 0.9080315 test: 1.0051878 best: 0.9990888 (85) total: 33m 54s remaining: 1m 46s
7800: learn: 0.9059661 test: 1.0053176 best: 0.9990888 (85) total: 34m 50s remaining: 53.3s
7999: learn: 0.9039621 test: 1.0054534 best: 0.9990888 (85) total: 35m 46s remaining: 0us

bestTest = 0.9990888423
bestIteration = 85

Shrink model to first 86 iterations.
[CAT] RMSE: 0.99909

[CAT] Seed 42 | Fold 2
0: learn: 0.9997547 test: 1.0008570 best: 1.0008570 (0) total: 321ms remaining: 42m 44s
200: learn: 0.9965350 test: 1.0006209 best: 1.0005681 (87) total: 56.1s remaining: 36m 18s
400: learn: 0.9938629 test: 1.0007505 best: 1.0005681 (87) total: 1m 51s remaining: 35m 18s
600: learn: 0.9909864 test: 1.0008814 best: 1.0005681 (87) total: 2m 43s remaining: 33m 31s
800: learn: 0.9880008 test: 1.0010393 best: 1.0005681 (87) total: 3m 34s remaining: 32m 11s
1000: learn: 0.9851133 test: 1.0012206 best: 1.0005681 (87) total: 4m 26s remaining: 31m 5s
1200: learn: 0.9822790 test: 1.0013770 best: 1.0005681 (87) total: 5m 19s remaining: 30m 9s
1400: learn: 0.9794812 test: 1.0015601 best: 1.0005681 (87) total: 6m 12s remaining: 29m 14s
1600: learn: 0.9768071 test: 1.0017414 best: 1.0005681 (87) total: 7m 4s remaining: 28m 17s
1800: learn: 0.9742115 test: 1.0018914 best: 1.0005681 (87) total: 7m 58s remaining: 27m 27s
2000: learn: 0.9715825 test: 1.0021358 best: 1.0005681 (87) total: 8m 55s remaining: 26m 45s
2200: learn: 0.9689716 test: 1.0022864 best: 1.0005681 (87) total: 9m 48s remaining: 25m 51s
2400: learn: 0.9664302 test: 1.0024832 best: 1.0005681 (87) total: 10m 42s remaining: 24m 59s
2600: learn: 0.9639222 test: 1.0026625 best: 1.0005681 (87) total: 11m 36s remaining: 24m 5s
2800: learn: 0.9614583 test: 1.0028484 best: 1.0005681 (87) total: 12m 29s remaining: 23m 10s
3000: learn: 0.9590399 test: 1.0030112 best: 1.0005681 (87) total: 13m 24s remaining: 22m 19s
3200: learn: 0.9565831 test: 1.0031795 best: 1.0005681 (87) total: 14m 19s remaining: 21m 29s
3400: learn: 0.9541703 test: 1.0033872 best: 1.0005681 (87) total: 15m 15s remaining: 20m 38s
3600: learn: 0.9518004 test: 1.0035489 best: 1.0005681 (87) total: 16m 11s remaining: 19m 46s
3800: learn: 0.9494240 test: 1.0037295 best: 1.0005681 (87) total: 17m 5s remaining: 18m 52s
4000: learn: 0.9471391 test: 1.0039107 best: 1.0005681 (87) total: 17m 59s remaining: 17m 58s
4200: learn: 0.9447928 test: 1.0041155 best: 1.0005681 (87) total: 18m 53s remaining: 17m 4s
4400: learn: 0.9425358 test: 1.0042828 best: 1.0005681 (87) total: 19m 46s remaining: 16m 10s
4600: learn: 0.9402807 test: 1.0044890 best: 1.0005681 (87) total: 20m 41s remaining: 15m 16s
4800: learn: 0.9380363 test: 1.0046166 best: 1.0005681 (87) total: 21m 36s remaining: 14m 24s
5000: learn: 0.9358758 test: 1.0048326 best: 1.0005681 (87) total: 22m 31s remaining: 13m 30s
5200: learn: 0.9337063 test: 1.0049903 best: 1.0005681 (87) total: 23m 24s remaining: 12m 36s
5400: learn: 0.9315255 test: 1.0051637 best: 1.0005681 (87) total: 24m 19s remaining: 11m 42s
5600: learn: 0.9292987 test: 1.0053252 best: 1.0005681 (87) total: 25m 14s remaining: 10m 48s
5800: learn: 0.9271392 test: 1.0055025 best: 1.0005681 (87) total: 26m 8s remaining: 9m 54s
6000: learn: 0.9249583 test: 1.0057127 best: 1.0005681 (87) total: 27m 2s remaining: 9m
6200: learn: 0.9228162 test: 1.0058807 best: 1.0005681 (87) total: 27m 57s remaining: 8m 6s
6400: learn: 0.9206751 test: 1.0060824 best: 1.0005681 (87) total: 28m 52s remaining: 7m 12s
6600: learn: 0.9185772 test: 1.0062857 best: 1.0005681 (87) total: 29m 47s remaining: 6m 18s
6800: learn: 0.9164771 test: 1.0064948 best: 1.0005681 (87) total: 30m 41s remaining: 5m 24s
7000: learn: 0.9143578 test: 1.0066760 best: 1.0005681 (87) total: 31m 36s remaining: 4m 30s
7200: learn: 0.9122328 test: 1.0068065 best: 1.0005681 (87) total: 32m 31s remaining: 3m 36s
7400: learn: 0.9101433 test: 1.0069245 best: 1.0005681 (87) total: 33m 25s remaining: 2m 42s
7600: learn: 0.9080785 test: 1.0070911 best: 1.0005681 (87) total: 34m 19s remaining: 1m 48s
7800: learn: 0.9060477 test: 1.0072759 best: 1.0005681 (87) total: 35m 15s remaining: 54s
7999: learn: 0.9040218 test: 1.0074249 best: 1.0005681 (87) total: 36m 10s remaining: 0us

bestTest = 1.000568112
bestIteration = 87

Shrink model to first 88 iterations.
[CAT] RMSE: 1.00057

[CAT] Seed 42 | Fold 3
0: learn: 0.9993762 test: 1.0023613 best: 1.0023613 (0) total: 332ms remaining: 44m 11s
200: learn: 0.9960684 test: 1.0021014 best: 1.0020516 (81) total: 55.6s remaining: 35m 57s
400: learn: 0.9933355 test: 1.0022064 best: 1.0020516 (81) total: 1m 51s remaining: 35m 17s
600: learn: 0.9903834 test: 1.0023926 best: 1.0020516 (81) total: 2m 42s remaining: 33m 17s
800: learn: 0.9874704 test: 1.0025995 best: 1.0020516 (81) total: 3m 32s remaining: 31m 51s
1000: learn: 0.9845827 test: 1.0028142 best: 1.0020516 (81) total: 4m 23s remaining: 30m 41s
1200: learn: 0.9817766 test: 1.0029765 best: 1.0020516 (81) total: 5m 15s remaining: 29m 47s
1400: learn: 0.9790278 test: 1.0031122 best: 1.0020516 (81) total: 6m 7s remaining: 28m 53s
1600: learn: 0.9762570 test: 1.0032631 best: 1.0020516 (81) total: 7m remaining: 28m
1800: learn: 0.9735964 test: 1.0034371 best: 1.0020516 (81) total: 7m 53s remaining: 27m 9s
2000: learn: 0.9710081 test: 1.0036131 best: 1.0020516 (81) total: 8m 46s remaining: 26m 17s
2200: learn: 0.9684152 test: 1.0038005 best: 1.0020516 (81) total: 9m 38s remaining: 25m 25s
2400: learn: 0.9658364 test: 1.0039763 best: 1.0020516 (81) total: 10m 31s remaining: 24m 32s
2600: learn: 0.9633040 test: 1.0041923 best: 1.0020516 (81) total: 11m 23s remaining: 23m 39s
2800: learn: 0.9607784 test: 1.0043995 best: 1.0020516 (81) total: 12m 16s remaining: 22m 47s
3000: learn: 0.9583525 test: 1.0045619 best: 1.0020516 (81) total: 13m 9s remaining: 21m 54s
3200: learn: 0.9559113 test: 1.0047773 best: 1.0020516 (81) total: 14m 6s remaining: 21m 9s
3400: learn: 0.9534797 test: 1.0049506 best: 1.0020516 (81) total: 15m 3s remaining: 20m 21s
3600: learn: 0.9510701 test: 1.0050671 best: 1.0020516 (81) total: 15m 56s remaining: 19m 28s
3800: learn: 0.9486955 test: 1.0052426 best: 1.0020516 (81) total: 16m 49s remaining: 18m 35s
4000: learn: 0.9463916 test: 1.0053596 best: 1.0020516 (81) total: 17m 43s remaining: 17m 42s
4200: learn: 0.9441073 test: 1.0054975 best: 1.0020516 (81) total: 18m 36s remaining: 16m 49s
4400: learn: 0.9418067 test: 1.0056307 best: 1.0020516 (81) total: 19m 30s remaining: 15m 56s
4600: learn: 0.9395057 test: 1.0057996 best: 1.0020516 (81) total: 20m 22s remaining: 15m 3s
4800: learn: 0.9372288 test: 1.0059774 best: 1.0020516 (81) total: 21m 16s remaining: 14m 10s
5000: learn: 0.9350276 test: 1.0061849 best: 1.0020516 (81) total: 22m 10s remaining: 13m 17s
5200: learn: 0.9327805 test: 1.0063400 best: 1.0020516 (81) total: 23m 6s remaining: 12m 26s
5400: learn: 0.9305606 test: 1.0064840 best: 1.0020516 (81) total: 24m 3s remaining: 11m 34s
5600: learn: 0.9283486 test: 1.0066729 best: 1.0020516 (81) total: 24m 57s remaining: 10m 41s
5800: learn: 0.9261304 test: 1.0068081 best: 1.0020516 (81) total: 25m 51s remaining: 9m 48s
6000: learn: 0.9239693 test: 1.0069973 best: 1.0020516 (81) total: 26m 45s remaining: 8m 54s
6200: learn: 0.9217624 test: 1.0071303 best: 1.0020516 (81) total: 27m 41s remaining: 8m 2s
6400: learn: 0.9196527 test: 1.0073171 best: 1.0020516 (81) total: 28m 37s remaining: 7m 9s
6600: learn: 0.9174610 test: 1.0074914 best: 1.0020516 (81) total: 29m 33s remaining: 6m 15s
6800: learn: 0.9153405 test: 1.0076408 best: 1.0020516 (81) total: 30m 28s remaining: 5m 22s
7000: learn: 0.9132944 test: 1.0077971 best: 1.0020516 (81) total: 31m 24s remaining: 4m 28s
7200: learn: 0.9111804 test: 1.0079431 best: 1.0020516 (81) total: 32m 20s remaining: 3m 35s
7400: learn: 0.9091067 test: 1.0081092 best: 1.0020516 (81) total: 33m 17s remaining: 2m 41s
7600: learn: 0.9070256 test: 1.0082635 best: 1.0020516 (81) total: 34m 12s remaining: 1m 47s
7800: learn: 0.9049733 test: 1.0084470 best: 1.0020516 (81) total: 35m 7s remaining: 53.8s
7999: learn: 0.9029489 test: 1.0085686 best: 1.0020516 (81) total: 36m 4s remaining: 0us

bestTest = 1.002051592
bestIteration = 81

Shrink model to first 82 iterations.
[CAT] RMSE: 1.00205

[CAT] Seed 42 | Fold 4
0: learn: 1.0000917 test: 0.9994864 best: 0.9994864 (0) total: 328ms remaining: 43m 47s
200: learn: 0.9968774 test: 0.9990840 best: 0.9990733 (136) total: 57.5s remaining: 37m 10s
400: learn: 0.9942226 test: 0.9991655 best: 0.9990733 (136) total: 1m 54s remaining: 36m 16s
600: learn: 0.9913743 test: 0.9992621 best: 0.9990733 (136) total: 2m 47s remaining: 34m 19s
800: learn: 0.9884117 test: 0.9994464 best: 0.9990733 (136) total: 3m 38s remaining: 32m 46s
1000: learn: 0.9855341 test: 0.9996821 best: 0.9990733 (136) total: 4m 30s remaining: 31m 32s
1200: learn: 0.9827186 test: 0.9998581 best: 0.9990733 (136) total: 5m 22s remaining: 30m 28s
1400: learn: 0.9799282 test: 1.0000768 best: 0.9990733 (136) total: 6m 15s remaining: 29m 28s
1600: learn: 0.9772107 test: 1.0002383 best: 0.9990733 (136) total: 7m 8s remaining: 28m 30s
1800: learn: 0.9745863 test: 1.0004429 best: 0.9990733 (136) total: 8m 1s remaining: 27m 36s
2000: learn: 0.9719063 test: 1.0006465 best: 0.9990733 (136) total: 8m 55s remaining: 26m 45s
2200: learn: 0.9693535 test: 1.0008310 best: 0.9990733 (136) total: 9m 49s remaining: 25m 53s
2400: learn: 0.9667584 test: 1.0010293 best: 0.9990733 (136) total: 10m 43s remaining: 25m 1s
2600: learn: 0.9642755 test: 1.0011984 best: 0.9990733 (136) total: 11m 37s remaining: 24m 7s
2800: learn: 0.9618037 test: 1.0013972 best: 0.9990733 (136) total: 12m 31s remaining: 23m 14s
3000: learn: 0.9594018 test: 1.0015898 best: 0.9990733 (136) total: 13m 24s remaining: 22m 19s
3200: learn: 0.9569964 test: 1.0017928 best: 0.9990733 (136) total: 14m 18s remaining: 21m 26s
3400: learn: 0.9546270 test: 1.0019643 best: 0.9990733 (136) total: 15m 11s remaining: 20m 32s
3600: learn: 0.9522442 test: 1.0021169 best: 0.9990733 (136) total: 16m 4s remaining: 19m 38s
3800: learn: 0.9498990 test: 1.0023124 best: 0.9990733 (136) total: 16m 59s remaining: 18m 46s
4000: learn: 0.9474946 test: 1.0025147 best: 0.9990733 (136) total: 17m 53s remaining: 17m 52s
4200: learn: 0.9451684 test: 1.0026874 best: 0.9990733 (136) total: 18m 47s remaining: 16m 59s
4400: learn: 0.9428557 test: 1.0028649 best: 0.9990733 (136) total: 19m 41s remaining: 16m 6s
4600: learn: 0.9405539 test: 1.0030296 best: 0.9990733 (136) total: 20m 36s remaining: 15m 13s
4800: learn: 0.9383343 test: 1.0032151 best: 0.9990733 (136) total: 21m 30s remaining: 14m 19s
5000: learn: 0.9361541 test: 1.0034411 best: 0.9990733 (136) total: 22m 24s remaining: 13m 26s
5200: learn: 0.9339436 test: 1.0036274 best: 0.9990733 (136) total: 23m 20s remaining: 12m 33s
5400: learn: 0.9317459 test: 1.0038024 best: 0.9990733 (136) total: 24m 15s remaining: 11m 40s
5600: learn: 0.9295480 test: 1.0039610 best: 0.9990733 (136) total: 25m 10s remaining: 10m 46s
5800: learn: 0.9273950 test: 1.0041149 best: 0.9990733 (136) total: 26m 5s remaining: 9m 53s
6000: learn: 0.9252641 test: 1.0043208 best: 0.9990733 (136) total: 26m 59s remaining: 8m 59s
6200: learn: 0.9231143 test: 1.0045161 best: 0.9990733 (136) total: 27m 54s remaining: 8m 5s
6400: learn: 0.9209693 test: 1.0047096 best: 0.9990733 (136) total: 28m 52s remaining: 7m 12s
6600: learn: 0.9188179 test: 1.0048448 best: 0.9990733 (136) total: 29m 51s remaining: 6m 19s
6800: learn: 0.9166906 test: 1.0050463 best: 0.9990733 (136) total: 30m 45s remaining: 5m 25s
7000: learn: 0.9146632 test: 1.0052045 best: 0.9990733 (136) total: 31m 43s remaining: 4m 31s
7200: learn: 0.9126387 test: 1.0053679 best: 0.9990733 (136) total: 32m 39s remaining: 3m 37s
7400: learn: 0.9105752 test: 1.0055487 best: 0.9990733 (136) total: 33m 33s remaining: 2m 42s
7600: learn: 0.9085269 test: 1.0057185 best: 0.9990733 (136) total: 34m 27s remaining: 1m 48s
7800: learn: 0.9065044 test: 1.0058409 best: 0.9990733 (136) total: 35m 24s remaining: 54.2s
7999: learn: 0.9044723 test: 1.0060213 best: 0.9990733 (136) total: 36m 23s remaining: 0us

bestTest = 0.9990733178
bestIteration = 136

Shrink model to first 137 iterations.
[CAT] RMSE: 0.99907

[CAT] Seed 42 | Fold 5
0: learn: 1.0004843 test: 0.9979265 best: 0.9979265 (0) total: 332ms remaining: 44m 14s
200: learn: 0.9971689 test: 0.9976640 best: 0.9976537 (151) total: 57.7s remaining: 37m 19s
400: learn: 0.9945030 test: 0.9977746 best: 0.9976537 (151) total: 1m 54s remaining: 36m 1s
600: learn: 0.9915910 test: 0.9979075 best: 0.9976537 (151) total: 2m 46s remaining: 34m 8s
800: learn: 0.9886446 test: 0.9980771 best: 0.9976537 (151) total: 3m 38s remaining: 32m 42s
1000: learn: 0.9857577 test: 0.9982896 best: 0.9976537 (151) total: 4m 31s remaining: 31m 39s
1200: learn: 0.9829642 test: 0.9984443 best: 0.9976537 (151) total: 5m 24s remaining: 30m 38s
1400: learn: 0.9802065 test: 0.9985919 best: 0.9976537 (151) total: 6m 18s remaining: 29m 42s
1600: learn: 0.9775155 test: 0.9987802 best: 0.9976537 (151) total: 7m 12s remaining: 28m 48s
1800: learn: 0.9748594 test: 0.9989722 best: 0.9976537 (151) total: 8m 7s remaining: 27m 56s
2000: learn: 0.9722490 test: 0.9991683 best: 0.9976537 (151) total: 9m 2s remaining: 27m 6s
2200: learn: 0.9697075 test: 0.9993219 best: 0.9976537 (151) total: 9m 56s remaining: 26m 10s
2400: learn: 0.9671948 test: 0.9995206 best: 0.9976537 (151) total: 10m 49s remaining: 25m 13s
2600: learn: 0.9646365 test: 0.9997507 best: 0.9976537 (151) total: 11m 43s remaining: 24m 20s
2800: learn: 0.9620813 test: 0.9999352 best: 0.9976537 (151) total: 12m 38s remaining: 23m 27s
3000: learn: 0.9596630 test: 1.0001244 best: 0.9976537 (151) total: 13m 31s remaining: 22m 32s
3200: learn: 0.9572216 test: 1.0003550 best: 0.9976537 (151) total: 14m 25s remaining: 21m 37s
3400: learn: 0.9548576 test: 1.0005246 best: 0.9976537 (151) total: 15m 20s remaining: 20m 44s
3600: learn: 0.9524897 test: 1.0007000 best: 0.9976537 (151) total: 16m 14s remaining: 19m 50s
3800: learn: 0.9501173 test: 1.0008819 best: 0.9976537 (151) total: 17m 8s remaining: 18m 56s
4000: learn: 0.9477957 test: 1.0010517 best: 0.9976537 (151) total: 18m 2s remaining: 18m 2s
4200: learn: 0.9454599 test: 1.0012389 best: 0.9976537 (151) total: 18m 56s remaining: 17m 8s
4400: learn: 0.9431450 test: 1.0014447 best: 0.9976537 (151) total: 19m 50s remaining: 16m 13s
4600: learn: 0.9408274 test: 1.0016243 best: 0.9976537 (151) total: 20m 43s remaining: 15m 18s
4800: learn: 0.9385965 test: 1.0018138 best: 0.9976537 (151) total: 21m 37s remaining: 14m 24s
5000: learn: 0.9362931 test: 1.0019966 best: 0.9976537 (151) total: 22m 31s remaining: 13m 30s
5200: learn: 0.9340670 test: 1.0021343 best: 0.9976537 (151) total: 23m 24s remaining: 12m 36s
5400: learn: 0.9319136 test: 1.0023127 best: 0.9976537 (151) total: 24m 18s remaining: 11m 41s
5600: learn: 0.9296959 test: 1.0025032 best: 0.9976537 (151) total: 25m 12s remaining: 10m 47s
5800: learn: 0.9275891 test: 1.0026829 best: 0.9976537 (151) total: 26m 6s remaining: 9m 53s
6000: learn: 0.9254585 test: 1.0028591 best: 0.9976537 (151) total: 27m remaining: 8m 59s
6200: learn: 0.9233588 test: 1.0029729 best: 0.9976537 (151) total: 27m 54s remaining: 8m 5s
6400: learn: 0.9212636 test: 1.0031299 best: 0.9976537 (151) total: 28m 47s remaining: 7m 11s
6600: learn: 0.9191404 test: 1.0032877 best: 0.9976537 (151) total: 29m 40s remaining: 6m 17s
6800: learn: 0.9170502 test: 1.0034138 best: 0.9976537 (151) total: 30m 33s remaining: 5m 23s
7000: learn: 0.9149597 test: 1.0035825 best: 0.9976537 (151) total: 31m 27s remaining: 4m 29s
7200: learn: 0.9129051 test: 1.0037190 best: 0.9976537 (151) total: 32m 21s remaining: 3m 35s
7400: learn: 0.9109098 test: 1.0038500 best: 0.9976537 (151) total: 33m 14s remaining: 2m 41s
7600: learn: 0.9088767 test: 1.0040155 best: 0.9976537 (151) total: 34m 8s remaining: 1m 47s
7800: learn: 0.9068278 test: 1.0041723 best: 0.9976537 (151) total: 35m 3s remaining: 53.6s
7999: learn: 0.9048277 test: 1.0043345 best: 0.9976537 (151) total: 35m 57s remaining: 0us

bestTest = 0.9976537105
bestIteration = 151

Shrink model to first 152 iterations.
[CAT] RMSE: 0.99765
[CAT] CV RMSE: 0.99969

Model CV RMSEs (post-inverse-transform):
  LGBM: 26.46272
  XGB : 26.46430
  CAT : 26.46284

[none] Best blend OOF RMSE: 26.46182 with weights LGBM=0.500, XGB=0.000, CAT=0.500
[none] After final isotonic: OOF RMSE: 26.45622

[per_model_iso] Best blend OOF RMSE: 26.45619 with weights LGBM=0.485, XGB=0.085, CAT=0.430
[per_model_iso] After final isotonic: OOF RMSE: 26.45482

Chosen mode: per_model_iso | OOF RMSE: 26.45482 | Weights: LGBM=0.485, XGB=0.085, CAT=0.430
Saved submission.csv