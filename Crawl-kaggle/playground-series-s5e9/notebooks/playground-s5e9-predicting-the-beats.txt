unfold_moreShow hidden cell
ðŸŽ¼ Playground S5E9 | Predicting the Beats EDA ðŸŽµ
Table of Content:
1.Data Loading & Overview
2.Missing Values & Data Quality
Target Variable Analysis (Distribution)
Feature Distributions
Correlation Analysis
Feature vs Target Relationships
Outlier Detection (Boxplots)
Feature Engineering Ideas
XGBoost Model
Evaluate Best Model
Feature Importance
In [2]:
# =====================================
# 1. Setup
# =====================================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Settings
pd.set_option('display.max_columns', None)
sns.set(style="whitegrid", palette="muted", font_scale=1.1)

#test  = pd.read_csv("/kaggle/input/playground-series-s5e8/test.csv")
df = pd.read_csv("/kaggle/input/playground-series-s5e9/train.csv")  
print("Shape of dataset:", df.shape)

# Quick look
df.head()
Shape of dataset: (524164, 11)
Out[2]:
id RhythmScore AudioLoudness VocalContent AcousticQuality InstrumentalScore LivePerformanceLikelihood MoodScore TrackDurationMs Energy BeatsPerMinute
0 0 0.603610 -7.636942 0.023500 0.000005 0.000001 0.051385 0.409866 290715.6450 0.826267 147.53020
1 1 0.639451 -16.267598 0.071520 0.444929 0.349414 0.170522 0.651010 164519.5174 0.145400 136.15963
2 2 0.514538 -15.953575 0.110715 0.173699 0.453814 0.029576 0.423865 174495.5667 0.624667 55.31989
3 3 0.734463 -1.357000 0.052965 0.001651 0.159717 0.086366 0.278745 225567.4651 0.487467 147.91212
4 4 0.532968 -13.056437 0.023500 0.068687 0.000001 0.331345 0.477769 213960.6789 0.947333 89.58511
Observation:
Dataset has (524164, 11) shape.
Most columns are numerical.
In [3]:
# =====================================
# 2. Basic Info & Data Quality
# =====================================
df.info()
df.describe(include='all')

# Missing values
missing = df.isnull().sum()
missing = missing[missing > 0].sort_values(ascending=False)
if not missing.empty:
    plt.figure(figsize=(10, 6))
    missing.plot(kind='bar')
    plt.title("Missing Values by Feature")
    plt.show()
else:
    print("âœ… No missing values found.")

# Duplicates
print("Duplicate rows:", df.duplicated().sum())
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 524164 entries, 0 to 524163
Data columns (total 11 columns):
 #   Column                     Non-Null Count   Dtype  
---  ------                     --------------   -----  
 0   id                         524164 non-null  int64  
 1   RhythmScore                524164 non-null  float64
 2   AudioLoudness              524164 non-null  float64
 3   VocalContent               524164 non-null  float64
 4   AcousticQuality            524164 non-null  float64
 5   InstrumentalScore          524164 non-null  float64
 6   LivePerformanceLikelihood  524164 non-null  float64
 7   MoodScore                  524164 non-null  float64
 8   TrackDurationMs            524164 non-null  float64
 9   Energy                     524164 non-null  float64
 10  BeatsPerMinute             524164 non-null  float64
dtypes: float64(10), int64(1)
memory usage: 44.0 MB
âœ… No missing values found.
Duplicate rows: 0
Observation:
No. of missing values is checked.
Data types are appropriate, no need of conversion
In [4]:
print("\n--- Descriptive Statistics ---")
print(df.describe().T)
--- Descriptive Statistics ---
                              count           mean            std  \
id                         524164.0  262081.500000  151313.257586   
RhythmScore                524164.0       0.632843       0.156899   
AudioLoudness              524164.0      -8.379014       4.616221   
VocalContent               524164.0       0.074443       0.049939   
AcousticQuality            524164.0       0.262913       0.223120   
InstrumentalScore          524164.0       0.117690       0.131845   
LivePerformanceLikelihood  524164.0       0.178398       0.118186   
MoodScore                  524164.0       0.555843       0.225480   
TrackDurationMs            524164.0  241903.692949   59326.601501   
Energy                     524164.0       0.500923       0.289952   
BeatsPerMinute             524164.0     119.034899      26.468077   

                                    min            25%            50%  \
id                             0.000000  131040.750000  262081.500000   
RhythmScore                    0.076900       0.515850       0.634686   
AudioLoudness                -27.509725     -11.551933      -8.252499   
VocalContent                   0.023500       0.023500       0.066425   
AcousticQuality                0.000005       0.069413       0.242502   
InstrumentalScore              0.000001       0.000001       0.074247   
LivePerformanceLikelihood      0.024300       0.077637       0.166327   
MoodScore                      0.025600       0.403921       0.564817   
TrackDurationMs            63973.000000  207099.876625  243684.058150   
Energy                         0.000067       0.254933       0.511800   
BeatsPerMinute                46.718000     101.070410     118.747660   

                                     75%            max  
id                         393122.250000  524163.000000  
RhythmScore                     0.739179       0.975000  
AudioLoudness                  -4.912298      -1.357000  
VocalContent                    0.107343       0.256401  
AcousticQuality                 0.396957       0.995000  
InstrumentalScore               0.204065       0.869258  
LivePerformanceLikelihood       0.268946       0.599924  
MoodScore                       0.716633       0.978000  
TrackDurationMs            281851.658500  464723.228100  
Energy                          0.746000       1.000000  
BeatsPerMinute                136.686590     206.037000  
In [5]:
# ========================
# 3. Target Variable Analysis (Target: BeatsPerMinute)
# ========================
target = "BeatsPerMinute"

plt.figure(figsize=(8,5))
sns.histplot(df[target], kde=True, bins=30)
plt.title(f"Distribution of {target}")
plt.show()
/usr/local/lib/python3.11/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
  with pd.option_context('mode.use_inf_as_na', True):
Observations:
The distribution looks approximately normal (bell-shaped), centered around 120 BPM.
In [6]:
# ========================
# 4. Feature Distributions
# ========================
num_features = df.select_dtypes(include=[np.number]).columns.tolist()
num_features.remove("id") # remove id if present

# Plot histograms for all numeric features
df[num_features].hist(bins=30, figsize=(15,12), layout=(4,3))
plt.suptitle("Feature Distributions")
plt.show()
Observations
RhythmScore: Bell-shaped, centered around 0.5â€“0.7.
AudioLoudness: Ranges between -25 dB to -5 dB and Looks slightly bimodal because it has two peaks.
VocalContent: Strongly right-skewed, most values clustered below 0.1.
AcousticQuality: Heavy skew towards 0 with a few high outliers.
InstrumentalScore: Mostly near 0, very few tracks have high instrumental score.
LivePerformanceLikelihood: right skewed, clustered near 0.05.
MoodScore: Distribution concentrated around 0â€“0.2, with some spread up to 1.
TrackDurationMs: binomial graph and Centered between 150kâ€“300k ms (~2.5â€“5 min).
Energy: Uniform-like distribution across 0â€“1.
BeatsPerMinute (Target): Nearly normal, centered around 120 BPM.
In [7]:
# ========================
# 5. Correlation Analysis
# ========================
plt.figure(figsize=(10,8))
sns.heatmap(df[num_features].corr(), annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()

# Correlation with target
print("\n--- Correlation with Target ---")
print(df.corr()[target].sort_values(ascending=False))
--- Correlation with Target ---
BeatsPerMinute               1.000000
MoodScore                    0.007059
TrackDurationMs              0.006637
RhythmScore                  0.005440
VocalContent                 0.004876
LivePerformanceLikelihood    0.003471
InstrumentalScore            0.001900
id                          -0.000355
AcousticQuality             -0.000820
AudioLoudness               -0.003327
Energy                      -0.004375
Name: BeatsPerMinute, dtype: float64
Observations:
very weak correlation with all features.
Other Feature Correlations (Between features) Energy & AcousticQuality: -0.42 (negative) Energy & AudioLoudness: +0.19 (weak positive) LivePerformanceLikelihood & Energy: -0.27 (negative) MoodScore & Energy: -0.24 (negative)
Since BeatsPerMinute has weak linear correlations, a simple linear regression may not explain much variance. better suited for non-linear regression approaches (like XGBoost) rather than plain linear regression.
In [8]:
# ========================
# 6. Feature vs Target Relationships
# ========================
for col in num_features:
    if col != target:
        plt.figure(figsize=(6,4))
        sns.scatterplot(x=df[col], y=df[target])
        plt.title(f"{col} vs {target}")
        plt.show()
In [9]:
# ========================
# 7. Outlier Detection (Boxplots)
# ========================
for col in num_features:
    plt.figure(figsize=(6,4))
    sns.boxplot(x=df[col])
    plt.title(f"Outliers in {col}")
    plt.show()
In [10]:
#df['RhythmScore'] = np.where(df['RhythmScore'] < lower, lower,
#                             np.where(df['RhythmScore'] > upper, upper, df['RhythmScore']))
#df['AudioLoudness'] = np.where(df['AudioLoudness'] < lower, lower,
#                             np.where(df['AudioLoudness'] > upper, upper, df['AudioLoudness']))

features = ['RhythmScore', 'AudioLoudness', 'VocalContent', 'AcousticQuality']

for col in features:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    
    # Winsorization (capping)
    df[col] = np.where(df[col] < lower, lower,
                       np.where(df[col] > upper, upper, df[col]))
In [11]:
# ========================
# 8. Feature Engineering Ideas
# ========================
# Example: Convert TrackDurationMs to minutes
df['TrackDurationMin'] = df['TrackDurationMs'] / 60000
df['Energy_Acoustic_Ratio'] = df['Energy'] / (df['AcousticQuality'] + 1e-5)
df['Vocal_Instrument_Balance'] = df['VocalContent'] / (df['InstrumentalScore'] + 1e-5)
df['MoodRhythm'] = df['MoodScore'] * df['RhythmScore']
df['PerformanceIntensity'] = df['LivePerformanceLikelihood'] * df['AudioLoudness']
df['RhythmEnergy'] = df['RhythmScore'] * df['Energy']
df['MoodAcoustic'] = df['MoodScore'] * df['AcousticQuality']
In [12]:
# ========================
# 8. XGBoost Model
# ========================
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from xgboost import XGBRegressor
X = df.drop(columns=['id', target])
y = df[target]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

param_dist = {
    'max_depth': [3, 7, 10],
    'learning_rate': [0.01, 0.05, 0.1],
    'n_estimators': [200, 500],
    'subsample': [0.7, 1.0],
    'colsample_bytree': [0.7, 1.0]
}

xgb = XGBRegressor(random_state=42)

random_search = RandomizedSearchCV(
estimator=xgb,
param_distributions=param_dist,
n_iter=20,
scoring='neg_mean_squared_error',
cv=3,
verbose=1,
n_jobs=-1,
random_state=42
)

random_search.fit(X_train, y_train)

print("Best parameters:", random_search.best_params_)
Fitting 3 folds for each of 20 candidates, totalling 60 fits
/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  warnings.warn(
Best parameters: {'subsample': 0.7, 'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 0.7}
In [13]:
# ========================
# 10. Evaluate Best Model
# ========================
best_model = random_search.best_estimator_
y_pred = best_model.predict(X_test)

print("RMSE:", mean_squared_error(y_test, y_pred, squared=False))
print("RÂ² Score:", r2_score(y_test, y_pred))
RMSE: 26.439905665876683
RÂ² Score: 0.0003954805892428803
In [14]:
# ========================
# 11. Feature Importance
# ========================
plt.figure(figsize=(10,6))
plt.barh(X.columns, best_model.feature_importances_)
plt.title("XGBoost Feature Importance")
plt.show()
In [15]:
import pandas as pd

# ========================
# 1. Load Test Data
# ========================
test_df = pd.read_csv("/kaggle/input/playground-series-s5e9/test.csv")

# ========================
# 2. Apply Same Feature Engineering
# ========================
test_df['TrackDurationMin'] = test_df['TrackDurationMs'] / 60000
test_df['Energy_Acoustic_Ratio'] = test_df['Energy'] / (test_df['AcousticQuality'] + 1e-5)
test_df['Vocal_Instrument_Balance'] = test_df['VocalContent'] / (test_df['InstrumentalScore'] + 1e-5)
test_df['MoodRhythm'] = test_df['MoodScore'] * test_df['RhythmScore']
test_df['PerformanceIntensity'] = test_df['LivePerformanceLikelihood'] * test_df['AudioLoudness']
test_df['RhythmEnergy'] = test_df['RhythmScore'] * test_df['Energy']
test_df['MoodAcoustic'] = test_df['MoodScore'] * test_df['AcousticQuality']

# ========================
# 3. Ensure Consistent Features
# ========================
train_features = best_model.get_booster().feature_names  # features used in training

X_test_final = test_df[train_features]  # select only training features (drop id automatically)

# ========================
# 4. Predict with Best Model
# ========================
y_pred_test = best_model.predict(X_test_final)

# ========================
# 5. Save Predictions
# ========================
output = pd.DataFrame({
    "id": test_df["id"],
    "Predicted_BPM": y_pred_test
})

output.to_csv("test_predictions.csv", index=False)

print("Predictions saved to test_predictions.csv")
print(output.head())
Predictions saved to test_predictions.csv
       id  Predicted_BPM
0  524164     118.924332
1  524165     118.963158
2  524166     119.379143
3  524167     119.383461
4  524168     119.364464