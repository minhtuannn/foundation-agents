In [1]:
import os
import pandas as pd
import polars as pl
import numpy as np
from pathlib import Path
from gc import collect 
from tqdm import tqdm
from dataclasses import dataclass, asdict
from scipy.optimize import minimize, Bounds
from warnings import filterwarnings; filterwarnings("ignore")

from sklearn.linear_model import ElasticNet, ElasticNetCV, LinearRegression
from sklearn.linear_model import RidgeCV

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.ensemble import StackingRegressor

from catboost import CatBoostRegressor, Pool
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler

import kaggle_evaluation.default_inference_server
Model_1
unfold_moreShow hidden code
Model_3
In [3]:
train = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/train.csv').dropna()
test = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/test.csv').dropna()

def preprocessing(data, typ):
    main_feature = ['E1','E10', 'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19',
                    'E2', 'E20', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9',
                    "S2", "P9", "S1", "S5", "I2", "P8",
                    "P10", "P12", "P13",]
    
    if typ == "train":
        data = data[main_feature + ["forward_returns"]]
    else:
        data = data[main_feature]
    for i in zip(data.columns, data.dtypes):
        data[i[0]].fillna(0, inplace=True)

    return data

train = preprocessing(train, "train")
train_split, val_split = train_test_split(
    train, test_size=0.01, random_state=42
)

X_train = train_split.drop(columns=["forward_returns"])
X_test = val_split.drop(columns=["forward_returns"])
y_train = train_split['forward_returns']
y_test = val_split['forward_returns']

improved_catboost_params = {'iterations': 3000,
                            'learning_rate': 0.01,
                            'depth': 6,
                            'l2_leaf_reg': 5.0,
                            'min_child_samples': 100,
                            'colsample_bylevel': 0.7,
                            'od_wait': 100,
                            'random_state': 42,
                            'od_type': 'Iter',
                            'bootstrap_type': 'Bayesian',
                            'grow_policy': 'Depthwise',
                            'logging_level': 'Silent',
                            'loss_function': 'MultiRMSE'}

R_Forest_parm = {'n_estimators': 100,
                 'min_samples_split': 5,
                 'max_depth': 15,
                 'min_samples_leaf': 3,
                 'max_features': 'sqrt',
                 'random_state': 42}
        
Extra_parm = {'n_estimators': 100,
              'min_samples_split': 5,
              'max_depth': 12,
              'min_samples_leaf': 3,
              'max_features': 'sqrt',
              'random_state': 42}
        
XGB_R_parm = {"n_estimators": 1500,
              "learning_rate": 0.05, 
              "max_depth": 6,
              "subsample": 0.8, 
              "colsample_bytree": 0.7,
              "reg_alpha": 1.0,
              "reg_lambda": 1.0,
              "random_state": 42}

LGBM_R_parm = {"n_estimators": 1500,
               "learning_rate": 0.05,
               "num_leaves": 50,
               "max_depth": 8,
               "reg_alpha": 1.0,
               "reg_lambda": 1.0,
               "random_state": 42,
               'verbosity': -1}

DecisionTree = {'criterion': 'poisson',
                'max_depth': 6}

GB_parm = {"learning_rate": 0.1,
           "min_samples_split": 500,
           "min_samples_leaf": 50,
           "max_depth": 8,
           "max_features": 'sqrt',
           "subsample": 0.8,
           "random_state": 10}

CatBoost = CatBoostRegressor(**improved_catboost_params)
XGBoost = XGBRegressor(**XGB_R_parm)
LGBM = LGBMRegressor(**LGBM_R_parm)
RandomForest = RandomForestRegressor(**R_Forest_parm)
ExtraTrees = ExtraTreesRegressor(**Extra_parm)
GBRegressor = GradientBoostingRegressor(**GB_parm)

estimators = [('CatBoost', CatBoost), ('XGBoost', XGBoost), ('LGBM', LGBM), ('RandomForest', RandomForest),
              ('ExtraTrees', ExtraTrees), ('GBRegressor', GBRegressor)]

model_3 = StackingRegressor(estimators, 
                          final_estimator = RidgeCV(alphas=[0.1, 1.0, 10.0, 100.0]), 
                          cv=3)
model_3.fit(X_train, y_train)
Out[3]:

StackingRegressor
CatBoost
CatBoostRegressor
XGBoost
XGBRegressor
LGBM
LGBMRegressor
RandomForest
RandomForestRegressor
ExtraTrees
ExtraTreesRegressor
GBRegressor
GradientBoostingRegressor
final_estimator
RidgeCV
Model_4
unfold_moreShow hidden code
Model_5
unfold_moreShow hidden code
Model_6
unfold_moreShow hidden code
Model_7
unfold_moreShow hidden cell
Model_2
In [8]:
train = pl.read_csv("/kaggle/input/hull-tactical-market-prediction/train.csv")
display(train)
test = pl.read_csv("/kaggle/input/hull-tactical-market-prediction/test.csv")
display(test)

MIN_SIGNAL:        float = 0.0                  # Minimum value for the daily signal 
MAX_SIGNAL:        float = 2.0                  # Maximum value for the daily signal 
SIGNAL_MULTIPLIER: float = 400.0                # Multiplier of the OLS market forward excess returns predictions to signal 

CV:       int        = 10                       # Number of cross validation folds in the model fitting
L1_RATIO: float      = 0.5                      # ElasticNet mixing parameter
ALPHAS:   np.ndarray = np.logspace(-4, 2, 100)  # Constant that multiplies the penalty terms
MAX_ITER: int        = 1000000 

@dataclass(frozen=True)
class RetToSignalParameters:
    signal_multiplier: float 
    min_signal : float = MIN_SIGNAL
    max_signal : float = MAX_SIGNAL
    
ret_signal_params = RetToSignalParameters ( signal_multiplier= SIGNAL_MULTIPLIER )
shape: (8_990, 98)
date_id D1 D2 D3 D4 D5 D6 D7 D8 D9 E1 E10 E11 E12 E13 E14 E15 E16 E17 E18 E19 E2 E20 E3 E4 E5 E6 E7 E8 E9 I1 I2 I3 I4 I5 I6 I7 … P13 P2 P3 P4 P5 P6 P7 P8 P9 S1 S10 S11 S12 S2 S3 S4 S5 S6 S7 S8 S9 V1 V10 V11 V12 V13 V2 V3 V4 V5 V6 V7 V8 V9 forward_returns risk_free_rate market_forward_excess_returns
i64 i64 i64 i64 i64 i64 i64 i64 i64 i64 str str str str str str str str str str str str str str str str str str str str str str str str str str str … str str str str str str str str str str str str str str str str str str str str str str str str str str str str str str str str str str f64 f64 f64
0 0 0 0 1 1 0 0 0 1 null null null null null null null null null null null null null null null null null null null null null null null null null null null … null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null -0.002421 0.000301 -0.003038
1 0 0 0 1 1 0 0 0 1 null null null null null null null null null null null null null null null null null null null null null null null null null null null … null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null -0.008495 0.000303 -0.009114
2 0 0 0 1 0 0 0 0 1 null null null null null null null null null null null null null null null null null null null null null null null null null null null … null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null -0.009624 0.000301 -0.010243
3 0 0 0 1 0 0 0 0 0 null null null null null null null null null null null null null null null null null null null null null null null null null null null … null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null 0.004662 0.000299 0.004046
4 0 0 0 1 0 0 0 0 0 null null null null null null null null null null null null null null null null null null null null null null null null null null null … null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null null -0.011686 0.000299 -0.012301
… … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … … …
8985 0 0 0 0 0 0 0 0 0 "1.56537850833686" "0.18452380952381" "0.0191798941798942" "0.0191798941798942" "0.00595238095238095" "0.00595238095238095" "0.911375661375661" "-0.0834957603011584" "-0.572446603148107" "0.22363818831577" "-0.122313603525027" "1.20925014422219" "1.54011631243565" "1.6551743632761" "0.0314153439153439" "0.331679894179894" "0.0347222222222222" "0.0382692660297448" "-0.301875981317616" "0.91468253968254" "0.274140211640212" "0.984115038349353" "0.0806878306878307" "0.476521164021164" "0.597442456305455" "0.718253968253968" "0.238756613756614" … "0.625330687830688" "-1.35449847084931" "0.0462962962962963" "0.514550264550265" "0.276768769975892" "-0.261325600057626" "0.811753887240293" "1.78492936574625" "0.0396825396825397" "0.249933173088953" "0.273148148148148" "0.134920634920635" "0.634464741288425" "-0.446681908068479" "-0.0526855763278288" "0.083994708994709" "0.055281954303961" "0.209656084656085" "0.409391534391534" "0.574660952369144" "0.748677248677249" "0.498677248677249" "-0.616394800009832" "0.561838624338624" "0.533730158730159" "-0.432282453978163" "0.78505291005291" "0.46957671957672" "0.837962962962963" "1.22677167174681" "0.822751322751323" "-0.707360636419722" "0.142857142857143" "-0.649616421794573" 0.002457 0.000155 0.00199
8986 0 0 0 0 0 0 0 0 0 "1.56294570736285" "0.184193121693122" "0.0188492063492063" "0.0188492063492063" "0.00562169312169312" "0.00562169312169312" "0.911706349206349" "-0.0835423385235207" "-0.572080270433783" "0.222909704235328" "-0.732396949860551" "1.22545909429746" "1.53776136440687" "1.67226210824883" "0.0310846560846561" "0.331349206349206" "0.0343915343915344" "0.0382047677964104" "-0.301897014182979" "0.915013227513228" "0.26984126984127" "0.904452962662242" "0.0734126984126984" "0.479166666666667" "0.605078681561105" "0.718253968253968" "0.220899470899471" … "0.739417989417989" "-1.38478508824334" "0.232142857142857" "0.379298941798942" "1.19925967661965" "-0.344273917030356" "0.690323485609809" "1.79159569953088" "0.037037037037037" "0.298532581791712" "0.933201058201058" "0.721560846560847" "1.21134501564263" "-0.118050110853058" "-0.249315284179041" "0.566798941798942" "0.107330113648913" "0.228174603174603" "0.409391534391534" "0.580932017246943" "0.37037037037037" "0.528439153439153" "-0.642039876784022" "0.587632275132275" "0.526455026455027" "-0.429505792996239" "0.767857142857143" "0.671957671957672" "0.837962962962963" "0.785876680219954" "0.805555555555556" "-0.715692186942146" "0.196097883597884" "-0.668289260803376" 0.002312 0.000156 0.001845
8987 0 0 1 0 0 0 0 0 0 "1.5605200537219" "0.183862433862434" "0.0185185185185185" "0.0185185185185185" "0.00529100529100529" "0.00529100529100529" "0.912037037037037" "-0.0838740749780047" "-0.572016205010642" "0.222211305740983" "-0.800464708548532" "1.24727312781993" "1.53474183594455" "1.69546884570455" "0.0307539682539683" "0.331018518518519" "0.0340608465608466" "0.038118211270212" "-0.301918047426536" "0.915343915343915" "0.273148148148148" "0.842294693008398" "0.0740740740740741" "0.478835978835979" "0.611319101072675" "0.724867724867725" "0.223544973544974" … "0.809193121693122" "-1.42000677294771" "0.849867724867725" "0.375661375661376" "0.429471475390351" "-0.233373569970158" "-0.289766464491947" "1.79281602958001" "0.041005291005291" "0.371361679434561" "0.793650793650794" "0.689814814814815" "0.885178015455313" "-0.316882407040784" "-0.422374286909508" "0.631613756613757" "-0.0297695460919471" "0.221891534391534" "0.409391534391534" "0.583555500826733" "0.477513227513228" "0.599206349206349" "-0.638658197334489" "0.39484126984127" "0.433531746031746" "-0.425462111645349" "0.734126984126984" "0.481481481481481" "0.787698412698413" "0.834897865394715" "0.823412698412698" "-0.723948535462705" "0.133928571428571" "-0.67094613354537" 0.002891 0.000156 0.002424
8988 0 0 0 0 0 0 0 0 0 "1.55810150804271" "0.183531746031746" "0.0181878306878307" "0.0181878306878307" "0.00496031746031746" "0.00496031746031746" "0.912367724867725" "-0.0842058914414559" "-0.571952144722391" "0.22151267935636" "-0.596939053960628" "1.27192582796906" "1.53234020642116" "1.7216916162139" "0.0304232804232804" "0.330687830687831" "0.0337301587301587" "0.0376470782051439" "-0.301939081048321" "0.915674603174603" "0.27281746031746" "0.858581646251435" "0.082010582010582" "0.478505291005291" "0.604658106925698" "0.71957671957672" "0.262566137566138" … "0.923611111111111" "-1.43102829482739" "0.303240740740741" "0.0687830687830688" "0.0448883973623847" "-0.26986245435381" "0.423267529815228" "1.79293416392505" "0.046957671957672" "0.411609523901239" "0.0119047619047619" "0.0264550264550265" "-0.00178527792358688" "-0.31796113525985" "-0.608347743326619" "0.0661375661375661" "-0.00159373754975138" "0.259920634920635" "0.409391534391534" "0.630089662276381" "0.915343915343915" "0.462301587301587" "-0.626926847514841" "0.326388888888889" "0.394179894179894" "-0.385169943772323" "0.69510582010582" "0.65542328042328" "0.783730158730159" "0.9940257708608" "0.851851851851852" "-0.684937261881957" "0.101851851851852" "-0.646264996301655" 0.00831 0.000156 0.007843
8989 0 0 0 0 0 0 0 0 0 "1.55569003125934" "0.183201058201058" "0.0178571428571429" "0.0178571428571429" "0.00462962962962963" "0.00462962962962963" "0.912698412698413" "-0.0845377880574921" "-0.571888089567891" "0.22081382366098" "-0.74706627431364" "1.31262576944327" "1.52987532972841" "1.76531662714271" "0.0300925925925926" "0.330357142857143" "0.0333994708994709" "0.037560950304484" "-0.267676958923522" "0.916005291005291" "0.305886243386243" "0.822594079482911" "0.0806878306878307" "0.478174603174603" "0.603072293060331" "0.705687830687831" "0.263888888888889" … "0.886243386243386" "-1.46998499265758" "0.553571428571429" "0.244378306878307" "0.306917368407162" "-0.177545897600496" "1.88554125770069" "1.79649150290836" "0.0476190476190476" "0.410794083459945" "0.351851851851852" "0.0707671957671958" "0.257442926006181" "0.0111091009226917" "-0.642479736305208" "0.170634920634921" "-0.105022008724652" "0.354166666666667" "0.409391534391534" "0.629295245255621" "0.771825396825397" "0.318783068783069" "-0.668049927742716" "0.12962962962963" "0.370039682539683" "-0.451307999749732" "0.663359788359788" "0.0667989417989418" "0.783730158730159" "1.06803685027394" "0.87962962962963" "-0.764805966930975" "0.0790343915343915" "-0.705661850563573" 0.000099 0.000156 -0.000368
shape: (10, 99)
date_id D1 D2 D3 D4 D5 D6 D7 D8 D9 E1 E10 E11 E12 E13 E14 E15 E16 E17 E18 E19 E2 E20 E3 E4 E5 E6 E7 E8 E9 I1 I2 I3 I4 I5 I6 I7 … P2 P3 P4 P5 P6 P7 P8 P9 S1 S10 S11 S12 S2 S3 S4 S5 S6 S7 S8 S9 V1 V10 V11 V12 V13 V2 V3 V4 V5 V6 V7 V8 V9 is_scored lagged_forward_returns lagged_risk_free_rate lagged_market_forward_excess_returns
i64 i64 i64 i64 i64 i64 i64 i64 i64 i64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 … f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 f64 bool f64 f64 f64
8980 0 0 0 0 1 0 0 1 0 1.577651 0.186177 0.001323 0.001323 0.001323 0.001323 0.955026 -0.583419 -0.704264 0.298365 -0.691361 1.259065 1.556516 1.71258 0.033069 0.333333 0.036376 -0.046483 -0.312326 0.913029 0.306217 1.025756 0.081349 0.478175 0.675627 0.699735 0.256283 … -1.427834 0.352513 0.926257 0.431383 -0.476976 0.500245 1.784173 0.029762 0.294719 0.51455 0.446429 0.466551 0.085717 -0.230132 0.272487 -0.106894 0.199735 0.409392 0.532717 0.744048 0.440476 -0.654839 0.699735 0.699074 -0.5024 0.882937 0.892196 0.828042 0.999172 0.759921 -0.803127 0.170966 -0.751909 true 0.003541 0.000161 0.003068
8981 0 0 0 0 1 0 0 1 0 1.575182 0.185847 0.000992 0.000992 0.000992 0.000992 0.955357 -0.583074 -0.703759 0.297608 -0.504499 1.193468 1.554184 1.640054 0.032738 0.333003 0.036045 0.073582 -0.312345 0.91336 0.305886 0.989571 0.082672 0.477844 0.661527 0.719577 0.255952 … -1.37652 0.953042 0.386905 0.523549 -0.421365 -0.234829 1.770175 0.03373 0.304496 0.638228 0.636905 1.849101 0.28169 -0.041995 0.448413 0.094321 0.215608 0.409392 0.597864 0.872354 0.691138 -0.583443 0.62996 0.598545 -0.394268 0.863757 0.699074 0.831349 1.120336 0.556217 -0.686192 0.141865 -0.660326 true -0.005964 0.000162 -0.006437
8982 0 0 0 0 1 0 0 0 1 1.57272 0.185516 0.000661 0.000661 0.000661 0.000661 0.955688 -0.083356 -0.573546 0.225822 -0.393903 1.123361 1.551723 1.562722 0.032407 0.332672 0.035714 0.033581 -0.312364 0.91369 0.291997 1.040514 0.081349 0.477513 0.655741 0.724206 0.22619 … -1.34762 0.210979 0.635251 -1.138198 -0.494248 -1.042718 1.754171 0.032407 0.257609 0.082011 0.152116 -0.201611 0.346373 0.054032 0.137566 0.294305 0.194444 0.409392 0.596528 0.778439 0.634921 -0.483236 0.669974 0.603836 -0.17042 0.848545 0.647487 0.832672 1.088992 0.665344 -0.459367 0.199405 -0.510979 true -0.00741 0.00016 -0.007882
8983 0 0 0 0 1 0 0 0 1 1.570266 0.185185 0.019841 0.019841 0.006614 0.006614 0.956019 -0.083403 -0.57318 0.225094 -0.541646 1.167982 1.549272 1.611215 0.032077 0.332341 0.035384 0.033998 -0.312383 0.914021 0.267196 1.091376 0.085979 0.477183 0.648582 0.728175 0.230159 … -1.399411 0.724868 0.21627 -0.230557 -0.318347 0.396917 1.769678 0.03373 0.20235 0.324074 0.212963 0.052878 -0.049023 0.120828 0.219577 0.137942 0.167328 0.409392 0.579726 0.449735 0.665344 -0.546298 0.590608 0.558862 -0.275099 0.826058 0.445767 0.835979 1.040988 0.594577 -0.561643 0.161706 -0.575997 true 0.00542 0.00016 0.004949
8984 0 0 0 0 0 0 1 0 1 1.567818 0.184854 0.019511 0.019511 0.006283 0.006283 0.956349 -0.083449 -0.572813 0.224366 -0.714549 1.243713 1.542543 1.693604 0.031746 0.332011 0.035053 0.029586 -0.301855 0.914352 0.260913 1.011571 0.093915 0.476852 0.638667 0.729497 0.238757 … -1.416282 0.611442 0.426257 0.046472 -0.262086 1.304142 1.785515 0.039683 0.230364 0.583995 0.445106 1.378927 -0.182025 0.01261 0.369048 0.011021 0.130291 0.409392 0.572656 0.489418 0.600529 -0.587258 0.46131 0.487434 -0.39548 0.80754 0.707672 0.839947 0.944593 0.715608 -0.692649 0.124669 -0.654045 true 0.008357 0.000159 0.007887
8985 0 0 0 0 0 0 0 0 0 1.565379 0.184524 0.01918 0.01918 0.005952 0.005952 0.911376 -0.083496 -0.572447 0.223638 -0.122314 1.20925 1.540116 1.655174 0.031415 0.33168 0.034722 0.038269 -0.301876 0.914683 0.27414 0.984115 0.080688 0.476521 0.597442 0.718254 0.238757 … -1.354498 0.046296 0.51455 0.276769 -0.261326 0.811754 1.784929 0.039683 0.249933 0.273148 0.134921 0.634465 -0.446682 -0.052686 0.083995 0.055282 0.209656 0.409392 0.574661 0.748677 0.498677 -0.616395 0.561839 0.53373 -0.432282 0.785053 0.469577 0.837963 1.226772 0.822751 -0.707361 0.142857 -0.649616 true -0.002896 0.000159 -0.003365
8986 0 0 0 0 0 0 0 0 0 1.562946 0.184193 0.018849 0.018849 0.005622 0.005622 0.911706 -0.083542 -0.57208 0.22291 -0.732397 1.225459 1.537761 1.672262 0.031085 0.331349 0.034392 0.038205 -0.301897 0.915013 0.269841 0.904453 0.073413 0.479167 0.605079 0.718254 0.220899 … -1.384785 0.232143 0.379299 1.19926 -0.344274 0.690323 1.791596 0.037037 0.298533 0.933201 0.721561 1.211345 -0.11805 -0.249315 0.566799 0.10733 0.228175 0.409392 0.580932 0.37037 0.528439 -0.64204 0.587632 0.526455 -0.429506 0.767857 0.671958 0.837963 0.785877 0.805556 -0.715692 0.196098 -0.668289 true 0.002457 0.000155 0.00199
8987 0 0 1 0 0 0 0 0 0 1.56052 0.183862 0.018519 0.018519 0.005291 0.005291 0.912037 -0.083874 -0.572016 0.222211 -0.800465 1.247273 1.534742 1.695469 0.030754 0.331019 0.034061 0.038118 -0.301918 0.915344 0.273148 0.842295 0.074074 0.478836 0.611319 0.724868 0.223545 … -1.420007 0.849868 0.375661 0.429471 -0.233374 -0.289766 1.792816 0.041005 0.371362 0.793651 0.689815 0.885178 -0.316882 -0.422374 0.631614 -0.02977 0.221892 0.409392 0.583556 0.477513 0.599206 -0.638658 0.394841 0.433532 -0.425462 0.734127 0.481481 0.787698 0.834898 0.823413 -0.723949 0.133929 -0.670946 true 0.002312 0.000156 0.001845
8988 0 0 0 0 0 0 0 0 0 1.558102 0.183532 0.018188 0.018188 0.00496 0.00496 0.912368 -0.084206 -0.571952 0.221513 -0.596939 1.271926 1.53234 1.721692 0.030423 0.330688 0.03373 0.037647 -0.301939 0.915675 0.272817 0.858582 0.082011 0.478505 0.604658 0.719577 0.262566 … -1.431028 0.303241 0.068783 0.044888 -0.269862 0.423268 1.792934 0.046958 0.41161 0.011905 0.026455 -0.001785 -0.317961 -0.608348 0.066138 -0.001594 0.259921 0.409392 0.63009 0.915344 0.462302 -0.626927 0.326389 0.39418 -0.38517 0.695106 0.655423 0.78373 0.994026 0.851852 -0.684937 0.101852 -0.646265 true 0.002891 0.000156 0.002424
8989 0 0 0 0 0 0 0 0 0 1.55569 0.183201 0.017857 0.017857 0.00463 0.00463 0.912698 -0.084538 -0.571888 0.220814 -0.747066 1.312626 1.529875 1.765317 0.030093 0.330357 0.033399 0.037561 -0.267677 0.916005 0.305886 0.822594 0.080688 0.478175 0.603072 0.705688 0.263889 … -1.469985 0.553571 0.244378 0.306917 -0.177546 1.885541 1.796492 0.047619 0.410794 0.351852 0.070767 0.257443 0.011109 -0.64248 0.170635 -0.105022 0.354167 0.409392 0.629295 0.771825 0.318783 -0.66805 0.12963 0.37004 -0.451308 0.66336 0.066799 0.78373 1.068037 0.87963 -0.764806 0.079034 -0.705662 true 0.00831 0.000156 0.007843
unfold_moreShow hidden cell