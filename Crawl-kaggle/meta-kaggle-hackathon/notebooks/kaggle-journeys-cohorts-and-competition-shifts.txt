Introduction
This analysis explores how Kaggle competition participation has evolved, both overall and by user cohorts. I first looked at the historical count of final team submissions, breaking it down by competition segment, accelerator usage, team size, and medal outcomes, along with trends in engagement scores.
I then applied a retrospective cohort analysis, grouping users by the year they joined their first competition (focusing on the 2019, 2020, 2023, and 2024 cohorts). This helped track shifts in behavior: competition preferences, dropout patterns, performance tiers, accelerator usage, team sizes, and time gaps between competitions. To complement this, I conducted a cross-sectional analysis of 2021–2024 submissions using Empirical Cumulative Distribution Function (ECDF) plots to examine how team size, competition segment, medal outcomes, and accelerator usage impact submission timing.
The time series shows that submission counts have grown, mainly driven by Playground competitions, alongside rising entry-level GPU adoption and a shift toward solo participation. Engagement scores have generally increased, though the adjusted engagement score remains stable and still favors higher-tier users.
From the cohort study, while most users still compete only once, a smaller group returns, improves, and moves up the tiers. However, recent cohorts behave differently from 2019–2020: they're more likely to compete solo, favor Playground competitions, have shorter gaps between competitions, and increasingly use entry GPUs over high-end hardware. The ECDF also shows a higher tendency to submit their final versions earlier.
The cross-sectional ECDF analysis reveals consistent patterns across years: bigger teams, medalists, accelerator users, and Research/Featured participants submit later, while solo teams, non-medalists, Playground participants, and even some high-tier solo users submit earlier.
Overall, Kaggle's user base is becoming more independent, casual, and faster-paced, possibly driven by more efficient workflows or by the impact of GenAI tools post-2019. Future studies could explore how these emerging patterns, especially among this year’s users, impact competition performance, dropout rates, and segment preferences in their next few competitions.
Analysis Overview:
Part 1: Time Series Overview: Submissions & Engagement
- Trends in submission counts and engagement over time.
Part 2: A Cohort Study of The Competitive Journey and Recent Trends
- Tracks submission pacing, gaps between competitions, and shifting behaviors across cohorts.
Part 3: Cross-Sectional Analysis of Final Submission Timing
- ECDF analysis of when teams submit, based on team size, accelerators, medal outcomes, and competition type.
Importing and Functions
unfold_moreShow hidden cell
Import Libraries
unfold_moreShow hidden code
Define Plotting functions
unfold_moreShow hidden code
Data Preparation
Importing data
more_horiz10 hidden cells
Exploring Data Peculiarities
In [10]:
# Show users with Medal but still marked Novice
suspicious = users_comp_stats[
    (users_comp_stats['UserPerformanceTierName']=='Unranked') &
    (users_comp_stats['IsMedal'] == True)
]
sus_users = (suspicious[['UserId','UserName','CompetitionId','MedalType','UserPerformanceTierName','CompTitle','UserTotalMedalsFinal']].drop_duplicates())
sus_users.head()
Out[10]:
UserId UserName CompetitionId MedalType UserPerformanceTierName CompTitle UserTotalMedalsFinal
136 3270 cofito 4852 Bronze Unranked BNP Paribas Cardif Claims Management 1
164 4934 mingyi 4986 Bronze Unranked Santander Customer Satisfaction 1
186 5411 mrboor 8076 Gold Unranked Toxic Comment Classification Challenge 1
192 6337 alejandrotobon 4481 Bronze Unranked Coupon Purchase Prediction 1
260 9487 guyko81 4594 Silver Unranked Rossmann Store Sales 1
In [11]:
# Show users with Medal from Playground Competitions
suspicious2 = users_comp_stats[
    (users_comp_stats['CompSegment']=='Playground') &
    (users_comp_stats['IsMedal'] == True)
]

sus2_users = (suspicious2[['UserId','UserName','CompetitionId','MedalType','UserPerformanceTierName','CompTitle','UserTotalMedalsFinal']].drop_duplicates())
sus2_users.head(5)
Out[11]:
UserId UserName CompetitionId MedalType UserPerformanceTierName CompTitle UserTotalMedalsFinal
162 4398 ccccat 5977 Silver Grandmaster Santa's Uncertain Bags 6
831 35844 china1000 5977 Bronze Master Santa's Uncertain Bags 1
1168 54836 titericz 5977 Bronze Grandmaster Santa's Uncertain Bags 17
1534 72177 superant 22838 Silver Master Rock, Paper, Scissors 2
1535 72177 superant 25401 Gold Master Hungry Geese 2
I noticed many records of users who have earned at least one medal but still remain in the Novice tier (as of June 2025 Kaggle data). At first glance, this seems inconsistent. However, according to the official Kaggle tier progression logic, these users may have missed some required engagement or profile completion steps, which causes them to remain in the Novice tier (or possibly some other reasons).
For example, when I look up the user with username mrboor (ID 5411) on Kaggle, he does have 1 Gold medal from a team competition, but remains Unranked for some reason.
Secondly, I also noticed there were medals given to Playground competitions in this data, but since Playground competition Medals typically do not count towards Kaggle ranking Points or Medals. These medals are likely rare cases where they are counted towards competition Medals.
For example, the user superant (ID 72177) is shown to be awarded a gold medal from this Playground Competition which does award points and medals for some reason.
To avoid misleading results, I will still consider these medals awarded from Playground Competitions since it was shown to be awarded officially as well.
unfold_moreShow hidden cell
unfold_moreShow hidden cell
unfold_moreShow hidden cell
Final processed data
The final processed dataset is structured at the user–competition–kernel level, where each row represents a unique combination of TeamId, UserId, and KernelId. For each team, the final kernel is selected as the submission with the most recent submission date. Although TeamId, UserId, and KernelId can each occur multiple times across the dataset, no combination of all three ever repeats.
For example, if a kernel was submitted as a team submission, the same KernelId can appear in multiple rows, one for each user on the team. Likewise, a UserId can occur in different rows if the user participated in multiple competitions or teams. In short, each row uniquely identifies one user linked to a specific final kernel within a particular team in a competition.
To reduce inaccuracies from outdated records, I have excluded all competition kernel submissions dated before 2017. Even so, I recognize that the data is still not completely reliable. In some cases, medals awarded to users are missing or not counted correctly based on the given data when cross-checked against official user profiles on the Kaggle website. As a result, this analysis may be partially distorted and could be improved in the future.
In [15]:
users_comp_stats.info()
unfold_moreShow hidden output
In [16]:
users_comp_stats.sort_values(by=['UserId','UserCompNumber']).head(5)
Out[16]:
UserId UserName TeamId ScriptId Accelerator CompetitionId UserPerformanceTierName SubmissionDate CompSegment CompTitle ... IsMedalInt UserTotalMedalsFinal UserGoldsFinal UserSilversFinal UserBronzesFinal UserTotalComps UserMedalNumber UserIsFirstMedal UserIsMedalFirstComp SubmissionYear
0 368 antgoldbloom 181913 2896 None 4366 Expert 2015-05-15 Featured West Nile Virus Prediction ... 0 0 0 0 0 3 0.0 False False 2015
1 368 antgoldbloom 181906 2604 None 4407 Expert 2015-05-15 Featured Crowdflower Search Results Relevance ... 0 0 0 0 0 3 0.0 False False 2015
2 368 antgoldbloom 240806 18828 None 4495 Expert 2015-11-19 Research How Much Did It Rain? II ... 0 0 0 0 0 3 0.0 False False 2015
3 421 dremovd 185602 4396 None 4378 Master 2015-06-16 Research ECML/PKDD 15: Taxi Trajectory Prediction (I) ... 0 2 0 1 1 7 0.0 False False 2015
4 421 dremovd 185603 5734 None 4407 Master 2015-06-16 Featured Crowdflower Search Results Relevance ... 0 2 0 1 1 7 0.0 False False 2015
5 rows × 43 columns
Part 1: Time Series Overview: Submissions & Engagement
In this section, I will provide an overview of competition submissions and their kernel engagement over time. Submission counts reflect only the final version of each kernel submitted before the competition deadline. I also break down submission activity by user cohorts, where each cohort consists of users who completed their first competition in the same year.
Engagement is measured by the sum of upvotes and comments per user view. An adjusted engagement score is also included, where the number of views is log-scaled to dampen the effect of inflated view counts. This adjustment helps highlight kernels that receive relatively few upvotes or comments despite high visibility.
Time Series Chart of Final Submissions By Attributes
The time series chart (Figure 1) shows a huge boom in final submission counts in 2023 and remains at a steady level ever since. This growth is driven mainly by Playground competitions, with Featured competitions showing a more recent upward trend. In contrast, Research competitions stayed flat, aside from a dip in 2020, likely tied to COVID-19 disruptions.
Entry-level GPU usage also increased starting in late 2022, with gradual growth since. Around the same time, there was a clear rise in solo team participation, pointing to a broader shift toward more individual-driven competition formats.
Note that the numbers shown represent the 60-day moving average instead of the raw count on a single day; this helps smooth out short-term fluctuations for clearer trend visibility.
unfold_moreShow hidden code
Competition Participation Cycle of Users by Cohorts
When analyzing final submissions by user cohorts - grouped by their first competition year (Figure 2), a clear lifecycle pattern could be seen. Most cohorts tend to ramp up participation, peak mid-year, and then gradually decline over time.
From 2022 to 2024, the peak moving average submission counts increased steadily, reaching a high of roughly 14 in January 2024. However, for the 2025 cohort, the current peak sits around 9 and has been declining since May, suggesting a possible slowdown in participation momentum this year.
unfold_moreShow hidden code
Engagement Levels Over Time by Performance Tier
Now let’s look at engagement scores of submissions based on user tiers. To prevent skill bias from teams, the chart below (Figure 3) includes only kernel submissions from solo teams. This ensures each submission is attributed to a single user and their performance tier, making comparisons more meaningful.
Kernel engagement scores have shown a strong upward trend since 2020, with submissions from Experts recently achieving the highest moving average. However, when using the adjusted engagement score (Figure 4), a clearer pattern emerges: engagement levels follow the Kaggle performance tier hierarchy, where Grandmasters consistently lead (10 or higher), followed by Masters (5–10), then Experts (below 5).
Note: All references to performance tier in this analysis reflect each user’s current highest overall tier.
unfold_moreShow hidden code
unfold_moreShow hidden code
Engagement Levels Over Time by Competition Segments
Figure 5 shows that submission counts have been climbing steadily since 2020, with the Playground segment leading the growth. Featured and Research competitions have had similar engagement levels over time, but a clear gap has opened up between them and Playground in recent years.
However, when we look at the adjusted engagement score in Figure 6, Research and Featured kernels consistently score higher than Playground. This suggests that while Playground competitions get a lot more views, they don’t necessarily generate more meaningful engagement such as upvotes and comments.
unfold_moreShow hidden code
unfold_moreShow hidden code
Part 2: A Cohort Study of The Competitive Journey and Recent Trends
In this part, I implemented a retrospective cohort approach to analyze user behavior based on the year of their first competition entry, focusing on the 2019, 2020, 2023, and 2024 cohorts. This approach gives us a clearer view of how user engagement evolves, instead of just looking at the community as one big pool.
Overview of How Participation Changes as Users Gain Experience
Figure 7 shows how many users completed the nth competition in 2015 or later, broken down by competition segments (only includes Playground, Featured, and Research).
Data clearly shows that most users stop competing on Kaggle after their first competition. Although the rate of drop-off is notably high during the first several competitions, it gradually declines and begins to stabilize beyond the seventh competition. The bottom plot shows the proportion of competition segments joined on the n-th competition; the proportions are mostly consistent overall with a greater preference for Featured competitions.
Note:
“Competitions Completed” refers to the number of competitions in which a Kaggle user submitted a kernel before the competition’s submission deadline.
"Number of participants" does not represent the number of teams, it represents all unique individuals within all teams participating in any of the competitions from the segments specified.
The whole analysis does not include users who had never participated in any competitions.
unfold_moreShow hidden code
Secondly, by looking at Kagglers grouped by their current highest performance tier overall (Figure 8), we see that the proportion of high-tier participants increases with the number of competitions completed. In other words, today’s Experts, Masters, and Grandmasters tend to have completed more competitions than those in lower tiers. The next normalized histogram shows this clearly (Figure 9): up to 80% of current unranked Kagglers have competed in only one competition, while more than 40% of Experts, Masters, and Grandmasters have entered multiple competitions.
Note:
The performance tier shown in this analysis represents each user’s highest overall tier and is not limited to Competition tier alone. As a result, some Experts or even Grandmasters with only one competition may have earned their title through other tracks such as Notebooks, Datasets, or Code.
unfold_moreShow hidden code
unfold_moreShow hidden code
Competition Participation and Tier Progression by Cohorts
Let’s examine how users from different cohorts (defined by the year they entered their first competition) have progressed in terms of participation and current performance tier (Figure 10). The distribution of tiers across competition counts mirrors earlier trends: users who compete more tend to reach higher tiers, and those who started earlier are more likely to have become Grandmasters.
Figure 11 further supports this by showing a Normalized Count Heatmap of user cohorts by current performance tier. Notably, the majority of current Masters and Grandmasters began competing as early as 2015, with another peak for Masters in 2021 and for Grandmasters in 2020. In contrast, the majority of current Experts and Unranked entered in 2024. This trend highlights that reaching higher tiers like Grandmaster typically requires long-term engagement, whereas lower tiers such as Expert can be achieved more quickly, though still often over multiple years.
unfold_moreShow hidden code
unfold_moreShow hidden code
Shifts in Competition Preferences: From Research to Playground
Figure 12 below shows that the 2019 and 2021 cohort group initially favored Featured and Research competitions. Over time, many who started in 2019 gradually shifted toward more advanced research problems, while those who started in 2020 often began with Research or Featured competitions but then moved to easier Playground competitions in subsequent entries.
For the 2023 and 2024 cohort, with the rise of LLMs and AI-assisted coding, far fewer newcomers started with Research competitions at all. Instead, around 50% of newcomers chose Playground competitions for their first, reflecting a influx of less-experienced users drawn in by the support of AI tools or access to more high-quality online courses. Although 2024 saw roughly 23% more new participants, these users had higher dropout rates after each competition when compared to users starting in 2023, suggesting many were quickly overwhelmed and failed to stay engaged beyond their first few competitions.
Based on the earlier chart of performance tier distribution, it’s clear that high-tier users are the ones who stay active across many competitions. However, this chart shows that once dominated by research-oriented challengers, Kagglers have increasingly tilted toward Playground competitions as the default training ground. This also suggests that many top-tier users increasingly rely on alternative paths to reach their current overall performance tier, such as contributing datasets and notebooks, since Playground competitions generally don’t award medals or points and therefore don’t contribute directly to tier progression. The rapid emergence of new ML techniques may also support this trend, as Playground contests offer a flexible platform for broad experimentation rather than deep specialization in competitions such as Research or Featured.
unfold_moreShow hidden code
Trends in Team Collaboration
Figure 13 below shows that across all cohorts, users are more likely to join teams in their first competition but increasingly compete solo as they gain experience. However, there's a noticeable shift when comparing the 2019-2020 cohorts to those from 2023-2024: in 2019-2020, at least 15% of users still joined small teams in their second competition, whereas in 2023-2024, that number drops below 10% and even lower for subsequent competitions. This suggests that newer users are transitioning to solo competition earlier than before.
The reason for this shift isn't entirely clear, but some patterns stand out. The normalized count heatmap (Figure 14) shows that for final submissions in 2019, 2020, 2023, and 2024, the majority of medal-winning kernel submissions came from non-solo teams, a trend that has been consistent since 2017.
Additionally, larger teams consistently submit later in competition timelines, suggesting they utilize more of the available time, and also due to greater coordination overhead. This is explored further in a later section.
Overall, these trends suggest a need to either address the barriers to team collaboration or better support the emerging preference for solo participation.
unfold_moreShow hidden code
unfold_moreShow hidden code
Accelerator Usage Trend
Figure 15 below shows which accelerators were used in the cohorts' n-th competition. Competitors who began their first competition in 2019 or 2020 relied more on high-end accelerators like P100s and A100s during their first 10 competitions, indicating a preference for compute-heavy tasks. In contrast, those starting in 2023 or 2024 show a noticeable decline in accelerator usage both relatively and absolutely, with more relying on no accelerator at all. However, usage of entry-level GPUs is growing within the 2024 cohort.
This shift may reflect a move toward more efficient modeling practices that reduce the need for powerful hardware, a lack of technical expertise with high-end accelerators among newer users, or simply a preference for tasks requiring less computational power.
unfold_moreShow hidden code
Trends in Time Usage Between Competitions
Kaggle’s Top Users Spend More Time In-Between Competitions
Figures 16.1 and 16.2 show the median and mean time gaps (in days) between competitions for overall competitors, grouped by their current performance tier as of 2025. The data reveals that competitors in higher tiers tend to take longer breaks between competitions, especially after their first one.
These time gaps are generally right-skewed, as the mean is consistently higher than the median. For all tiers except Unranked, the gaps generally shrink with more competition experience, stabilizing at around 30–50 median days or 100–150 mean days between submissions over the next few competitions.
unfold_moreShow hidden code
unfold_moreShow hidden code
Figures 17.1 and 17.2 break these competitors down by cohort. Competitors from the 2019–2020 cohorts show much larger and clearer time gaps between tier groups observed both in terms of median and mean days. In contrast, the 2023–2024 cohorts have shorter and more compressed time gaps across all tiers.
This suggests competitors starting from 2023 have either become much more efficient in participating and completing competitions, or that the rate of new competitions has increased compared to previous years, leading to more frequent participation.
unfold_moreShow hidden code
unfold_moreShow hidden code
Part 3: Distribution Analysis: Time to Final Submission
To better understand how much time each team (Solo/Non-Solo) spends on competitions submitted in recent years, I created an Empirical Cumulative Distribution Function (ECDF) plot to visualize the cumulative proportion of final kernel submissions over the course of each competition (measured as a percentage of competition duration). Figure 18, for example, shows the overall time spent by teams based on their final medal awarded. This can be interpreted as the probability of completing a competition before a given point in time, conditional on a specific factor (e.g., team size, segment, or hardware used).
Because competitions vary in length, I standardized the time variable as the percentage of days used relative to the total competition duration.
Definitions:
Competition Duration: Measured as days from competition enabled date to competition deadline date.
Days used to complete the competition: Measured as days from competition enabled date to last submission date.
Percentage of Competition Duration Used: Computed as the percentage of days taken to complete the competition relative to Competition Duration: Days taken to complete the competition / Competition Duration.
unfold_moreShow hidden code
To interpret the ECDF plot: the y-axis represents the cumulative proportion of a final submission occurring at or before the percentage of competition duration shown on the x-axis.
At the right end of the x-axis (the competition deadline), you’ll notice that the cumulative proportion for each tier jumps vertically. This happens because all remaining submissions made exactly at the deadline are included there. The height of this final jump reflects the proportion of submissions made by teams who waited until the last moment to submit. For example, roughly 50% of all teams awarded Gold made their final submission just before the competition deadline (around 99% of the total duration). The remaining 50% submitted exactly at 100%, right at the deadline date.
By looking at the curvature of the plot, it shows that teams with no medals have a near-linear curve, meaning there is a consistent rate of final submissions made throughout the competition timeline, and they usually submitted before reaching the end of the deadline. In contrast, the slowest teams were the more successful ones, with a higher rate of final submissions made near the end of the competition.
Time to Submission by Cohorts
When comparing time to submission across cohorts based on their first three competitions, the ECDF plot (Figure 19) shows that the curves for more recent cohorts (2023–2024) are slightly steeper compared to those from 2019 and 2020. This suggests that newer cohorts are submitting a bit earlier, or at least are less inclined to wait until the very last moment compared to earlier cohorts.
unfold_moreShow hidden code
Cross-Sectional Distribution Analysis by Submission Year
The following figures break down the distribution of final submissions by the year of submission (2021 to 2024), rather than by user cohort. By analyzing each year separately, we can see how factors like competition segment, team size, and accelerator usage influence when teams tend to submit.
This helps reveal whether certain behaviors or strategies have become more or less common over time.
Competition Segment On Time to Submission:
Submission timing patterns have remained fairly consistent across the past four years (Figure 20). Playground competitions show the steepest and most consistently rising ECDF curves, indicating faster and steadier submission rates.
In contrast, Featured and Research competitions have slightly flatter curves early on, with varying degrees of mid-competition submission speed year to year, likely reflecting differences in difficulty or the level of commitment required each year.
unfold_moreShow hidden code
Current Performance Tier On Time to Submission:
To avoid bias from mixed-tier teams when analyzing submission timing by performance tier, this part includes only submissions from solo teams. By looking at the curvature of the functions conditional on different tier groups (Figure 21), some distinct submission behaviour is observed:
Current Masters and Grandmasters have a much steeper curve in the beginning of their competition, and slow down gradually mid-way or near the end of the competition. This pattern is consistent even in submissions from three years ago and suggests many submit well before the deadline, likely reflecting experience, good preparation, and a strong head start.
Experts show a steady, almost linear increase throughout the competition duration, implying a consistent rate of final submissions. Their curve still ends nearly at the same point, but their pacing is more uniform compared to higher tiers.
Unranked Kagglers have the slowest curve in the beginning and accelerate mid-way to catch up on other tier groups, likely due to inexperience or a lack of early preparation to submit as quickly as other tier groups.
unfold_moreShow hidden code
Team Size On Time to Submission:
Submission timing by team size (Figure 22) follows a consistent pattern across most submission years. Around 80% of solo competitors tend to submit before the last day of the competition, compared to roughly 60% for small teams (2–3 members) and 50% for larger teams (4+ members). However, trends for large teams in all years remain slightly uncertain due to limited data.
The overall pattern suggests that bigger teams often utilize more of the available competition time than solo teams, likely due to greater coordination overhead and barriers in forming teams in general. This may help explain why solo teams have always been the majority and continue to grow, as working in larger teams can often slow down progress and lead to more last-minute submissions.
unfold_moreShow hidden code
Accelerator Usage on Time to Submission:
Submission timing patterns vary quite a lot by accelerator type and year as seen in Figure 23. The ECDF curves for TPU users from 2022 to 2024 are jagged due to limited and inconsistent usage. In contrast, entry-level GPU usage became more widespread starting in 2022, reflected by smoother, more stable curves year by year. Notably, 2024 and 2022 TPU users appeared to submit earlier than the same type of users in the previous year in general, suggesting a higher time-efficiency for TPU users in those years, though data for those groups are still quite limited.
Teams that did not use any accelerators generally submitted earlier than those using entry-level or high-end GPUs, which is expected since tasks requiring accelerators tend to be more compute-heavy. Interestingly, TPU users submitted ahead of all other groups in 2022 and 2024 but were the slowest in 2023.
unfold_moreShow hidden code
Medal Outcome on Time to Submission:
Figure 24 shows an interesting pattern among medalists that clearly sets them apart: most medal-winning participants tend to use nearly the entire duration of the competition before making their final submission, regardless of medal type or year.
In contrast, non-medalists typically submit much earlier, often well before the competition deadline. This suggests that taking more time correlates with better outcomes, possibly due to extended periods spent on model tuning or strategy planning.
unfold_moreShow hidden code
Conclusion
This analysis highlights a clear evolution in how Kaggle users approach competitions. Participation has grown overall, but with distinct shifts in behavior: more users are competing solo, favoring Playground competitions, adopting entry-level GPUs, and submitting earlier. Meanwhile, medalists, teams competing in challenging competitions, and larger teams still consistently prefer to optimize their time and resources by submitting closer to deadlines.
These changes point to a rising trend of faster, more casual competition dynamics, potentially influenced by GenAI tools and maturing workflows. Understanding these patterns is crucial for Kaggle to adapt its platform, whether to encourage more sustained engagement, support team collaborations, or design competitions that align with these new participation styles.
Recommendations
To adapt to the growing trend of solo participation, Kaggle could consider making team collaboration more attractive, especially for newcomers, by introducing features like team matchmaking, mentorship programs, or bonus incentives for team-based submissions. For the growing segment of casual and fast-paced competitors, shorter, time-bound challenges or mini-competitions could better align with their preferences and engagement style. Additionally, Kaggle should actively monitor the influence of GenAI tools, as they appear to be accelerating participation cycles and changing how users approach competitions.
To sustain long-term engagement, Kaggle could implement programs that reward continuous participation and progression, helping prevent dropouts after the first competition. Since Playground competitions are the most popular entry point, Kaggle might also develop clearer pathways or progression frameworks that encourage users to transition into more advanced Research or Featured competitions, keeping them challenged and engaged.
Future studies could explore how these recent patterns, especially among this year’s users, impact competition performance, dropout rates, and segment preferences in their next few competitions.