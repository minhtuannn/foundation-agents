In [1]:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
/kaggle/input/playground-series-s5e3/sample_submission.csv
/kaggle/input/playground-series-s5e3/train.csv
/kaggle/input/playground-series-s5e3/test.csv
/kaggle/input/rapids-knn-starter-ensemble-lb-0-961-wow/submission_ensemble.csv
/kaggle/input/rapids-knn-starter-ensemble-lb-0-961-wow/__results__.html
/kaggle/input/rapids-knn-starter-ensemble-lb-0-961-wow/__notebook__.ipynb
/kaggle/input/rapids-knn-starter-ensemble-lb-0-961-wow/__output__.json
/kaggle/input/rapids-knn-starter-ensemble-lb-0-961-wow/custom.css
/kaggle/input/rainfall-prediction-using-machine-learning/Rainfall.csv
In [2]:
import pandas as pd, numpy as np
In [3]:
import warnings
warnings.simplefilter('ignore')
In [4]:
train = pd.read_csv("/kaggle/input/playground-series-s5e3/train.csv", index_col='id')
test = pd.read_csv("/kaggle/input/playground-series-s5e3/test.csv", index_col='id')
train_extra=pd.read_csv("/kaggle/input/rainfall-prediction-using-machine-learning/Rainfall.csv")
In [5]:
train_extra.columns = train_extra.columns.str.replace(' ', '')
train_extra = train_extra[train_extra.columns].copy()
train_extra['rainfall'] = train_extra['rainfall'].map({'no': 0, 'yes': 1})
train_extra['humidity']=train_extra['humidity'].astype(float)
train_extra['cloud']=train_extra['cloud'].astype(float)
train_features=list(train)
train_extra=train_extra[train_features]
In [6]:
train = pd.concat([train, train_extra], axis=0, ignore_index=True)
In [7]:
train = train.drop_duplicates()
train.shape
Out[7]:
(2556, 12)
In [8]:
train['cloud'].min(),train['cloud'].max(),test['cloud'].min(),test['cloud'].max()
Out[8]:
(0.0, 100.0, 0.0, 100.0)
In [9]:
train['sunshine'].min(),train['sunshine'].max(),test['sunshine'].min(),test['sunshine'].max()
Out[9]:
(0.0, 12.1, 0.0, 11.8)
In [10]:
train['day'].min(),train['day'].max(),test['day'].min(),test['day'].max()
Out[10]:
(1, 365, 1, 365)
In [11]:
features=list(test)
features.append('rainfall')
train=train[features]
In [12]:
train.sample(2)
Out[12]:
day pressure maxtemp temparature mintemp dewpoint humidity cloud sunshine winddirection windspeed rainfall
23 24 1013.0 24.8 19.9 17.1 18.1 90.0 92.0 0.0 70.0 30.4 1
2188 364 1022.3 16.4 15.2 13.8 14.7 92.0 93.0 0.1 40.0 18.0 1
In [13]:
test.sample(2)
Out[13]:
day pressure maxtemp temparature mintemp dewpoint humidity cloud sunshine winddirection windspeed
id
2438 249 1011.2 31.0 28.1 26.9 25.0 82.0 84.0 0.4 180.0 12.0
2766 212 1005.2 31.1 28.0 26.8 24.7 87.0 86.0 4.3 180.0 12.4
No categorical feature
In [14]:
train['rainfall'].value_counts()
Out[14]:
rainfall
1    1899
0     657
Name: count, dtype: int64
imbalanced labels; oversampling or undersampling can be considered later.
In [15]:
train.isnull().sum().sort_values(ascending=False)
Out[15]:
winddirection    1
windspeed        1
day              0
pressure         0
temparature      0
maxtemp          0
mintemp          0
dewpoint         0
cloud            0
humidity         0
sunshine         0
rainfall         0
dtype: int64
In [16]:
test.isnull().sum().sort_values(ascending=False)
Out[16]:
winddirection    1
pressure         0
day              0
maxtemp          0
temparature      0
dewpoint         0
mintemp          0
humidity         0
cloud            0
sunshine         0
windspeed        0
dtype: int64
In [17]:
target = "rainfall"
In [18]:
test['winddirection']=test['winddirection'].fillna(value=test['winddirection'].mean())
train['winddirection']=train['winddirection'].fillna(value=train['winddirection'].mean())
train['windspeed']=train['windspeed'].fillna(value=train['windspeed'].mean()) 
penalty{‘l1’, ‘l2’, ‘elasticnet’, None}, default=’l2’
Cfloat, default=1.0
class_weightdict or ‘balanced’, default=None
solver{‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’}, default=’lbfgs’
For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones;
In [19]:
train['rainfall'].value_counts()
Out[19]:
rainfall
1    1899
0     657
Name: count, dtype: int64
In [20]:
class_weight={0:657.0/(1899+657), 1:1899.0/(1899+657)}
class_weight
Out[20]:
{0: 0.25704225352112675, 1: 0.7429577464788732}
In [21]:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

y=train['rainfall']
drop_features=['cloud','humidity']
drop_features.append('rainfall')
X=train.drop(columns=drop_features,axis=1)
print(X.head(2))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)

clf_1 = LogisticRegression(solver='liblinear',penalty='l1',max_iter=10000, random_state=88,C=1.0) #81.05
clf_1.fit(X_train, y_train)
y_pred_1 =  clf_1.predict(X_test)   
acc = accuracy_score(y_test,y_pred_1 ) * 100
print(f"Logistic Regression model accuracy: {acc:.2f}%")
   day  pressure  maxtemp  temparature  mintemp  dewpoint  sunshine  \
0    1    1017.4     21.2         20.6     19.9      19.4       1.1   
1    2    1019.5     16.2         16.9     15.8      15.4       0.0   

   winddirection  windspeed  
0           60.0       17.2  
1           50.0       21.9  
Logistic Regression model accuracy: 81.05%
In [22]:
drop_features.remove('rainfall')
_test=test.drop(columns=drop_features,axis=1)
test_preds_1 = clf_1.predict_proba(_test)[:,1]
In [23]:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

y=train['rainfall']
drop_features=['day','mintemp','pressure','sunshine','winddirection','windspeed','maxtemp','dewpoint','temparature']
drop_features.append('rainfall')
X=train.drop(columns=drop_features,axis=1)
print(X.head(2))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)

clf_2 = LogisticRegression(solver='newton-cg',penalty='none',max_iter=10000, random_state=43,C=1.0) #83.79
#clf_2 = LogisticRegression(solver='saga',penalty='l2',max_iter=10000, random_state=42,C=1.0) #83.20
clf_2.fit(X_train, y_train)
    
y_pred_2 =  clf_2.predict(X_test)   
acc = accuracy_score(y_test,y_pred_2 ) * 100
print(f"Logistic Regression model accuracy: {acc:.2f}%")
   humidity  cloud
0      87.0   88.0
1      95.0   91.0
Logistic Regression model accuracy: 83.40%
In [24]:
drop_features.remove('rainfall')
_test=test.drop(columns=drop_features,axis=1)
test_preds_2 = clf_2.predict_proba(_test)[:,1]
In [25]:
sub = pd.DataFrame({"id": test.index, "rainfall": list(test_preds_1)})
#sub.to_csv("submission.csv", index=False)
sub.head()
Out[25]:
id rainfall
0 2190 0.957779
1 2191 0.953586
2 2192 0.863109
3 2193 0.216346
4 2194 0.042856
In [26]:
sub = pd.DataFrame({"id": test.index, "rainfall": list(test_preds_2)})
#sub.to_csv("submission.csv", index=False)
sub.head()
Out[26]:
id rainfall
0 2190 0.987704
1 2191 0.988487
2 2192 0.970187
3 2193 0.216403
4 2194 0.192097
Reference:
https://www.kaggle.com/code/cdeotte/rapids-knn-starter-ensemble-lb-0-961-wow
In [27]:
sub = pd.read_csv("/kaggle/input/rapids-knn-starter-ensemble-lb-0-961-wow/submission_ensemble.csv")
In [28]:
sub['rainfall'] = 0.98 * sub['rainfall'] + 0.02 * test_preds_2 

sub.to_csv("submission.csv", index=False)
sub.head(10)
Out[28]:
id rainfall
0 2190 0.997069
1 2191 0.995742
2 2192 0.998061
3 2193 0.123808
4 2194 0.060225
5 2195 0.790608
6 2196 0.864746
7 2197 0.994183
8 2198 0.960564
9 2199 0.821763
In [ ]:
 