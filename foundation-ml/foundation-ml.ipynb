{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e1a34b",
   "metadata": {},
   "source": [
    "## First attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21145292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import selectors \n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL_NAME = \"llama3\"  # Ensure you have this model: `ollama pull llama3`\n",
    "WORKSPACE_DIR = \"./contest_workspace\"\n",
    "PYTHON_EXEC = sys.executable \n",
    "\n",
    "# ROOT DATA PATH (Adjust this to your actual path)\n",
    "ALL_DATA_PATH = \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets\" \n",
    "\n",
    "# Setup Directories\n",
    "os.makedirs(WORKSPACE_DIR, exist_ok=True)\n",
    "os.makedirs(ALL_DATA_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- 1. LLM COMMUNICATION LAYER ---\n",
    "def call_llm(system_prompt, user_prompt):\n",
    "    \"\"\"Sends request to Ollama with a strict JSON payload.\"\"\"\n",
    "    full_prompt = f\"System: {system_prompt}\\nUser: {user_prompt}\"\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME, \n",
    "        \"prompt\": full_prompt, \n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.2, # Low temp for precise coding\n",
    "            \"num_ctx\": 16384,    # Large context for code + error logs\n",
    "            \"stop\": [\"User:\", \"System:\", \"```python\\n\\n\"] # Stop tokens\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        print(\"   ðŸ§  (Agent is thinking...)\", end=\"\\r\")\n",
    "        response = requests.post(OLLAMA_URL, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json().get('response', '')\n",
    "    except Exception as e:\n",
    "        return f\"LLM_ERROR: {str(e)}\"\n",
    "\n",
    "# def save_code_to_file(code_text, filename):\n",
    "#     \"\"\"Extracts Python block and saves to file.\"\"\"\n",
    "#     clean_code = code_text\n",
    "#     # Extract code between markdown ticks\n",
    "#     if \"```python\" in code_text:\n",
    "#         clean_code = code_text.split(\"```python\")[1].split(\"```\")[0]\n",
    "#     elif \"```\" in code_text:\n",
    "#         clean_code = code_text.split(\"```\")[1].split(\"```\")[0]\n",
    "    \n",
    "#     clean_code = clean_code.strip()\n",
    "    \n",
    "#     # Remove accidental starting text\n",
    "#     lines = clean_code.splitlines()\n",
    "#     if lines and lines[0].lower().startswith(\"python\"):\n",
    "#         clean_code = \"\\n\".join(lines[1:])\n",
    "\n",
    "#     filepath = os.path.join(WORKSPACE_DIR, filename)\n",
    "#     with open(filepath, \"w\") as f:\n",
    "#         f.write(clean_code)\n",
    "#     return filepath, clean_code\n",
    "\n",
    "# def execute_code(filepath):\n",
    "#     \"\"\"Executes the script and returns (Success_Bool, Output/Error).\"\"\"\n",
    "#     print(f\"   âš¡ Executing {os.path.basename(filepath)}...\")\n",
    "#     try:\n",
    "#         result = subprocess.run(\n",
    "#             [PYTHON_EXEC, os.path.basename(filepath)], \n",
    "#             cwd=WORKSPACE_DIR,       \n",
    "#             capture_output=True,     \n",
    "#             text=True,               \n",
    "#             timeout=120 \n",
    "#         )\n",
    "#         if result.returncode == 0:\n",
    "#             return True, result.stdout\n",
    "#         else:\n",
    "#             return False, result.stderr\n",
    "#     except subprocess.TimeoutExpired:\n",
    "#         return False, \"TIMEOUT: Script took longer than 120 seconds.\"\n",
    "#     except Exception as e:\n",
    "#         return False, str(e)\n",
    "\n",
    "def execute_script(script_name, work_dir = \".\", device=\"0\"):    \n",
    "    if not os.path.exists(os.path.join(work_dir, script_name)):\n",
    "        raise Exception(f\"The file {script_name} does not exist.\")\n",
    "    try:\n",
    "        script_path = script_name\n",
    "        device = device        \n",
    "        cmd = f\"CUDA_VISIBLE_DEVICES={device} python -u {script_path}\"\n",
    "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, shell=True, cwd=work_dir)\n",
    "\n",
    "        stdout_lines = []\n",
    "        stderr_lines = []\n",
    "\n",
    "        selector = selectors.DefaultSelector()\n",
    "        selector.register(process.stdout, selectors.EVENT_READ)\n",
    "        selector.register(process.stderr, selectors.EVENT_READ)\n",
    "\n",
    "        while process.poll() is None and selector.get_map():\n",
    "            events = selector.select(timeout=1)\n",
    "\n",
    "            for key, _ in events:\n",
    "                line = key.fileobj.readline()\n",
    "                if key.fileobj == process.stdout:\n",
    "                    # print(\"STDOUT:\", line, end =\" \")\n",
    "                    stdout_lines.append(line)\n",
    "                else:\n",
    "                    # print(\"STDERR:\", line, end =\" \")\n",
    "                    stderr_lines.append(line)\n",
    "\n",
    "        for line in process.stdout:\n",
    "            line = line\n",
    "            # print(\"STDOUT:\", line, end =\" \")\n",
    "            stdout_lines.append(line)\n",
    "        for line in process.stderr:\n",
    "            line = line\n",
    "            # print(\"STDERR:\", line, end =\" \")\n",
    "            stderr_lines.append(line)\n",
    "\n",
    "        return_code = process.returncode\n",
    "\n",
    "        if return_code != 0:\n",
    "            observation = \"\".join(stderr_lines)\n",
    "        else:\n",
    "            observation = \"\".join(stdout_lines)\n",
    "        if observation == \"\" and return_code == 0:\n",
    "            # printed to stderr only\n",
    "            observation = \"\".join(stderr_lines)\n",
    "        return return_code, \"The script has been executed. Here is the output:\\n\" + observation\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"++++\", \"Wrong!\")\n",
    "        # raise Exception(f\"Something went wrong in executing {script_name}: {e}. Please check if it is ready to be executed.\")\n",
    "        return -1, f\"Something went wrong in executing {script_name}: {e}. Please check if it is ready to be executed.\"\n",
    "\n",
    "\n",
    "class BaseAgent:\n",
    "    \"\"\"Parent class capable of Generating and Fixing code.\"\"\"\n",
    "    def __init__(self, role, system_instruction, specs, agent_profile):\n",
    "        self.specs = specs\n",
    "        self.role = role\n",
    "        self.system = system_instruction\n",
    "        self.code_path = \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/foundation-ml/bucket-code\"\n",
    "        self.root_path_data = \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets\"\n",
    "        self.agent_profile = agent_profile\n",
    "        self.device = 0\n",
    "\n",
    "    def make_plan(self, data_description, requirement, plan_conditions):\n",
    "\n",
    "        prompt = \"\"\"Devise an end-to-end actionable plan for {} task according to the user's requirements described in the following JSON object.\n",
    "\n",
    "        # User Requirements & Data Info\n",
    "        ```json\n",
    "        {{\n",
    "            \"Data Description\": \"{}\",\n",
    "            \"Requirements\": \"{}\"\n",
    "            \"Root path data\": \"{}\"\n",
    "        }}\n",
    "        ```\n",
    "        When devising a plan, follow these instructions and do not forget them:\n",
    "        {}\n",
    "        \"\"\"\n",
    "        prompt = prompt.format(\n",
    "                self.role,\n",
    "                data_description,\n",
    "                requirement,\n",
    "                self.root_path_data,\n",
    "                self.system\n",
    "        )\n",
    "\n",
    "        return call_llm(self.agent_profile, prompt)\n",
    "\n",
    "    def implement_solution(self, code_instructions, code=\"\", n_attempts=5):\n",
    "\n",
    "        print(\n",
    "            self.role,\n",
    "            \"I am implementing the following instruction\"\n",
    "        )\n",
    "\n",
    "        log = \"Nothing. This is your first attempt.\"\n",
    "        error_logs = []\n",
    "        code = code  # if a template/skeleton code is provided\n",
    "        iteration = 0\n",
    "        completion = None\n",
    "        action_result = \"\"\n",
    "        rcode = -1\n",
    "\n",
    "\n",
    "        while iteration < n_attempts:\n",
    "            try:\n",
    "                exec_prompt = \"\"\"Carefully read the following instructions to write Python code for {} task.\n",
    "                {}\n",
    "                \n",
    "                # Previously Written Code\n",
    "                ```python\n",
    "                {}\n",
    "                ```\n",
    "                \n",
    "                # Error from the Previously Written Code\n",
    "                {}\n",
    "                \n",
    "\n",
    "                Note that you need to write the python code for the {}. \n",
    "                Start the python code with \"```python\". Please ensure the completeness of the code so that it can be run without additional modifications.\n",
    "                If there is any error from the previous attempt, please carefully fix it first.\"\"\"\n",
    "       \n",
    "                exec_prompt = exec_prompt.format(\n",
    "                    self.specs['requirements'],\n",
    "                    code_instructions,\n",
    "                    code,\n",
    "                    log,\n",
    "                    \"data processing part\",\n",
    "                )\n",
    "\n",
    "                res = call_llm(self.agent_profile, exec_prompt)\n",
    "                raw_completion = res\n",
    "                completion = raw_completion.split(\"```python\")[1].split(\"```\")[0]\n",
    "                \n",
    "                if not completion.strip(\" \\n\"):\n",
    "                    continue\n",
    "                \n",
    "                self.code_name = self.role \n",
    "                filename = f\"{self.code_path}{self.code_name}.py\"\n",
    "                os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "                with open(filename, \"wt\") as file:\n",
    "                    file.write(completion)\n",
    "                code = completion\n",
    "                rcode, log = self.self_validation(filename)\n",
    "                if rcode == 0:\n",
    "                    action_result = log\n",
    "                    break\n",
    "                else:\n",
    "                    log = log\n",
    "                    error_logs.append(log)\n",
    "                    action_result = log\n",
    "                    print(self.agent_type, f\"I got this error (itr #{iteration}): {log}\")\n",
    "                    iteration += 1                    \n",
    "                    # break\n",
    "            except Exception as e:\n",
    "                iteration += 1\n",
    "                print(self.role, f\"===== Retry: {iteration} =====\")\n",
    "                print(self.role, f\"Executioin error occurs: {e}\")\n",
    "            continue\n",
    "        if not completion:\n",
    "            completion = \"\"\n",
    "\n",
    "        print(\n",
    "            self.role,\n",
    "            f\"I executed the given plan and got the follow results:\\n\\n{action_result}\",\n",
    "        )\n",
    "        return {\"rcode\": rcode, \"action_result\": action_result, \"code\": completion, \"error_logs\": error_logs}\n",
    "\n",
    "\n",
    "    def run_code(self, completion, code=\"\", n_attempts=5):\n",
    "\n",
    "        print(\n",
    "            self.role,\n",
    "            \"I am implementing the following instruction\"\n",
    "        )\n",
    "\n",
    "        log = \"Nothing. This is your first attempt.\"\n",
    "        error_logs = []\n",
    "        code = code  # if a template/skeleton code is provided\n",
    "        iteration = 0\n",
    "        action_result = \"\"\n",
    "        rcode = -1\n",
    "\n",
    "        while iteration < n_attempts:\n",
    "            try:\n",
    "                raw_completion = completion\n",
    "                if \"```python\" in raw_completion:\n",
    "                    completion = raw_completion.split(\"```python\")[1].split(\"```\")[0]\n",
    "                elif \"```Python\" in raw_completion:\n",
    "                    completion = raw_completion.split(\"```Python\")[1].split(\"```\")[0]\n",
    "                else:\n",
    "                    completion = raw_completion.split(\"```\")[1].split(\"```\")[0]\n",
    "\n",
    "                if not completion.strip(\" \\n\"):\n",
    "                    continue\n",
    "                self.code_name = self.role \n",
    "                filename = f\"{self.code_path}/{self.code_name}.py\"\n",
    "                os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "                with open(filename, \"wt\") as file:\n",
    "                    file.write(completion)\n",
    "                code = completion\n",
    "                rcode, log = self.self_validation(filename)\n",
    "                if rcode == 0:\n",
    "                    action_result = log\n",
    "                    break\n",
    "                else:\n",
    "                    log = log\n",
    "                    error_logs.append(log)\n",
    "                    action_result = log\n",
    "                    print(self.role, f\"I got this error (itr #{iteration}): {log}\")\n",
    "                    iteration += 1                    \n",
    "                    # break\n",
    "            except Exception as e:\n",
    "                iteration += 1\n",
    "                print(self.role, f\"===== Retry: {iteration} =====\")\n",
    "                print(self.role, f\"Executioin error occurs: {e}\")\n",
    "            continue\n",
    "        if not completion:\n",
    "            completion = \"\"\n",
    "\n",
    "        print(\n",
    "            self.role,\n",
    "            f\"I executed the given plan and got the follow results:\\n\\n{action_result}\",\n",
    "        )\n",
    "        return {\"rcode\": rcode, \"action_result\": action_result, \"code\": completion, \"error_logs\": error_logs}\n",
    "\n",
    "\n",
    "    def self_validation(self, filename):\n",
    "        rcode, log = execute_script(filename, device=self.device)\n",
    "        return rcode, log\n",
    "\n",
    "class Reflexion:\n",
    "\n",
    "    def __init__(self, specs, code):\n",
    "\n",
    "        self.specs = specs \n",
    "        self.code = code \n",
    "\n",
    "    def self_reflexion(self):\n",
    "        \"\"\" \n",
    "        This is for self-reflexion to self check code to make sure logic + syntax is right before executive\n",
    "        \"\"\"\n",
    "\n",
    "        agent_profile = \"\"\" \n",
    "                    You are a Senior Python Code Reviewer and Quality Assurance Engineer\n",
    "                    \n",
    "                    Objective: \n",
    "\n",
    "                    Audit the generated code to ensure it is runnable, efficient, and error-free. You must strictly enforce the following rules:\n",
    "                    \n",
    "                    1. Logical Correctness: Verify the code logic is sound. Ensure variables are defined before use and data flows correctly through the pipeline.\n",
    "                    2. Strict Import Verification (CRITICAL):\n",
    "                        - No \"Ghost\" Imports: Scan the code for every external class or function used.\n",
    "                        - Mapping Check: You must verify that a corresponding import or from ... import ... statement exists at the top of the file for every single tool used.\n",
    "                    \n",
    "                    3. Structural Efficiency: Ensure all classes and functions have a clear purpose. Remove any redundant code or \"pass\" blocks that do not contribute to the solution.\n",
    "                    4. Requirement Compliance: The code must produce the exact output format requested by the user (e.g., specific CSV filenames or print statements).\n",
    "                    5. Using complete: Ensure all library from import is using right syntax in the solution.  \n",
    "                    \"\"\"\n",
    "\n",
    "        prompt = \"\"\" \n",
    "                    Carefully read the requirement for task: \n",
    "                    {} \n",
    "\n",
    "                    Carefully read the description of data for task\n",
    "                    {} \n",
    "\n",
    "                    And review code result below: \n",
    "                    {}\n",
    "\n",
    "                    If there is any error from the previous attempt, please carefully fix it first. Delete all columns uesless!\n",
    "\n",
    "                    Response return will have two components (in two list):\n",
    "                    - Error you check and solution\n",
    "                    - Full code complete \n",
    "                \"\"\"    \n",
    "\n",
    "        prompt = prompt.format(\n",
    "                    self.specs['requirements'],\n",
    "                    self.specs['data_description'],\n",
    "                    self.code\n",
    "        )\n",
    "\n",
    "        response = call_llm(agent_profile, prompt) \n",
    "        return response \n",
    "\n",
    "class DataAgent(BaseAgent):\n",
    "    def __init__(self, specs):\n",
    "        system = (\n",
    "            \"TASK: Write a robust Python script to Find, Load, and Preprocess data.\\n\"\n",
    "            \"STRICT RULES:\\n\"\n",
    "            \"1. USE `os.walk` to find the folder matching the Contest Name.\\n\"\n",
    "            \"2. CHECK file count using `glob`. IF 1 file -> Split 80/20. IF 2 files -> Load separately.\\n\"\n",
    "            \"3. PREPROCESS: Handle missing values and categorical encoding and delete cols useless.\\n\"\n",
    "            \"4. OUTPUT: Save 'processed_train.csv' and 'processed_test.csv'.\\n\"\n",
    "            \"5. PRINT 'DATA_READY: processed_train.csv processed_test.csv' at the very end.\\n\"\n",
    "            \"OUTPUT FORMAT: Single valid ```python block.\"\n",
    "        )\n",
    "\n",
    "        agent_profile = \"\"\"You are the world's best data scientist of an automated machine learning project (AutoML) that can find the most relevant datasets,run useful preprocessing, perform suitable data augmentation, and make meaningful visulaization to comprehensively understand the data based on the user requirements. You have the following main responsibilities to complete.\n",
    "                        1. Retrieve a dataset from the user or search for the dataset based on the user instruction.\n",
    "                        2. Perform data preprocessing based on the user instruction or best practice based on the given tasks.\n",
    "                        3. Perform data augmentation as neccesary.\n",
    "                        4. Extract useful information and underlying characteristics of the dataset.\"\"\"\n",
    "\n",
    "        super().__init__(\"Data_Engineering\", system, specs, agent_profile)\n",
    "\n",
    "\n",
    "# --- 3. THE MANAGER (WORKFLOW ORCHESTRATOR) ---\n",
    "\n",
    "class CopilotManager:\n",
    "    def __init__(self, specs):\n",
    "        self.specs = specs\n",
    "        self.data_agent = DataAgent(specs)\n",
    "        # self.model_agent = ModelAgent(specs)\n",
    "\n",
    "    def run_agent_cycle(self, agent, max_retries=3):\n",
    "        \"\"\"\n",
    "        The Core Loop: Gen -> Exec -> Fix -> Retry\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸ”¹ STARTED AGENT: {agent.role}\")\n",
    "\n",
    "        print(f\"\\n Generating plan and code for: {agent.role} \")\n",
    "\n",
    "        code = agent.make_plan(self.specs[\"data_description\"], self.specs[\"requirements\"], self.data_agent.system)\n",
    "        \n",
    "        print(\"__\"* 69 )\n",
    "        print(f\"\\n Self-reflexion code for: {agent.role} \")\n",
    "\n",
    "        # re_code = Reflexion(self.specs, code)\n",
    "        # print(re_code)\n",
    "\n",
    "        # print(f\"\\n Generating code and self-reflexion for: {agent.role} \")\n",
    "        # # results = agent.implement_solution(plan)\n",
    "        # results = agent.run_code(code)\n",
    "            \n",
    "        return code\n",
    "\n",
    "    def start_workflow(self):\n",
    "        print(f\"ðŸš€ STARTING WORKFLOW: {self.specs['name']}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # ==========================================\n",
    "        # 1. DATA AGENT PHASE\n",
    "        # ==========================================\n",
    "        output = self.run_agent_cycle(self.data_agent)\n",
    "\n",
    "        return output\n",
    "        \n",
    "# --- 4. EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Requirement Spec\n",
    "    my_challenge = {\n",
    "        \"name\": \"Titanic\",\n",
    "        \"requirements\": \"Predict survival (0/1)\",\n",
    "        \n",
    "        \"data_description\": \"\"\"\n",
    "        The dataset contains the following columns:\n",
    "        - PassengerId: Unique ID (Drop this for training)\n",
    "        - Survived: Target variable (0 = No, 1 = Yes)\n",
    "        - Pclass: Ticket class (1, 2, 3). Treat as Ordinal.\n",
    "        - Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')\n",
    "        - Sex: 'male'/'female'. (Map to 0/1)\n",
    "        - Age: Numeric. Contains missing values (Fill with Median by Title).\n",
    "        - Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')\n",
    "        - Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode).\n",
    "        \"\"\",\n",
    "        \n",
    "        \"target_metric\": \"Accuracy\",\n",
    "        \"output_expectations\": \"submission.csv with PassengerId, Survived\"\n",
    "    }\n",
    "\n",
    "    # Run\n",
    "    manager = CopilotManager(my_challenge)\n",
    "    # manager.start_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e78d8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ STARTED AGENT: Data_Engineering\n",
      "\n",
      " Generating plan and code for: Data_Engineering \n",
      "__________________________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "data_agent = DataAgent(my_challenge)\n",
    "a = manager.run_agent_cycle(data_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b381572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As a Senior Python Code Reviewer and Quality Assurance Engineer, I\\'ve reviewed the provided code and identified some issues that need to be addressed. Here are my findings:\\n\\n**Error 1:**\\nThe code is trying to split a single file into train and test sets using `train_test_split`. However, this approach is not suitable for a single file. Instead, we should use stratified splitting or simply use the entire dataset as the training set.\\n\\n**Solution:** Remove the `train_test_split` variable and use the entire dataset as the training set.\\n\\n**Error 2:**\\nThe code is trying to load two files (train and test) when there\\'s only one file. This will cause an error when trying to split the data into train and test sets.\\n\\n**Solution:** Load the single file and use it as both the training and testing sets.\\n\\n**Error 3:**\\nThe code is not handling missing values in the `Age` column correctly. The median imputer should be used with caution, as it can introduce bias if there are many missing values.\\n\\n**Solution:** Use a more robust method to handle missing values in the `Age` column, such as mean or median imputation only for specific groups (e.g., by title).\\n\\n**Error 4:**\\nThe code is not checking if the preprocessing steps are applied correctly. We should verify that the preprocessing steps are working as expected.\\n\\n**Solution:** Add assertions to ensure that the preprocessing steps are correct.\\n\\nHere\\'s the corrected code:\\n\\n```\\nimport os\\nimport glob\\nimport pandas as pd\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.pipeline import Pipeline\\n\\n# Set root path to find the dataset folder\\nroot_path = \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets\"\\n\\n# Find the folder matching the Contest Name\\ncontest_folder = [folder for folder in os.listdir(root_path) if \"titanic\" in folder][0]\\ndata_folder = os.path.join(root_path, contest_folder)\\n\\n# Check file count using glob\\nfiles = glob.glob(os.path.join(data_folder, \"*.csv\"))\\nfile_count = len(files)\\nif file_count == 1:\\n    # Load the single file as both train and test sets\\n    data_file = files[0]\\n    train_data = pd.read_csv(os.path.join(data_folder, data_file))\\n    test_data = train_data.copy()\\nelif file_count > 1:\\n    raise ValueError(\"Invalid number of files found\")\\n\\n# Define preprocessing steps\\ncategorical_cols = [\"Pclass\", \"Sex\", \"Embarked\"]\\nnumerical_cols = [\"Age\"]\\n\\n# Preprocess categorical columns\\ncategorical_transformer = Pipeline(steps=[\\n    (\\'imputer\\', SimpleImporter(strategy=\\'constant\\', fill_value=\\'Unknown\\')),\\n    (\\'encoder\\', OrdinalEncoder(categories=[[\\'1st\\', \\'2nd\\', \\'3rd\\'], [\\'male\\', \\'female\\'], [\\'C\\', \\'Q\\', \\'S\\']]))\\n])\\n\\n# Preprocess numerical columns\\nnumerical_transformer = Pipeline(steps=[\\n    (\\'imputer\\', SimpleImputer(strategy=\\'median\\'))\\n])\\n\\npreprocessor = ColumnTransformer(\\n    transformers=[\\n        (\\'categorical\\', categorical_transformer, categorical_cols),\\n        (\\'numerical\\', numerical_transformer, numerical_cols)\\n    ]\\n)\\n\\ntrain_data_processed = preprocessor.fit_transform(train_data.drop(\"PassengerId\", axis=1))\\ntest_data_processed = preprocessor.transform(test_data.drop(\"PassengerId\", axis=1))\\n\\n# Verify preprocessing steps\\nassert train_data_processed.shape[0] == train_data.shape[0]\\nassert test_data_processed.shape[0] == test_data.shape[0]\\n\\n# Save processed data\\ntrain_data_processed_df = pd.DataFrame(train_data_processed, columns=categorical_cols + numerical_cols)\\ntest_data_processed_df = pd.DataFrame(test_data_processed, columns=categorical_cols + numerical_cols)\\n\\ntrain_data_processed_df.to_csv(\\'processed_train.csv\\', index=False)\\ntest_data_processed_df.to_csv(\\'processed_test.csv\\', index=False)\\n\\nprint(\"DATA_READY: processed_train.csv processed_test.csv\")\\n```\\n\\nThis code should now be free of errors and produce the expected output.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if \"```Python\" in a:\n",
    "    code = a.split(\"```Python\")[1].split(\"```\")[0]\n",
    "    review = Reflexion(my_challenge, code)\n",
    "    re_code = review.self_reflexion()\n",
    "\n",
    "re_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31d6a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = re_code.split(\"the corrected code:\\n\\n```\")[1].split(\"```\")[0]\n",
    "n = m.strip(\"\\n\")\n",
    "\n",
    "\n",
    "with open('/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/foundation-ml/bucket-code/re_write.py', \"wt\") as file:\n",
    "                    file.write(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cecb201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_READY: processed_train.csv processed_test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define constants\n",
    "CONTEST_NAME = 'Titanic'\n",
    "TRAIN_FILE = f'{CONTEST_NAME}_train.csv'\n",
    "TEST_FILE = f'{CONTEST_NAME}_test.csv'\n",
    "\n",
    "def find_files(contest_name):\n",
    "    root_dir = '/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets'\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        if contest_name in dirpath:\n",
    "            return [os.path.join(dirpath, f) for f in filenames]\n",
    "    return []\n",
    "\n",
    "def preprocess_data(train_file, test_file):\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(test_file)\n",
    "\n",
    "    categorical_cols = ['Pclass', 'Sex', 'Embarked']\n",
    "    numerical_cols = ['Age']\n",
    "\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    categorical_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('categorical', categorical_encoder, categorical_cols),\n",
    "            ('numerical', imputer, numerical_cols)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_df[['Age']] = preprocessor.fit_transform(train_df[['Age']])\n",
    "    test_df[['Age']] = preprocessor.transform(test_df[['Age']])\n",
    "\n",
    "    train_df['Title'] = train_df['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip().title())\n",
    "    test_df['Title'] = test_df['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip().title())\n",
    "\n",
    "    sex_mapping = {'male': 0, 'female': 1}\n",
    "    train_df['Sex'] = train_df['Sex'].map(sex_mapping)\n",
    "    test_df['Sex'] = test_df['Sex'].map(sex_mapping)\n",
    "\n",
    "    cabin_mapping = {'': 'Unknown'}\n",
    "    train_df['Cabin'] = train_df['Cabin'].apply(lambda x: x[0] if pd.notna(x) else 'Unknown')\n",
    "    test_df['Cabin'] = test_df['Cabin'].apply(lambda x: x[0] if pd.notna(x) else 'Unknown')\n",
    "\n",
    "    embarked_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    train_df[['Embarked']] = embarked_encoder.fit_transform(train_df[['Embarked']])\n",
    "    test_df[['Embarked']] = embarked_encoder.transform(test_df[['Embarked']])\n",
    "\n",
    "    train_df.drop('PassengerId', axis=1, inplace=True)\n",
    "    test_df.drop('PassengerId', axis=1, inplace=True)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "# Find files\n",
    "files = find_files(CONTEST_NAME)\n",
    "if len(files) == 1:\n",
    "    file_count = 1\n",
    "    train_file, test_file = files[0], None\n",
    "elif len(files) == 2:\n",
    "    file_count = 2\n",
    "    train_file, test_file = files\n",
    "else:\n",
    "    print(\"Error: No matching files found.\")\n",
    "    exit()\n",
    "\n",
    "# Split data if necessary\n",
    "if file_count == 1:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_df, test_df = train_test_split(pd.read_csv(train_file), test_size=0.2)\n",
    "    train_file, test_file = 'processed_train.csv', 'processed_test.csv'\n",
    "else:\n",
    "    train_df, test_df = preprocess_data(train_file, test_file)\n",
    "\n",
    "# Save preprocessed data\n",
    "train_df.to_csv('processed_train.csv', index=False)\n",
    "test_df.to_csv('processed_test.csv', index=False)\n",
    "\n",
    "print(\"DATA_READY: processed_train.csv processed_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72e05386",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories [1, 2, 3] in column 0 during fit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 49\u001b[0m\n\u001b[1;32m     38\u001b[0m numerical_transformer \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     39\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m'\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     40\u001b[0m ])\n\u001b[1;32m     42\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer(\n\u001b[1;32m     43\u001b[0m     transformers\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     44\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m, categorical_transformer, categorical_cols),\n\u001b[1;32m     45\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumerical\u001b[39m\u001b[38;5;124m'\u001b[39m, numerical_transformer, numerical_cols)\n\u001b[1;32m     46\u001b[0m     ]\n\u001b[1;32m     47\u001b[0m )\n\u001b[0;32m---> 49\u001b[0m train_data_processed \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPassengerId\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m test_data_processed \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mtransform(test_data\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassengerId\u001b[39m\u001b[38;5;124m\"\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Verify preprocessing steps\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:976\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_empty_routing()\n\u001b[0;32m--> 976\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_func_on_transformers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_fit_transform_one\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_as_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fitted_transformers([])\n",
      "File \u001b[0;32m~/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:885\u001b[0m, in \u001b[0;36mColumnTransformer._call_func_on_transformers\u001b[0;34m(self, X, y, func, column_as_labels, routed_params)\u001b[0m\n\u001b[1;32m    873\u001b[0m             extra_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    874\u001b[0m         jobs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    875\u001b[0m             delayed(func)(\n\u001b[1;32m    876\u001b[0m                 transformer\u001b[38;5;241m=\u001b[39mclone(trans) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted \u001b[38;5;28;01melse\u001b[39;00m trans,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    882\u001b[0m             )\n\u001b[1;32m    883\u001b[0m         )\n\u001b[0;32m--> 885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[0;32m~/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hanoi/lib/python3.10/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/anaconda3/envs/hanoi/lib/python3.10/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/pipeline.py:1310\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1310\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1312\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   1313\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m   1314\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/pipeline.py:541\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    539\u001b[0m last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(last_step, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlast_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m last_step\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m    546\u001b[0m         Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1084\u001b[0m             (\n\u001b[1;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m         )\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:1496\u001b[0m, in \u001b[0;36mOrdinalEncoder.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1490\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown_value should only be set when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1491\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandle_unknown is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_encoded_value\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1492\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munknown_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1493\u001b[0m     )\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;66;03m# `_fit` will only raise an error when `self.handle_unknown=\"error\"`\u001b[39;00m\n\u001b[0;32m-> 1496\u001b[0m fit_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_and_ignore_missing_for_infrequent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_missing_indices \u001b[38;5;241m=\u001b[39m fit_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1504\u001b[0m cardinalities \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(categories) \u001b[38;5;28;01mfor\u001b[39;00m categories \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories_]\n",
      "File \u001b[0;32m~/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:160\u001b[0m, in \u001b[0;36m_BaseEncoder._fit\u001b[0;34m(self, X, handle_unknown, force_all_finite, return_counts, return_and_ignore_missing_for_infrequent)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[1;32m    156\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    157\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound unknown categories \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m in column \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m during fit\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(diff, i)\n\u001b[1;32m    159\u001b[0m         )\n\u001b[0;32m--> 160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_counts:\n\u001b[1;32m    162\u001b[0m     category_counts\u001b[38;5;241m.\u001b[39mappend(_get_counts(Xi, cats))\n",
      "\u001b[0;31mValueError\u001b[0m: Found unknown categories [1, 2, 3] in column 0 during fit"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Set root path to find the dataset folder\n",
    "root_path = \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets\"\n",
    "\n",
    "# Find the folder matching the Contest Name\n",
    "contest_folder = [folder for folder in os.listdir(root_path) if \"titanic\" in folder][0]\n",
    "data_folder = os.path.join(root_path, contest_folder)\n",
    "\n",
    "# Check file count using glob\n",
    "files = glob.glob(os.path.join(data_folder, \"*.csv\"))\n",
    "file_count = len(files)\n",
    "if file_count == 1:\n",
    "    # Load the single file as both train and test sets\n",
    "    data_file = files[0]\n",
    "    train_data = pd.read_csv(os.path.join(data_folder, data_file))\n",
    "    test_data = train_data.copy()\n",
    "elif file_count > 1:\n",
    "    raise ValueError(\"Invalid number of files found\")\n",
    "\n",
    "# Define preprocessing steps\n",
    "categorical_cols = [\"Pclass\", \"Sex\", \"Embarked\"]\n",
    "numerical_cols = [\"Age\"]\n",
    "\n",
    "# Preprocess categorical columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('encoder', OrdinalEncoder(categories=[['1st', '2nd', '3rd'], ['male', 'female'], ['C', 'Q', 'S']]))\n",
    "])\n",
    "\n",
    "# Preprocess numerical columns\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('categorical', categorical_transformer, categorical_cols),\n",
    "        ('numerical', numerical_transformer, numerical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_data_processed = preprocessor.fit_transform(train_data.drop(\"PassengerId\", axis=1))\n",
    "test_data_processed = preprocessor.transform(test_data.drop(\"PassengerId\", axis=1))\n",
    "\n",
    "# Verify preprocessing steps\n",
    "assert train_data_processed.shape[0] == train_data.shape[0]\n",
    "assert test_data_processed.shape[0] == test_data.shape[0]\n",
    "\n",
    "# Save processed data\n",
    "train_data_processed_df = pd.DataFrame(train_data_processed, columns=categorical_cols + numerical_cols)\n",
    "test_data_processed_df = pd.DataFrame(test_data_processed, columns=categorical_cols + numerical_cols)\n",
    "\n",
    "train_data_processed_df.to_csv('processed_train.csv', index=False)\n",
    "test_data_processed_df.to_csv('processed_test.csv', index=False)\n",
    "\n",
    "print(\"DATA_READY: processed_train.csv processed_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a8c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelAgent(BaseAgent):\n",
    "    def __init__(self, specs):\n",
    "        self.specs = specs\n",
    "        # Placeholder system prompt, will be updated when data is ready\n",
    "        self.train_file = \"processed_train.csv\"\n",
    "        self.test_file = \"processed_test.csv\"\n",
    "        self._update_system()\n",
    "\n",
    "    def _update_system(self):\n",
    "        system = (\n",
    "            f\"You are a Kaggle Grandmaster. Goal: Maximize {self.specs['target_metric']}.\\n\"\n",
    "            f\"INPUT FILES: {self.train_file}, {self.test_file}\\n\"\n",
    "            \"TASK: Write Python code to Train a Model and Generate Submission.\\n\"\n",
    "            \"STEPS:\\n\"\n",
    "            \"1. Load the processed CSV files.\\n\"\n",
    "            \"2. Train a robust model (RandomForest/XGBoost/LightGBM).\\n\"\n",
    "            \"3. Evaluate on internal validation set.\\n\"\n",
    "            \"4. Generate 'submission.csv'.\\n\"\n",
    "            \"5. PRINT 'FINAL_SCORE: <score>' at the end.\\n\"\n",
    "            \"OUTPUT FORMAT: Single valid ```python block.\"\n",
    "        )\n",
    "        super().__init__(\"Model Training\", system)\n",
    "    \n",
    "    def set_data_files(self, train, test):\n",
    "        self.train_file = train\n",
    "        self.test_file = test\n",
    "        self._update_system()\n",
    "\n",
    "\n",
    "    def generate_initial_code(self, plan):\n",
    "        \"\"\"Step 1: Create code from scratch.\"\"\"\n",
    "        prompt = \"\"\"Write the Python code for the {} task based on your System Instructions:\n",
    "                    {}\"\"\"\n",
    "\n",
    "        prompt = prompt.format(\n",
    "                        self.role, \n",
    "                        plan\n",
    "                    )\n",
    "\n",
    "        return call_llm(self.system, prompt)\n",
    "\n",
    "    def fix_code(self, broken_code, error_log):\n",
    "        \"\"\"Step 2: Self-Reflection & Fix.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        ### SELF-REFLECTION & FIX TASK\n",
    "        Your previous code failed to execute.\n",
    "        \n",
    "        **1. THE BROKEN CODE:**\n",
    "        ```python\n",
    "        {broken_code}\n",
    "        ```\n",
    "        **2. THE ERROR LOG:**\n",
    "        {error_log}\n",
    "        \n",
    "        **INSTRUCTION:**\n",
    "        - Analyze the error log carefully.\n",
    "        - Rewrite the code to fix the specific bug.\n",
    "        - Return the COMPLETE corrected Python script.\n",
    "        \"\"\"\n",
    "        return call_llm(self.system, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a16261c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m find_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitanic\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m train_df, test_df \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Preprocess data\u001b[39;00m\n\u001b[1;32m     69\u001b[0m processed_train_df \u001b[38;5;241m=\u001b[39m preprocess_data(train_df)\n",
      "Cell \u001b[0;32mIn[82], line 25\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(data_dir):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Check file count using glob. IF 1 file -> Split 80/20. IF 2 files -> Load separately.\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     files \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m*.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(files) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     27\u001b[0m         train_test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(files[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/hanoi/lib/python3.10/posixpath.py:76\u001b[0m, in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(a, \u001b[38;5;241m*\u001b[39mp):\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Join two or more pathname components, inserting '/' as needed.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    If any component is an absolute path, all previous path components\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    will be discarded.  An empty last part will result in a path that\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    ends with a separator.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     sep \u001b[38;5;241m=\u001b[39m _get_sep(a)\n\u001b[1;32m     78\u001b[0m     path \u001b[38;5;241m=\u001b[39m a\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Task: Write a robust Python script to Find, Load, and Preprocess data.\n",
    "# Follow these instructions:\n",
    "# 1. Use `os.walk` to find the folder matching the Contest Name.\n",
    "# 2. Check file count using `glob`. IF 1 file -> Split 80/20. IF 2 files -> Load separately.\n",
    "# 3. Preprocess: Handle missing values and categorical encoding, delete useless cols.\n",
    "# 4. Output: Save 'processed_train.csv' and 'processed_test.csv'.\n",
    "# 5. Print 'DATA_READY: processed_train.csv processed_test.csv' at the very end.\n",
    "\n",
    "def find_data(contest_name):\n",
    "    # Use os.walk to find the folder matching the Contest Name\n",
    "    for root, dirs, files in os.walk('/path/to/contest'):\n",
    "        if contest_name in root:\n",
    "            return root\n",
    "\n",
    "def load_data(data_dir):\n",
    "    # Check file count using glob. IF 1 file -> Split 80/20. IF 2 files -> Load separately.\n",
    "    files = glob.glob(os.path.join(data_dir, '*.csv'))\n",
    "    if len(files) == 1:\n",
    "        train_test_df = pd.read_csv(files[0])\n",
    "        train_df, test_df = train_test_split(train_test_df, test_size=0.2, random_state=42)\n",
    "    elif len(files) == 2:\n",
    "        train_df = pd.read_csv(files[0])\n",
    "        test_df = pd.read_csv(files[1])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid number of files found\")\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def preprocess_data(train_df):\n",
    "    # Preprocess: Handle missing values and categorical encoding, delete useless cols.\n",
    "    # Extract Title from Name\n",
    "    train_df['Title'] = train_df['Name'].apply(lambda x: x.split(', ')[0].strip().title())\n",
    "\n",
    "    # Map Sex to 0/1\n",
    "    sex_mapping = {'male': 0, 'female': 1}\n",
    "    train_df['Sex'] = train_df['Sex'].map(sex_mapping)\n",
    "\n",
    "    # Fill missing Age values with Median by Title\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    age_pipeline = Pipeline(steps=[('imputer', imputer)])\n",
    "    train_df[['Age']] = age_pipeline.fit_transform(train_df[['Age']])\n",
    "\n",
    "    # Extract first letter of Cabin as 'Deck'\n",
    "    train_df['Cabin'] = train_df['Cabin'].apply(lambda x: x[0].upper() if pd.notna(x) else 'Unknown')\n",
    "\n",
    "    # One-Hot Encode Embarked\n",
    "    embarked_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    embarked_pipeline = Pipeline(steps=[('encoder', embarked_encoder)])\n",
    "    embarked_pipeline.fit(train_df[['Embarked']])\n",
    "    train_df[['Embarked']] = embarked_pipeline.transform(train_df[['Embarked']])\n",
    "\n",
    "    return train_df\n",
    "\n",
    "# Find the folder matching the Contest Name\n",
    "data_dir = find_data('titanic')\n",
    "\n",
    "# Load data\n",
    "train_df, test_df = load_data(data_dir)\n",
    "\n",
    "# Preprocess data\n",
    "processed_train_df = preprocess_data(train_df)\n",
    "\n",
    "# Save preprocessed data\n",
    "processed_train_df.to_csv('processed_train.csv', index=False)\n",
    "test_df.to_csv('processed_test.csv', index=False)\n",
    "\n",
    "print(\"DATA_READY: processed_train.csv processed_test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793067e0",
   "metadata": {},
   "source": [
    "## Second attempt\n",
    "\n",
    "REDESIGN WORKFLOW from FIRST ATTEMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b748e5c9",
   "metadata": {},
   "source": [
    "1. Based on previous foundation attempt, I created a workflow basis: \n",
    "```Gen -> Self check -> Exec -> Fix -> Retry. ```\n",
    "\n",
    "2. But in real life, this is quite not suitable, circle in second will focus on loop: ``` gen -> self check -> exec -> re-gen.... ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d37a6870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ STARTING WORKFLOW: Titanic\n",
      "\n",
      "ðŸ”¹ STARTED AGENT: Data_Engineering\n",
      "\n",
      " Generating plan and code for: Data_Engineering \n",
      "Data_Engineering I am implementing the following instruction\n",
      "Attempt:  0\n",
      "Devise an end-to-end actionable plan for Data_Engineering task according to the user's requirements described in the following JSON object.\n",
      "\n",
      "        # User Requirements & Data Info\n",
      "        ```json\n",
      "        {\n",
      "            \"Data Description\": \"\n",
      "        The dataset contains the following columns:\n",
      "        - PassengerId: Unique ID (Drop this for training)\n",
      "        - Survived: Target variable (0 = No, 1 = Yes)\n",
      "        - Pclass: Ticket class (1, 2, 3). Treat as Ordinal.\n",
      "        - Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')\n",
      "        - Sex: 'male'/'female'. (Map to 0/1)\n",
      "        - Age: Numeric. Contains missing values (Fill with Median by Title).\n",
      "        - Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')\n",
      "        - Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode).\n",
      "        \",\n",
      "            \"Requirements\": \"Predict survival (0/1)\"\n",
      "            \"Root path data\": \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets\"\n",
      "        }\n",
      "        ```\n",
      "        When devising a plan, follow these instructions and do not forget them:\n",
      "        TASK: Write a robust Python script to Find, Load, and Preprocess data.\n",
      "STRICT RULES:\n",
      "1. USE `os.walk` to find the folder matching the Contest Name.\n",
      "2. CHECK file count using `glob`. IF 1 file -> Split 80/20. IF 2 files -> Load separately.\n",
      "3. PREPROCESS: Handle missing values and categorical encoding and delete cols useless.\n",
      "4. OUTPUT: Save 'processed_train.csv' and 'processed_test.csv'.\n",
      "5. PRINT 'DATA_READY: processed_train.csv processed_test.csv' at the very end.\n",
      "OUTPUT FORMAT: Single valid ```python block.\n",
      "                        \n",
      "        # Error from the Previously Written Code\n",
      "                        Nothing. This is your first attempt.\n",
      "        \n",
      "Data_Engineering I got this error (itr #0): The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/foundation-ml/bucket-code/Data_Engineering.py\", line 17, in <module>\n",
      "    train_file = glob.glob(os.path.join(root_path, contest_folder, \"*_train.csv\"))[0]\n",
      "IndexError: list index out of range\n",
      "\n",
      "Attempt:  1\n",
      "Devise an end-to-end actionable plan for Data_Engineering task according to the user's requirements described in the following JSON object.\n",
      "\n",
      "        # User Requirements & Data Info\n",
      "        ```json\n",
      "        {\n",
      "            \"Data Description\": \"\n",
      "        The dataset contains the following columns:\n",
      "        - PassengerId: Unique ID (Drop this for training)\n",
      "        - Survived: Target variable (0 = No, 1 = Yes)\n",
      "        - Pclass: Ticket class (1, 2, 3). Treat as Ordinal.\n",
      "        - Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')\n",
      "        - Sex: 'male'/'female'. (Map to 0/1)\n",
      "        - Age: Numeric. Contains missing values (Fill with Median by Title).\n",
      "        - Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')\n",
      "        - Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode).\n",
      "        \",\n",
      "            \"Requirements\": \"Predict survival (0/1)\"\n",
      "            \"Root path data\": \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets\"\n",
      "        }\n",
      "        ```\n",
      "        When devising a plan, follow these instructions and do not forget them:\n",
      "        TASK: Write a robust Python script to Find, Load, and Preprocess data.\n",
      "STRICT RULES:\n",
      "1. USE `os.walk` to find the folder matching the Contest Name.\n",
      "2. CHECK file count using `glob`. IF 1 file -> Split 80/20. IF 2 files -> Load separately.\n",
      "3. PREPROCESS: Handle missing values and categorical encoding and delete cols useless.\n",
      "4. OUTPUT: Save 'processed_train.csv' and 'processed_test.csv'.\n",
      "5. PRINT 'DATA_READY: processed_train.csv processed_test.csv' at the very end.\n",
      "OUTPUT FORMAT: Single valid ```python block.\n",
      "                        \n",
      "        # Error from the Previously Written Code\n",
      "                        The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/foundation-ml/bucket-code/Data_Engineering.py\", line 17, in <module>\n",
      "    train_file = glob.glob(os.path.join(root_path, contest_folder, \"*_train.csv\"))[0]\n",
      "IndexError: list index out of range\n",
      "\n",
      "        \n",
      "Data_Engineering I got this error (itr #1): The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/foundation-ml/bucket-code/Data_Engineering.py\", line 29, in <module>\n",
      "    raise ValueError(\"Invalid file count\")\n",
      "ValueError: Invalid file count\n",
      "\n",
      "Attempt:  2\n",
      "Devise an end-to-end actionable plan for Data_Engineering task according to the user's requirements described in the following JSON object.\n",
      "\n",
      "        # User Requirements & Data Info\n",
      "        ```json\n",
      "        {\n",
      "            \"Data Description\": \"\n",
      "        The dataset contains the following columns:\n",
      "        - PassengerId: Unique ID (Drop this for training)\n",
      "        - Survived: Target variable (0 = No, 1 = Yes)\n",
      "        - Pclass: Ticket class (1, 2, 3). Treat as Ordinal.\n",
      "        - Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')\n",
      "        - Sex: 'male'/'female'. (Map to 0/1)\n",
      "        - Age: Numeric. Contains missing values (Fill with Median by Title).\n",
      "        - Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')\n",
      "        - Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode).\n",
      "        \",\n",
      "            \"Requirements\": \"Predict survival (0/1)\"\n",
      "            \"Root path data\": \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets\"\n",
      "        }\n",
      "        ```\n",
      "        When devising a plan, follow these instructions and do not forget them:\n",
      "        TASK: Write a robust Python script to Find, Load, and Preprocess data.\n",
      "STRICT RULES:\n",
      "1. USE `os.walk` to find the folder matching the Contest Name.\n",
      "2. CHECK file count using `glob`. IF 1 file -> Split 80/20. IF 2 files -> Load separately.\n",
      "3. PREPROCESS: Handle missing values and categorical encoding and delete cols useless.\n",
      "4. OUTPUT: Save 'processed_train.csv' and 'processed_test.csv'.\n",
      "5. PRINT 'DATA_READY: processed_train.csv processed_test.csv' at the very end.\n",
      "OUTPUT FORMAT: Single valid ```python block.\n",
      "                        \n",
      "        # Error from the Previously Written Code\n",
      "                        The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/foundation-ml/bucket-code/Data_Engineering.py\", line 29, in <module>\n",
      "    raise ValueError(\"Invalid file count\")\n",
      "ValueError: Invalid file count\n",
      "\n",
      "        \n",
      "Data_Engineering I got this error (itr #2): The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/foundation-ml/bucket-code/Data_Engineering.py\", line 44, in <module>\n",
      "    train_df_processed = preprocessor.fit_transform(train_df)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 313, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py\", line 976, in fit_transform\n",
      "    result = self._call_func_on_transformers(\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py\", line 885, in _call_func_on_transformers\n",
      "    return Parallel(n_jobs=self.n_jobs)(jobs)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 74, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 136, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/pipeline.py\", line 541, in fit_transform\n",
      "    return last_step.fit_transform(\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 313, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/base.py\", line 1098, in fit_transform\n",
      "    return self.fit(X, **fit_params).transform(X)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py\", line 1482, in fit\n",
      "    raise TypeError(\n",
      "TypeError: unknown_value should be an integer or np.nan when handle_unknown is 'use_encoded_value', got None.\n",
      "\n",
      "Attempt:  3\n",
      "Devise an end-to-end actionable plan for Data_Engineering task according to the user's requirements described in the following JSON object.\n",
      "\n",
      "        # User Requirements & Data Info\n",
      "        ```json\n",
      "        {\n",
      "            \"Data Description\": \"\n",
      "        The dataset contains the following columns:\n",
      "        - PassengerId: Unique ID (Drop this for training)\n",
      "        - Survived: Target variable (0 = No, 1 = Yes)\n",
      "        - Pclass: Ticket class (1, 2, 3). Treat as Ordinal.\n",
      "        - Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')\n",
      "        - Sex: 'male'/'female'. (Map to 0/1)\n",
      "        - Age: Numeric. Contains missing values (Fill with Median by Title).\n",
      "        - Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')\n",
      "        - Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode).\n",
      "        \",\n",
      "            \"Requirements\": \"Predict survival (0/1)\"\n",
      "            \"Root path data\": \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets\"\n",
      "        }\n",
      "        ```\n",
      "        When devising a plan, follow these instructions and do not forget them:\n",
      "        TASK: Write a robust Python script to Find, Load, and Preprocess data.\n",
      "STRICT RULES:\n",
      "1. USE `os.walk` to find the folder matching the Contest Name.\n",
      "2. CHECK file count using `glob`. IF 1 file -> Split 80/20. IF 2 files -> Load separately.\n",
      "3. PREPROCESS: Handle missing values and categorical encoding and delete cols useless.\n",
      "4. OUTPUT: Save 'processed_train.csv' and 'processed_test.csv'.\n",
      "5. PRINT 'DATA_READY: processed_train.csv processed_test.csv' at the very end.\n",
      "OUTPUT FORMAT: Single valid ```python block.\n",
      "                        \n",
      "        # Error from the Previously Written Code\n",
      "                        The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/foundation-ml/bucket-code/Data_Engineering.py\", line 44, in <module>\n",
      "    train_df_processed = preprocessor.fit_transform(train_df)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 313, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py\", line 976, in fit_transform\n",
      "    result = self._call_func_on_transformers(\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py\", line 885, in _call_func_on_transformers\n",
      "    return Parallel(n_jobs=self.n_jobs)(jobs)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 74, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 136, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/pipeline.py\", line 541, in fit_transform\n",
      "    return last_step.fit_transform(\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 313, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/base.py\", line 1098, in fit_transform\n",
      "    return self.fit(X, **fit_params).transform(X)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py\", line 1482, in fit\n",
      "    raise TypeError(\n",
      "TypeError: unknown_value should be an integer or np.nan when handle_unknown is 'use_encoded_value', got None.\n",
      "\n",
      "        \n",
      "Data_Engineering I got this error (itr #3): The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/foundation-ml/bucket-code/Data_Engineering.py\", line 18, in <module>\n",
      "    test_file = glob.glob(os.path.join(root_path, contest_folder, \"*_Test.csv\"))[0]\n",
      "IndexError: list index out of range\n",
      "\n",
      "Attempt:  4\n",
      "Devise an end-to-end actionable plan for Data_Engineering task according to the user's requirements described in the following JSON object.\n",
      "\n",
      "        # User Requirements & Data Info\n",
      "        ```json\n",
      "        {\n",
      "            \"Data Description\": \"\n",
      "        The dataset contains the following columns:\n",
      "        - PassengerId: Unique ID (Drop this for training)\n",
      "        - Survived: Target variable (0 = No, 1 = Yes)\n",
      "        - Pclass: Ticket class (1, 2, 3). Treat as Ordinal.\n",
      "        - Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')\n",
      "        - Sex: 'male'/'female'. (Map to 0/1)\n",
      "        - Age: Numeric. Contains missing values (Fill with Median by Title).\n",
      "        - Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')\n",
      "        - Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode).\n",
      "        \",\n",
      "            \"Requirements\": \"Predict survival (0/1)\"\n",
      "            \"Root path data\": \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets\"\n",
      "        }\n",
      "        ```\n",
      "        When devising a plan, follow these instructions and do not forget them:\n",
      "        TASK: Write a robust Python script to Find, Load, and Preprocess data.\n",
      "STRICT RULES:\n",
      "1. USE `os.walk` to find the folder matching the Contest Name.\n",
      "2. CHECK file count using `glob`. IF 1 file -> Split 80/20. IF 2 files -> Load separately.\n",
      "3. PREPROCESS: Handle missing values and categorical encoding and delete cols useless.\n",
      "4. OUTPUT: Save 'processed_train.csv' and 'processed_test.csv'.\n",
      "5. PRINT 'DATA_READY: processed_train.csv processed_test.csv' at the very end.\n",
      "OUTPUT FORMAT: Single valid ```python block.\n",
      "                        \n",
      "        # Error from the Previously Written Code\n",
      "                        The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/foundation-ml/bucket-code/Data_Engineering.py\", line 18, in <module>\n",
      "    test_file = glob.glob(os.path.join(root_path, contest_folder, \"*_Test.csv\"))[0]\n",
      "IndexError: list index out of range\n",
      "\n",
      "        \n",
      "Data_Engineering I executed the given plan and got the follow results:\n",
      "\n",
      "The script has been executed. Here is the output:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import selectors \n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL_NAME = \"llama3\"  # Ensure you have this model: `ollama pull llama3`\n",
    "WORKSPACE_DIR = \"./contest_workspace\"\n",
    "PYTHON_EXEC = sys.executable \n",
    "\n",
    "# ROOT DATA PATH (Adjust this to your actual path)\n",
    "ALL_DATA_PATH = \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets\" \n",
    "\n",
    "# Setup Directories\n",
    "os.makedirs(WORKSPACE_DIR, exist_ok=True)\n",
    "os.makedirs(ALL_DATA_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- 1. LLM COMMUNICATION LAYER ---\n",
    "def call_llm(system_prompt, user_prompt):\n",
    "    \"\"\"Sends request to Ollama with a strict JSON payload.\"\"\"\n",
    "    full_prompt = f\"System: {system_prompt}\\nUser: {user_prompt}\"\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME, \n",
    "        \"prompt\": full_prompt, \n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.2, # Low temp for precise coding\n",
    "            \"num_ctx\": 16384,    # Large context for code + error logs\n",
    "            \"stop\": [\"User:\", \"System:\", \"```python\\n\\n\"] # Stop tokens\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        print(\"   ðŸ§  (Agent is thinking...)\", end=\"\\r\")\n",
    "        response = requests.post(OLLAMA_URL, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json().get('response', '')\n",
    "    except Exception as e:\n",
    "        return f\"LLM_ERROR: {str(e)}\"\n",
    "\n",
    "def execute_script(script_name, work_dir = \".\", device=\"0\"):    \n",
    "    if not os.path.exists(os.path.join(work_dir, script_name)):\n",
    "        raise Exception(f\"The file {script_name} does not exist.\")\n",
    "    try:\n",
    "        script_path = script_name\n",
    "        device = device        \n",
    "        cmd = f\"CUDA_VISIBLE_DEVICES={device} python -u {script_path}\"\n",
    "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, shell=True, cwd=work_dir)\n",
    "\n",
    "        stdout_lines = []\n",
    "        stderr_lines = []\n",
    "\n",
    "        selector = selectors.DefaultSelector()\n",
    "        selector.register(process.stdout, selectors.EVENT_READ)\n",
    "        selector.register(process.stderr, selectors.EVENT_READ)\n",
    "\n",
    "        while process.poll() is None and selector.get_map():\n",
    "            events = selector.select(timeout=1)\n",
    "\n",
    "            for key, _ in events:\n",
    "                line = key.fileobj.readline()\n",
    "                if key.fileobj == process.stdout:\n",
    "                    # print(\"STDOUT:\", line, end =\" \")\n",
    "                    stdout_lines.append(line)\n",
    "                else:\n",
    "                    # print(\"STDERR:\", line, end =\" \")\n",
    "                    stderr_lines.append(line)\n",
    "\n",
    "        for line in process.stdout:\n",
    "            line = line\n",
    "            # print(\"STDOUT:\", line, end =\" \")\n",
    "            stdout_lines.append(line)\n",
    "        for line in process.stderr:\n",
    "            line = line\n",
    "            # print(\"STDERR:\", line, end =\" \")\n",
    "            stderr_lines.append(line)\n",
    "\n",
    "        return_code = process.returncode\n",
    "\n",
    "        if return_code != 0:\n",
    "            observation = \"\".join(stderr_lines)\n",
    "        else:\n",
    "            observation = \"\".join(stdout_lines)\n",
    "        if observation == \"\" and return_code == 0:\n",
    "            # printed to stderr only\n",
    "            observation = \"\".join(stderr_lines)\n",
    "        return return_code, \"The script has been executed. Here is the output:\\n\" + observation\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"++++\", \"Wrong!\")\n",
    "        # raise Exception(f\"Something went wrong in executing {script_name}: {e}. Please check if it is ready to be executed.\")\n",
    "        return -1, f\"Something went wrong in executing {script_name}: {e}. Please check if it is ready to be executed.\"\n",
    "\n",
    "\n",
    "class BaseAgent:\n",
    "    \"\"\"Parent class capable of Generating and Fixing code.\"\"\"\n",
    "    def __init__(self, role, system_instruction, specs, agent_profile):\n",
    "        self.specs = specs\n",
    "        self.role = role\n",
    "        self.system = system_instruction\n",
    "        self.code_path = \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/foundation-ml/bucket-code\"\n",
    "        self.root_path_data = \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets\"\n",
    "        self.agent_profile = agent_profile\n",
    "        self.device = 0\n",
    "\n",
    "    def make_plan(self, data_description, requirement, plan_conditions, code=\"\", error_logs=[]):\n",
    "        \n",
    "        log = \"Nothing. This is your first attempt.\"\n",
    "        code = code \n",
    "\n",
    "        prompt = \"\"\"Devise an end-to-end actionable plan for {} task according to the user's requirements described in the following JSON object.\n",
    "\n",
    "        # User Requirements & Data Info\n",
    "        ```json\n",
    "        {{\n",
    "            \"Data Description\": \"{}\",\n",
    "            \"Requirements\": \"{}\"\n",
    "            \"Root path data\": \"{}\"\n",
    "        }}\n",
    "        ```\n",
    "        When devising a plan, follow these instructions and do not forget them:\n",
    "        {}\n",
    "                        \n",
    "        # Error from the Previously Written Code\n",
    "                        {}\n",
    "        \"\"\"\n",
    "        prompt = prompt.format(\n",
    "                self.role,\n",
    "                data_description,\n",
    "                requirement,\n",
    "                self.root_path_data,\n",
    "                self.system,\n",
    "                # code, \n",
    "                error_logs\n",
    "        )\n",
    "\n",
    "        print(prompt)\n",
    "        return call_llm(self.agent_profile, prompt)\n",
    "\n",
    "    def implement_solution(self, code_instructions, code=\"\", n_attempts=5):\n",
    "\n",
    "        print(\n",
    "            self.role,\n",
    "            \"I am implementing the following instruction\"\n",
    "        )\n",
    "\n",
    "        log = \"Nothing. This is your first attempt.\"\n",
    "        error_logs = []\n",
    "        code = code  # if a template/skeleton code is provided\n",
    "        iteration = 0\n",
    "        completion = None\n",
    "        action_result = \"\"\n",
    "        rcode = -1\n",
    "\n",
    "\n",
    "        while iteration < n_attempts:\n",
    "            try:\n",
    "                exec_prompt = \"\"\"Carefully read the following instructions to write Python code for {} task.\n",
    "                {}\n",
    "                \n",
    "                # Previously Written Code\n",
    "                ```python\n",
    "                {}\n",
    "                ```\n",
    "                \n",
    "                # Error from the Previously Written Code\n",
    "                {}\n",
    "                \n",
    "\n",
    "                Note that you need to write the python code for the {}. \n",
    "                Start the python code with \"```python\". Please ensure the completeness of the code so that it can be run without additional modifications.\n",
    "                If there is any error from the previous attempt, please carefully fix it first.\"\"\"\n",
    "       \n",
    "                exec_prompt = exec_prompt.format(\n",
    "                    self.specs['requirements'],\n",
    "                    code_instructions,\n",
    "                    code,\n",
    "                    log,\n",
    "                    \"data processing part\",\n",
    "                )\n",
    "\n",
    "                res = call_llm(self.agent_profile, exec_prompt)\n",
    "                raw_completion = res\n",
    "                completion = raw_completion.split(\"```python\")[1].split(\"```\")[0]\n",
    "                \n",
    "                if not completion.strip(\" \\n\"):\n",
    "                    continue\n",
    "                \n",
    "                self.code_name = self.role \n",
    "                filename = f\"{self.code_path}{self.code_name}.py\"\n",
    "                os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "                with open(filename, \"wt\") as file:\n",
    "                    file.write(completion)\n",
    "                code = completion\n",
    "                rcode, log = self.self_validation(filename)\n",
    "                if rcode == 0:\n",
    "                    action_result = log\n",
    "                    break\n",
    "                else:\n",
    "                    log = log\n",
    "                    error_logs.append(log)\n",
    "                    action_result = log\n",
    "                    print(self.agent_type, f\"I got this error (itr #{iteration}): {log}\")\n",
    "                    iteration += 1                    \n",
    "                    # break\n",
    "            except Exception as e:\n",
    "                iteration += 1\n",
    "                print(self.role, f\"===== Retry: {iteration} =====\")\n",
    "                print(self.role, f\"Executioin error occurs: {e}\")\n",
    "            continue\n",
    "        if not completion:\n",
    "            completion = \"\"\n",
    "\n",
    "        print(\n",
    "            self.role,\n",
    "            f\"I executed the given plan and got the follow results:\\n\\n{action_result}\",\n",
    "        )\n",
    "        return {\"rcode\": rcode, \"action_result\": action_result, \"code\": completion, \"error_logs\": error_logs}\n",
    "\n",
    "\n",
    "    def run_code(self, completion, code=\"\", n_attempts=5):\n",
    "\n",
    "        print(\n",
    "            self.role,\n",
    "            \"I am implementing the following instruction\"\n",
    "        )\n",
    "\n",
    "        log = \"Nothing. This is your first attempt.\"\n",
    "        error_logs = []\n",
    "        code = code  # if a template/skeleton code is provided\n",
    "        iteration = 0\n",
    "        action_result = \"\"\n",
    "        rcode = -1\n",
    "\n",
    "        while iteration < n_attempts:\n",
    "            try:\n",
    "                raw_completion = completion\n",
    "                if \"```python\" in raw_completion:\n",
    "                    completion = raw_completion.split(\"```python\")[1].split(\"```\")[0]\n",
    "                elif \"```Python\" in raw_completion:\n",
    "                    completion = raw_completion.split(\"```Python\")[1].split(\"```\")[0]\n",
    "                else:\n",
    "                    completion = raw_completion.split(\"```\")[1].split(\"```\")[0]\n",
    "\n",
    "                if not completion.strip(\" \\n\"):\n",
    "                    continue\n",
    "                self.code_name = self.role \n",
    "                filename = f\"{self.code_path}/{self.code_name}.py\"\n",
    "                os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "                with open(filename, \"wt\") as file:\n",
    "                    file.write(completion)\n",
    "                code = completion\n",
    "                rcode, log = self.self_validation(filename)\n",
    "                if rcode == 0:\n",
    "                    action_result = log\n",
    "                    break\n",
    "                else:\n",
    "                    log = log\n",
    "                    error_logs.append(log)\n",
    "                    action_result = log\n",
    "                    print(self.role, f\"I got this error (itr #{iteration}): {log}\")\n",
    "                    iteration += 1                    \n",
    "                    # break\n",
    "            except Exception as e:\n",
    "                iteration += 1\n",
    "                print(self.role, f\"===== Retry: {iteration} =====\")\n",
    "                print(self.role, f\"Executioin error occurs: {e}\")\n",
    "            continue\n",
    "        if not completion:\n",
    "            completion = \"\"\n",
    "\n",
    "        print(\n",
    "            self.role,\n",
    "            f\"I executed the given plan and got the follow results:\\n\\n{action_result}\",\n",
    "        )\n",
    "        return {\"rcode\": rcode, \"action_result\": action_result, \"code\": completion, \"error_logs\": error_logs}\n",
    "\n",
    "\n",
    "    def self_validation(self, filename):\n",
    "        rcode, log = execute_script(filename, device=self.device)\n",
    "        return rcode, log\n",
    "\n",
    "class Reflexion:\n",
    "\n",
    "    def __init__(self, specs, code, rules):\n",
    "\n",
    "        self.specs = specs \n",
    "        self.code = code \n",
    "        self.rules = rules\n",
    "\n",
    "    def self_reflexion(self):\n",
    "        \"\"\" \n",
    "        This is for self-reflexion to self check code to make sure logic + syntax is right before executive\n",
    "        \"\"\"\n",
    "\n",
    "        agent_profile = \"\"\" \n",
    "                    You are a Senior Python Code Reviewer and Quality Assurance Engineer\n",
    "                    \n",
    "                    Objective: \n",
    "\n",
    "                    Audit the generated code to ensure it is runnable, efficient, and error-free. You must strictly enforce the following rules:\n",
    "                    \n",
    "                    1. Logical Correctness: Verify the code logic is sound. Ensure variables are defined before use and data flows correctly through the pipeline.\n",
    "                    2. Strict Import Verification (CRITICAL):\n",
    "                        - No \"Ghost\" Imports: Scan the code for every external class or function used.\n",
    "                        - Mapping Check: You must verify that a corresponding import or from ... import ... statement exists at the top of the file for every single tool used.\n",
    "                    \n",
    "                    3. Structural Efficiency: Ensure all classes and functions have a clear purpose. Remove any redundant code or \"pass\" blocks that do not contribute to the solution.\n",
    "                    4. Requirement Compliance: The code must produce the exact output format requested by the user (e.g., specific CSV filenames or print statements).\n",
    "                    5. Using complete: Ensure all library from import is using right syntax in the solution.  \n",
    "                    \"\"\"\n",
    "\n",
    "        prompt = \"\"\" \n",
    "                    Carefully read the requirement for task: \n",
    "                    {} \n",
    "\n",
    "                    Carefully read the description of data for task\n",
    "                    {} \n",
    "\n",
    "                    Carefully read the rule for task engineering data\n",
    "                    {}\n",
    "\n",
    "                    And review code result below: \n",
    "                    {}\n",
    "\n",
    "                    If there is any error from the previous attempt, please carefully fix it first. Delete all columns uesless!\n",
    "\n",
    "                    Response return will have two components (in two list):\n",
    "                    - Error you check and solution\n",
    "                    - Full code complete \n",
    "                \"\"\"    \n",
    "\n",
    "        prompt = prompt.format(\n",
    "                    self.specs['requirements'],\n",
    "                    self.specs['data_description'],\n",
    "                    self.rules,\n",
    "                    self.code\n",
    "        )\n",
    "\n",
    "        response = call_llm(agent_profile, prompt) \n",
    "        return response \n",
    "\n",
    "class DataAgent(BaseAgent):\n",
    "    def __init__(self, specs):\n",
    "        system = (\n",
    "            \"TASK: Write a robust Python script to Find, Load, and Preprocess data.\\n\"\n",
    "            \"STRICT RULES:\\n\"\n",
    "            \"1. USE `os.walk` to find the folder matching the Contest Name.\\n\"\n",
    "            \"2. CHECK file count using `glob`. IF 1 file -> Split 80/20. IF 2 files -> Load separately.\\n\"\n",
    "            \"3. PREPROCESS: Handle missing values and categorical encoding and delete cols useless.\\n\"\n",
    "            \"4. OUTPUT: Save 'processed_train.csv' and 'processed_test.csv'.\\n\"\n",
    "            \"5. PRINT 'DATA_READY: processed_train.csv processed_test.csv' at the very end.\\n\"\n",
    "            \"OUTPUT FORMAT: Single valid ```python block.\"\n",
    "        )\n",
    "\n",
    "        agent_profile = \"\"\"You are the world's best data scientist of an automated machine learning project (AutoML) that can find the most relevant datasets,run useful preprocessing, perform suitable data augmentation, and make meaningful visulaization to comprehensively understand the data based on the user requirements. You have the following main responsibilities to complete.\n",
    "                        1. Retrieve a dataset from the user or search for the dataset based on the user instruction.\n",
    "                        2. Perform data preprocessing based on the user instruction or best practice based on the given tasks.\n",
    "                        3. Perform data augmentation as neccesary.\n",
    "                        4. Extract useful information and underlying characteristics of the dataset.\"\"\"\n",
    "\n",
    "        super().__init__(\"Data_Engineering\", system, specs, agent_profile)\n",
    "\n",
    "\n",
    "# --- 3. THE MANAGER (WORKFLOW ORCHESTRATOR) ---\n",
    "\n",
    "class CopilotManager:\n",
    "    def __init__(self, specs):\n",
    "        self.specs = specs\n",
    "        self.data_agent = DataAgent(specs)\n",
    "        # self.model_agent = ModelAgent(specs)\n",
    "\n",
    "    def run_agent_cycle(self, agent, max_retries=3):\n",
    "        \"\"\"\n",
    "        The Core Loop: Gen -> Exec -> Fix -> Retry\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸ”¹ STARTED AGENT: {agent.role}\")\n",
    "\n",
    "        print(f\"\\n Generating plan and code for: {agent.role} \")\n",
    "\n",
    "        code = agent.make_plan(self.specs[\"data_description\"], self.specs[\"requirements\"], self.data_agent.system)\n",
    "        \n",
    "        print(\"__\"* 69 )\n",
    "        print(f\"\\n Self-reflexion code for: {agent.role} \")\n",
    "\n",
    "        # re_code = Reflexion(self.specs, code)\n",
    "        # print(re_code)\n",
    "\n",
    "        # print(f\"\\n Generating code and self-reflexion for: {agent.role} \")\n",
    "        # # results = agent.implement_solution(plan)\n",
    "        # results = agent.run_code(code)\n",
    "            \n",
    "        return code\n",
    "\n",
    "    def start_workflow(self):\n",
    "        print(f\"ðŸš€ STARTING WORKFLOW: {self.specs['name']}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # ==========================================\n",
    "        # 1. DATA AGENT PHASE\n",
    "        # ==========================================\n",
    "        output = self.run_agent_cycle(self.data_agent)\n",
    "\n",
    "        return output\n",
    "        \n",
    "# --- 4. EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Requirement Spec\n",
    "    my_challenge = {\n",
    "        \"name\": \"Titanic\",\n",
    "        \"requirements\": \"Predict survival (0/1)\",\n",
    "        \n",
    "        \"data_description\": \"\"\"\n",
    "        The dataset contains the following columns:\n",
    "        - PassengerId: Unique ID (Drop this for training)\n",
    "        - Survived: Target variable (0 = No, 1 = Yes)\n",
    "        - Pclass: Ticket class (1, 2, 3). Treat as Ordinal.\n",
    "        - Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')\n",
    "        - Sex: 'male'/'female'. (Map to 0/1)\n",
    "        - Age: Numeric. Contains missing values (Fill with Median by Title).\n",
    "        - Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')\n",
    "        - Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode).\n",
    "        \"\"\",\n",
    "        \n",
    "        \"target_metric\": \"Accuracy\",\n",
    "        \"output_expectations\": \"submission.csv with PassengerId, Survived\"\n",
    "    }\n",
    "\n",
    "    # Run\n",
    "    # manager = CopilotManager(my_challenge)\n",
    "    # manager.start_workflow()\n",
    "\n",
    "\n",
    "my_challenge = {\n",
    "        \"name\": \"Titanic\",\n",
    "        \"requirements\": \"Predict survival (0/1)\",\n",
    "        \n",
    "        \"data_description\": \"\"\"\n",
    "        The dataset contains the following columns:\n",
    "        - PassengerId: Unique ID (Drop this for training)\n",
    "        - Survived: Target variable (0 = No, 1 = Yes)\n",
    "        - Pclass: Ticket class (1, 2, 3). Treat as Ordinal.\n",
    "        - Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')\n",
    "        - Sex: 'male'/'female'. (Map to 0/1)\n",
    "        - Age: Numeric. Contains missing values (Fill with Median by Title).\n",
    "        - Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')\n",
    "        - Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode).\n",
    "        \"\"\",\n",
    "        \n",
    "        \"target_metric\": \"Accuracy\",\n",
    "        \"output_expectations\": \"submission.csv with PassengerId, Survived\"\n",
    "    }\n",
    "\n",
    "specs = my_challenge\n",
    "data_agent = DataAgent(specs)\n",
    "\n",
    "print(f\"ðŸš€ STARTING WORKFLOW: {specs['name']}\")\n",
    "\n",
    "\"\"\"\n",
    "The Core Loop: Gen -> Exec -> Fix -> Retry\n",
    "\"\"\"\n",
    "print(f\"\\nðŸ”¹ STARTED AGENT: {data_agent.role}\")\n",
    "\n",
    "print(f\"\\n Generating plan and code for: {data_agent.role} \")\n",
    "\n",
    "print(\n",
    "    data_agent.role,\n",
    "    \"I am implementing the following instruction\"\n",
    ")\n",
    "\n",
    "log = \"Nothing. This is your first attempt.\"\n",
    "error_logs = []\n",
    "code = \"\"  # if a template/skeleton code is provided\n",
    "iteration = 0\n",
    "action_result = \"\"\n",
    "rcode = -1\n",
    "code_path = \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/foundation-ml/bucket-code\"\n",
    "\n",
    "n_attempts = 5\n",
    "\n",
    "while iteration < n_attempts:\n",
    "    print(\"Attempt: \", iteration)\n",
    "    try:\n",
    "\n",
    "        raw_completion = data_agent.make_plan(specs[\"data_description\"], \n",
    "                                        specs[\"requirements\"], \n",
    "                                        data_agent.system, \n",
    "                                        code,\n",
    "                                        log)\n",
    "\n",
    "\n",
    "        if \"```python\" in raw_completion:\n",
    "            completion = raw_completion.split(\"```python\")[1].split(\"```\")[0]\n",
    "        elif \"```Python\" in raw_completion:\n",
    "            completion = raw_completion.split(\"```Python\")[1].split(\"```\")[0]\n",
    "        else:\n",
    "            completion = raw_completion.split(\"```\")[1].split(\"```\")[0]\n",
    "\n",
    "        if not completion.strip(\" \\n\"):\n",
    "            continue\n",
    "\n",
    "        review = Reflexion(specs, completion, data_agent.system)\n",
    "        re_code_raw = review.self_reflexion()\n",
    "\n",
    "        if \"```python\" in re_code_raw:\n",
    "            re_code = re_code_raw.split(\"```python\")[1].split(\"```\")[0]\n",
    "        elif \"```Python\" in raw_completion:\n",
    "            re_code = re_code_raw.split(\"```Python\")[1].split(\"```\")[0]\n",
    "        else:\n",
    "            re_code = re_code_raw.split(\"```\")[1].split(\"```\")[0]\n",
    "            \n",
    "        re_code = re_code.strip(\"\\n\")\n",
    "\n",
    "        completion = re_code\n",
    "\n",
    "        code_name = data_agent.role \n",
    "        filename = f\"{code_path}/{code_name}.py\"\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        with open(filename, \"wt\") as file:\n",
    "            file.write(completion)\n",
    "        code = completion\n",
    "        rcode, log = data_agent.self_validation(filename)\n",
    "        if rcode == 0:\n",
    "            action_result = log\n",
    "            break\n",
    "        else:\n",
    "            log = log\n",
    "            error_logs.append(log)\n",
    "            action_result = log\n",
    "            print(data_agent.role, f\"I got this error (itr #{iteration}): {log}\")\n",
    "            iteration += 1                    \n",
    "            # break\n",
    "    except Exception as e:\n",
    "        iteration += 1\n",
    "        print(data_agent.role, f\"===== Retry: {iteration} =====\")\n",
    "        print(data_agent.role, f\"Executioin error occurs: {e}\")\n",
    "    continue\n",
    "\n",
    "print(\n",
    "    data_agent.role,\n",
    "    f\"I executed the given plan and got the follow results:\\n\\n{action_result}\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "99e37b4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m contest_folder \u001b[38;5;241m=\u001b[39m [folder \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(root_path) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitanic\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m folder\u001b[38;5;241m.\u001b[39mlower()][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m train_file \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontest_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*_train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     17\u001b[0m test_file \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_path, contest_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*_test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Check file count using glob. If 1 file -> Split 80/20. If 2 files -> Load separately.\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Set root path to the dataset\n",
    "root_path = \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets\"\n",
    "\n",
    "# Find the folder matching the Contest Name (Titanic)\n",
    "contest_folder = [folder for folder in os.listdir(root_path) if \"titanic\" in folder.lower()][0]\n",
    "\n",
    "# Load the dataset\n",
    "train_file = glob.glob(os.path.join(root_path, contest_folder, \"*_train.csv\"))[0]\n",
    "test_file = glob.glob(os.path.join(root_path, contest_folder, \"*_test.csv\"))[0]\n",
    "\n",
    "# Check file count using glob. If 1 file -> Split 80/20. If 2 files -> Load separately.\n",
    "if len(train_file) == 1:\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    test_data = pd.read_csv(test_file)\n",
    "else:\n",
    "    train_data = pd.concat([pd.read_csv(f) for f in glob.glob(os.path.join(root_path, contest_folder, \"*_train.csv\"))], ignore_index=True)\n",
    "    test_data = pd.concat([pd.read_csv(f) for f in glob.glob(os.path.join(root_path, contest_folder, \"*_test.csv\"))], ignore_index=True)\n",
    "\n",
    "# Preprocess data\n",
    "categorical_cols = [\"Pclass\", \"Sex\", \"Embarked\"]\n",
    "numerical_cols = [\"Age\"]\n",
    "\n",
    "# Handle missing values and categorical encoding\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", imputer, numerical_cols),\n",
    "        (\"cat\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown=\"missing\"), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_data_preprocessed = pd.DataFrame(preprocessor.fit_transform(train_data.drop(\"PassengerId\", axis=1)), columns=train_data.drop(\"PassengerId\", axis=1).columns)\n",
    "test_data_preprocessed = pd.DataFrame(preprocessor.transform(test_data.drop(\"PassengerId\", axis=1)), columns=test_data.drop(\"PassengerId\", axis=1).columns)\n",
    "\n",
    "# Extract Title from Name\n",
    "train_data_preprocessed[\"Title\"] = train_data_preprocessed[\"Name\"].apply(lambda x: x.split(\",\")[1].split(\".\")[0].strip().title())\n",
    "test_data_preprocessed[\"Title\"] = test_data_preprocessed[\"Name\"].apply(lambda x: x.split(\",\")[1].split(\".\")[0].strip().title())\n",
    "\n",
    "# Extract first letter of Cabin as 'Deck'\n",
    "train_data_preprocessed[\"Cabin\"] = train_data_preprocessed[\"Cabin\"].str[0]\n",
    "test_data_preprocessed[\"Cabin\"] = test_data_preprocessed[\"Cabin\"].str[0]\n",
    "\n",
    "# Fill missing values in Age with Median by Title\n",
    "title_age_median = imputer.fit_transform(train_data_preprocessed[[\"Age\", \"Title\"]].groupby(\"Title\")[\"Age\"].apply(lambda x: x.fillna(x.median())))\n",
    "train_data_preprocessed.loc[(train_data_preprocessed[\"Age\"].isna()) & (train_data_preprocessed[\"Title\"].isin(title_age_median[:, 0])), \"Age\"] = title_age_median[:, 1]\n",
    "test_data_preprocessed.loc[test_data_preprocessed[\"Age\"].isna(), \"Age\"] = title_age_median[:, 1]\n",
    "\n",
    "# One-Hot Encode Embarked\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "embarked_encoded = ohe.fit_transform(train_data_preprocessed[[\"Embarked\"]])\n",
    "train_data_preprocessed = pd.DataFrame(embarked_encoded.toarray(), columns=ohe.get_feature_names_out([\"Embarked\"]))\n",
    "test_data_preprocessed = pd.DataFrame(ohe.transform(test_data_preprocessed[[\"Embarked\"]]).toarray(), columns=ohe.get_feature_names_out([\"Embarked\"]))\n",
    "\n",
    "# Save preprocessed data\n",
    "train_data_preprocessed.to_csv(\"processed_train.csv\", index=False)\n",
    "test_data_preprocessed.to_csv(\"processed_test.csv\", index=False)\n",
    "\n",
    "print(\"DATA_READY: processed_train.csv processed_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4ebdabc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_READY: processed_train.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the root path for the dataset\n",
    "root_path = \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets\"\n",
    "\n",
    "# Find the folder matching the Contest Name (Titanic)\n",
    "contest_folder = [folder for folder in os.listdir(root_path) if \"titanic\" in folder][0]\n",
    "\n",
    "# Load the dataset\n",
    "train_files = glob.glob(os.path.join(root_path, contest_folder, \"*.csv\"))\n",
    "test_file = None\n",
    "\n",
    "if len(train_files) > 1:\n",
    "    train_file = train_files[0]\n",
    "    test_file = train_files[1] if len(train_files) > 1 else None\n",
    "else:\n",
    "    train_file = train_files[0]\n",
    "\n",
    "train_df = pd.read_csv(train_file)\n",
    "test_df = None\n",
    "\n",
    "if test_file:\n",
    "    test_df = pd.read_csv(test_file)\n",
    "\n",
    "# Define preprocessing steps\n",
    "categorical_cols = [\"Pclass\", \"Sex\", \"Embarked\"]\n",
    "numerical_cols = [\"Age\"]\n",
    "\n",
    "# Preprocess categorical columns\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"ordinal_encoder\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
    "])\n",
    "\n",
    "# Preprocess numerical columns\n",
    "numerical_transformer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"categorical\", categorical_transformer, categorical_cols),\n",
    "        (\"numerical\", numerical_transformer, numerical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply preprocessing to the data\n",
    "train_df_preprocessed = preprocessor.fit_transform(train_df.drop(\"Survived\", axis=1))\n",
    "test_df_preprocessed = None\n",
    "\n",
    "if test_df:\n",
    "    test_df_preprocessed = preprocessor.transform(test_df.drop(\"Survived\", axis=1))\n",
    "\n",
    "# Convert back to DataFrame\n",
    "train_df_preprocessed = pd.DataFrame(train_df_preprocessed, columns=categorical_cols + numerical_cols)\n",
    "test_df_preprocessed = None\n",
    "\n",
    "if test_df_preprocessed:\n",
    "    test_df_preprocessed = pd.DataFrame(test_df_preprocessed, columns=categorical_cols + numerical_cols)\n",
    "\n",
    "# Save the processed data\n",
    "train_df_preprocessed.to_csv(\"processed_train.csv\", index=False)\n",
    "test_df_preprocessed.to_csv(\"processed_test.csv\", index=False) if test_df_preprocessed else None\n",
    "\n",
    "print(\"DATA_READY: processed_train.csv\" + (\" processed_test.csv\" if test_df_preprocessed else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ddf9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hanoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
