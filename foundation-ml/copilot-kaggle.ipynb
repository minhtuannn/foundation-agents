{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb345c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéì ACADEMIMI COPILOT INITIALIZED: Titanic Survival\n",
      "\n",
      "--- PHASE 1: DATA PREPARATION ---\n",
      "   (Copilot is thinking...)\n",
      "============================================================\n",
      "ü§ñ Data Agent suggests the following:\n",
      "üìÇ Saved to: ./contest_workspace/01_data_prep.py\n",
      "------------------------------------------------------------\n",
      "import os\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "# Set root search path\n",
      "root_search_path = '/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets'\n",
      "\n",
      "# Find subfolder matching contest name\n",
      "contest_folder = [f for f in os.listdir(root_search_path) if f.startswith('Titanic Survival')][0]\n",
      "contest_folder_path = os.path.join(root_search_path, contest_folder)\n",
      "... [ 31 lines hidden ] ...\n",
      "# Output prepared data\n",
      "df.to_csv('processed_train.csv', index=False)\n",
      "test_df.to_csv('processed_test.csv', index=False)\n",
      "\n",
      "print(\"DATA_PROCESSED: processed_train.csv processed_test.csv\")\n",
      "============================================================\n",
      "\n",
      "üëâ ACTION: Run 'python ./contest_workspace/01_data_prep.py' in your terminal.\n",
      "\n",
      "Did the code run successfully? (y/n/error)\n",
      "‚úÖ Data Phase Complete.\n",
      "\n",
      "--- PHASE 2: MODELING ---\n",
      "   (Copilot is thinking...)\n",
      "============================================================\n",
      "ü§ñ Model Agent suggests the following:\n",
      "üìÇ Saved to: ./contest_workspace/02_train_model.py\n",
      "------------------------------------------------------------\n",
      "import pandas as pd\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split, GridSearchCV\n",
      "from sklearn.metrics import accuracy_score, classification_report\n",
      "\n",
      "# Load processed data\n",
      "train_df = pd.read_csv('train.csv')\n",
      "test_df = pd.read_csv('test.csv')\n",
      "\n",
      "# Split data into training and validation sets\n",
      "... [ 17 lines hidden ] ...\n",
      "test_pred = grid_search.best_estimator_.predict(test_df.drop(['PassengerId'], axis=1))\n",
      "\n",
      "# Save the submission file\n",
      "submission_df = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived': test_pred})\n",
      "submission_df.to_csv('submission.csv', index=False)\n",
      "============================================================\n",
      "\n",
      "üëâ ACTION: Run 'python ./contest_workspace/02_train_model.py' in your terminal.\n",
      "\n",
      "Did the code produce a good score? (y/n/error)\n",
      "üéâ Workflow Complete.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL_NAME = \"llama3\"\n",
    "WORKSPACE_DIR = \"./contest_workspace\"\n",
    "\n",
    "os.makedirs(WORKSPACE_DIR, exist_ok=True)\n",
    "\n",
    "# --- LLM CLIENT ---\n",
    "def call_llm(system_prompt, user_prompt):\n",
    "    full_prompt = f\"System: {system_prompt}\\nUser: {user_prompt}\"\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME, \"prompt\": full_prompt, \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0.2, \"num_ctx\": 8192}\n",
    "    }\n",
    "    try:\n",
    "        print(\"   (Copilot is thinking...)\", end=\"\\r\")\n",
    "        response = requests.post(OLLAMA_URL, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()['response']\n",
    "    except Exception as e:\n",
    "        return f\"LLM_ERROR: {str(e)}\"\n",
    "\n",
    "def save_code(code_text, filename):\n",
    "    clean_code = code_text\n",
    "    if \"```python\" in code_text:\n",
    "        clean_code = code_text.split(\"```python\")[1].split(\"```\")[0]\n",
    "    elif \"```\" in code_text:\n",
    "        clean_code = code_text.split(\"```\")[1].split(\"```\")[0]\n",
    "    clean_code = clean_code.strip()\n",
    "    \n",
    "    filepath = os.path.join(WORKSPACE_DIR, filename)\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(clean_code)\n",
    "    return filepath, clean_code\n",
    "\n",
    "# --- UPDATED AGENTS ---\n",
    "\n",
    "class DataCopilot:\n",
    "    def __init__(self, specs):\n",
    "        self.role = \"Data Engineer\"\n",
    "        # UPDATED: Now strictly prompted with Data Description\n",
    "        self.system = (\n",
    "            f\"You are an Autonomous Data Engineer. Contest: '{specs['name']}'.\\n\"\n",
    "            f\"ROOT SEARCH PATH: '{os.path.abspath(ALL_DATA_PATH)}'.\\n\"\n",
    "            f\"DATA DESCRIPTION: {specs['data_description']}.\\n\\n\"\n",
    "            \n",
    "            \"TASK: Write a Python script to Find, Inspect, and Prepare data.\\n\"\n",
    "            \"LOGIC FLOW:\\n\"\n",
    "            \"1. SEARCH: Find the subfolder inside ROOT SEARCH PATH that matches the Contest Name.\\n\"\n",
    "            \"2. INSPECT: List all files in that folder.\\n\"\n",
    "            \"3. DECIDE STRATEGY:\\n\"\n",
    "            \"   - CASE A: If you find separate 'train' and 'test' files -> Load them directly.\\n\"\n",
    "            \"   - CASE B: If you find 'train' and 'test' and 'val' -> Merge 'val' into 'train' or ignore it, then Load train/test.\\n\"\n",
    "            \"   - CASE C: check in folder, if only one file csv => load this as data files.\\n\"\n",
    "            \"4. PREPARE: Handle missing values and Encode categoricals (OneHot/Label).\\n\"\n",
    "            \"5. DETECT: Check label and data, label can based on data_description, rename this col into 'target' \"\n",
    "            \"6. OUTPUT: Save 'processed_train.csv' and 'processed_test.csv' to the current directory.\\n\"\n",
    "            \"7. PRINT: 'DATA_PROCESSED: processed_train.csv processed_test.csv' at the end.\\n\"\n",
    "            \"OUTPUT: ONLY valid Python code.\"\n",
    "        )\n",
    "\n",
    "    def suggest(self, feedback=\"\"):\n",
    "        prompt = \"Generate the data preparation script based on the Data Description.\"\n",
    "        if feedback:\n",
    "            prompt += f\"\\nCONTEXT: The user ran previous code and reported: {feedback}\"\n",
    "        return call_llm(self.system, prompt)\n",
    "\n",
    "class ModelCopilot:\n",
    "    def __init__(self, specs):\n",
    "        self.specs = specs \n",
    "        self.role = \"Modeling Copilot\"\n",
    "        self.system = (\n",
    "            f\"You are a ML Modeling Copilot. Target Metric: {specs['target_metric']}. \"\n",
    "            f\"Output Specs: {specs['output_expectations']}. \"\n",
    "            \"Task: Write Python code to load the PROCESSED data, train a model, and print validation metrics. \"\n",
    "            \"Output Format: \\n\"\n",
    "            \"1. A brief note on strategy.\\n\"\n",
    "            \"2. The valid Python code block.\"\n",
    "        )\n",
    "\n",
    "    def suggest(self, feedback=\"\"):\n",
    "        prompt = f\"Generate training code optimizing for {self.specs['target_metric']}.\"\n",
    "        if feedback:\n",
    "            prompt += f\"\\nCONTEXT: User reported previous result/error: {feedback}\"\n",
    "        return call_llm(self.system, prompt)\n",
    "\n",
    "\n",
    "# --- MANAGER ---\n",
    "\n",
    "class CopilotManager:\n",
    "    def __init__(self, specs):\n",
    "        self.specs = specs\n",
    "        self.data_agent = DataCopilot(specs)\n",
    "        self.model_agent = ModelCopilot(self.specs)\n",
    "\n",
    "    def present_solution(self, agent_name, raw_response, filename):\n",
    "        filepath, clean_code = save_code(raw_response, filename)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ü§ñ {agent_name} suggests the following:\")\n",
    "        print(f\"üìÇ Saved to: {filepath}\")\n",
    "        print(f\"{'-'*60}\")\n",
    "        \n",
    "        # Preview lines\n",
    "        lines = clean_code.splitlines()\n",
    "        if len(lines) > 20:\n",
    "            print(\"\\n\".join(lines[:10]))\n",
    "            print(f\"... [ {len(lines)-15} lines hidden ] ...\")\n",
    "            print(\"\\n\".join(lines[-5:]))\n",
    "        else:\n",
    "            print(clean_code)\n",
    "        \n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\\nüëâ ACTION: Run 'python {filepath}' in your terminal.\")\n",
    "        return filepath\n",
    "\n",
    "    def start_session(self):\n",
    "        print(f\"üéì ACADEMIMI COPILOT INITIALIZED: {self.specs['name']}\")\n",
    "        \n",
    "        # PHASE 1: DATA\n",
    "        print(\"\\n--- PHASE 1: DATA PREPARATION ---\")\n",
    "        feedback = \"\"\n",
    "        while True:\n",
    "            response = self.data_agent.suggest(feedback)\n",
    "            self.present_solution(\"Data Agent\", response, \"01_data_prep.py\")\n",
    "            \n",
    "            print(\"\\nDid the code run successfully? (y/n/error)\")\n",
    "            user_status = input(\">> \").strip()\n",
    "            \n",
    "            if user_status.lower() == 'y':\n",
    "                print(\"‚úÖ Data Phase Complete.\")\n",
    "                break\n",
    "            else:\n",
    "                feedback = f\"Error or User Feedback: {user_status}\"\n",
    "\n",
    "        # PHASE 2: MODELING\n",
    "        print(\"\\n--- PHASE 2: MODELING ---\")\n",
    "        feedback = \"\"\n",
    "        while True:\n",
    "            response = self.model_agent.suggest(feedback)\n",
    "            self.present_solution(\"Model Agent\", response, \"02_train_model.py\")\n",
    "            \n",
    "            print(\"\\nDid the code produce a good score? (y/n/error)\")\n",
    "            user_status = input(\">> \").strip()\n",
    "            \n",
    "            if user_status.lower() == 'y':\n",
    "                print(\"üéâ Workflow Complete.\")\n",
    "                break\n",
    "            else:\n",
    "                feedback = user_status\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- HERE IS WHERE YOU DEFINE THE DATA DESCRIPTION ---\n",
    "    my_challenge = {\n",
    "        \"name\": \"Titanic Survival\",\n",
    "        \n",
    "        \"requirements\": \"Predict survival (0/1). Train on train.csv, predict on test.csv.\",\n",
    "        \n",
    "        # NEW FIELD: The Agent will read this to decide how to code\n",
    "        \"data_description\": \"\"\"\n",
    "        The dataset contains the following columns:\n",
    "        - PassengerId: Unique ID (Drop this for training)\n",
    "        - Survived: Target variable (0 = No, 1 = Yes)\n",
    "        - Pclass: Ticket class (1, 2, 3). Treat as Ordinal.\n",
    "        - Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')\n",
    "        - Sex: 'male'/'female'. (Map to 0/1)\n",
    "        - Age: Numeric. Contains missing values (Fill with Median by Title).\n",
    "        - Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')\n",
    "        - Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode).\n",
    "        \"\"\",\n",
    "        \n",
    "        \"target_metric\": \"Accuracy\",\n",
    "        \"output_expectations\": \"submission.csv with PassengerId, Survived\"\n",
    "    }\n",
    "\n",
    "    manager = CopilotManager(my_challenge)\n",
    "    manager.start_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eec83f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING JOB: Titanic\n",
      "\n",
      "üîµ PHASE: Data Prep (Smart Search)\n",
      "   Attempt 1...\n",
      "‚ö° Executing ./contest_workspace/01_data_prep.py...\n",
      "   ‚ùå Failed.\n",
      "      Err: Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/contest_workspace/01_data_prep.py\", line 29, in <module>\n",
      "    train_df, test_df = df.sample(frac=0.8, random_state=42), df.drop(train_df.index)\n",
      "NameError: name 'train_df' is not defined\n",
      "   Attempt 2...\n",
      "‚ö° Executing ./contest_workspace/01_data_prep.py...\n",
      "   ‚ùå Failed.\n",
      "      Err: Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/contest_workspace/01_data_prep.py\", line 37, in <module>\n",
      "    train_df['Title'] = train_df.Name.str.extract('([A-Za-z]+)', expand=False)\n",
      "NameError: name 'train_df' is not defined\n",
      "   Attempt 3...\n",
      "‚ö° Executing ./contest_workspace/01_data_prep.py...\n",
      "   ‚ùå Failed.\n",
      "      Err: Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/contest_workspace/01_data_prep.py\", line 23, in <module>\n",
      "    train_df, test_df = train_test_split(pd.read_csv(os.path.join(root_search_path, contest_folder, file_name)), test_size=0.2)\n",
      "NameError: name 'train_test_split' is not defined\n",
      "‚ùå Data Phase Failed. Stopping.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import subprocess  # <--- FIXED: Added missing import\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL_NAME = \"llama3\"\n",
    "WORKSPACE_DIR = \"./contest_workspace\"\n",
    "PYTHON_EXEC = sys.executable # <--- FIXED: Define python executable\n",
    "\n",
    "# 1. ROOT DATA FOLDER (Agent searches inside here)\n",
    "ALL_DATA_PATH = \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets\" \n",
    "\n",
    "# Create directories\n",
    "os.makedirs(WORKSPACE_DIR, exist_ok=True)\n",
    "os.makedirs(ALL_DATA_PATH, exist_ok=True)\n",
    "\n",
    "# --- LLM CLIENT ---\n",
    "def call_llm(system_prompt, user_prompt):\n",
    "    full_prompt = f\"System: {system_prompt}\\nUser: {user_prompt}\"\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME, \"prompt\": full_prompt, \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0.2, \"num_ctx\": 8192}\n",
    "    }\n",
    "    try:\n",
    "        print(\"   (Copilot is thinking...)\", end=\"\\r\")\n",
    "        response = requests.post(OLLAMA_URL, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json().get('response', '')\n",
    "    except Exception as e:\n",
    "        return f\"LLM_ERROR: {str(e)}\"\n",
    "\n",
    "def save_code(code_text, filename):\n",
    "    clean_code = code_text\n",
    "    # Robust extraction of code blocks\n",
    "    if \"```python\" in code_text:\n",
    "        clean_code = code_text.split(\"```python\")[1].split(\"```\")[0]\n",
    "    elif \"```\" in code_text:\n",
    "        clean_code = code_text.split(\"```\")[1].split(\"```\")[0]\n",
    "    \n",
    "    clean_code = clean_code.strip()\n",
    "\n",
    "    # Remove accidental \"python\" string at start\n",
    "    lines = clean_code.splitlines()\n",
    "    if lines and lines[0].strip().lower() == \"python\":\n",
    "        clean_code = \"\\n\".join(lines[1:]).strip()\n",
    "\n",
    "    filepath = os.path.join(WORKSPACE_DIR, filename)\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(clean_code)\n",
    "    return filepath, clean_code\n",
    "\n",
    "\n",
    "def execute_script(filepath):\n",
    "    print(f\"‚ö° Executing {filepath}...\")\n",
    "    \n",
    "    # Run inside the workspace directory\n",
    "    filename = os.path.basename(filepath)\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [PYTHON_EXEC, filename], \n",
    "            cwd=WORKSPACE_DIR,       \n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=120 \n",
    "        )\n",
    "        return result\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"   ‚ùå Timeout Expired!\")\n",
    "        return None\n",
    "\n",
    "# --- UPDATED AGENTS ---\n",
    "\n",
    "class DataCopilot:\n",
    "    def __init__(self, specs):\n",
    "        self.role = \"Data Engineer\"\n",
    "        self.specs = specs\n",
    "        self.system = (\n",
    "            f\"You are an Autonomous Data Engineer. Contest: '{specs['name']}'.\\n\"\n",
    "            f\"ROOT SEARCH PATH: '{os.path.abspath(ALL_DATA_PATH)}'.\\n\"\n",
    "            f\"DATA DESCRIPTION: {specs['data_description']}.\\n\\n\"\n",
    "            \n",
    "            \"TASK: Write a Python script to Find, Inspect, and Prepare data.\\n\"\n",
    "            \"LOGIC FLOW:\\n\"\n",
    "            \"1. SEARCH: Find the subfolder inside ROOT SEARCH PATH that matches the Contest Name.\\n\"\n",
    "            \"2. INSPECT: List all files in that folder.\\n\"\n",
    "            \"3. DECIDE STRATEGY:\\n\"\n",
    "            \"   - CASE A: If you find separate 'train' and 'test' files -> Load them directly.\\n\"\n",
    "            \"   - CASE B: If you find 'train' and 'test' and 'val' -> Merge 'val' into 'train'.\\n\"\n",
    "            \"   - CASE C: If only one csv file exists -> Load it and split 80/20.\\n\"\n",
    "            \"4. PREPARE: Handle missing values and Encode categoricals (OneHot/Label).\\n\"\n",
    "            \"5. OUTPUT: Save 'processed_train.csv' and 'processed_test.csv' to the current directory.\\n\"\n",
    "            \"6. PRINT: 'DATA_PROCESSED: processed_train.csv processed_test.csv' at the very end.\\n\"\n",
    "            \"OUTPUT: ONLY valid Python code block.\"\n",
    "        )\n",
    "\n",
    "    def generate(self, feedback=\"\"):\n",
    "        # Base Prompt\n",
    "        prompt = f\"Generate the code for {self.role}.\"\n",
    "        \n",
    "        # Inject Feedback if it exists\n",
    "        if feedback:\n",
    "            prompt += (\n",
    "                \"\\n\\n‚ö†Ô∏è CRITICAL FIX REQUIRED ‚ö†Ô∏è\\n\"\n",
    "                f\"{feedback}\\n\"\n",
    "                \"Review the error above. REWRITE the code to fix this specific issue.\"\n",
    "            )\n",
    "            \n",
    "        return call_llm(self.system, prompt)\n",
    "\n",
    "class ModelCopilot:\n",
    "    def __init__(self, specs):\n",
    "        self.specs = specs \n",
    "        self.role = \"Modeling Copilot\"\n",
    "        self.train_file = \"processed_train.csv\"\n",
    "        self.test_file = \"processed_test.csv\"\n",
    "        self._update_system_prompt()\n",
    "\n",
    "    # FIXED: Added helper to update prompt dynamically\n",
    "    def _update_system_prompt(self):\n",
    "        self.system = (\n",
    "            f\"You are a ML Modeling Copilot. Target Metric: {self.specs['target_metric']}. \"\n",
    "            f\"Output Specs: {self.specs['output_expectations']}. \"\n",
    "            f\"TRAIN DATA: {self.train_file}, TEST DATA: {self.test_file}\\n\"\n",
    "            \"Task: Write Python code to load the PROCESSED data, train a model, and print validation metrics. \"\n",
    "            \"Output Format: \\n\"\n",
    "            \"1. Load data from the files specified above.\\n\"\n",
    "            \"2. Train model (RandomForest/XGBoost).\\n\"\n",
    "            \"3. Create submission file.\\n\"\n",
    "            \"4. PRINT 'FINAL_METRIC: <score>' at the end.\\n\"\n",
    "            \"OUTPUT: ONLY valid Python code.\"\n",
    "        )\n",
    "\n",
    "    # FIXED: Added method to receive file names from Manager\n",
    "    def set_data_files(self, train_f, test_f):\n",
    "        self.train_file = train_f\n",
    "        self.test_file = test_f\n",
    "        self._update_system_prompt()\n",
    "\n",
    "    def generate(self, feedback=\"\"):\n",
    "        # Base Prompt\n",
    "        prompt = f\"Generate the code for {self.role}.\"\n",
    "        \n",
    "        # Inject Feedback if it exists\n",
    "        if feedback:\n",
    "            prompt += (\n",
    "                \"\\n\\n‚ö†Ô∏è CRITICAL FIX REQUIRED ‚ö†Ô∏è\\n\"\n",
    "                f\"{feedback}\\n\"\n",
    "                \"Review the error above. REWRITE the code to fix this specific issue.\"\n",
    "            )\n",
    "            \n",
    "        return call_llm(self.system, prompt)\n",
    "\n",
    "# --- MANAGER ---\n",
    "\n",
    "class CopilotManager:\n",
    "    def __init__(self, specs):\n",
    "        self.specs = specs\n",
    "        self.data_agent = DataCopilot(specs)\n",
    "        self.model_agent = ModelCopilot(self.specs)\n",
    "\n",
    "    def run_phase(self, phase_name, agent, filename, max_retries=3):\n",
    "        print(f\"\\nüîµ PHASE: {phase_name}\")\n",
    "        feedback = \"\"  # Start with no errors\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            print(f\"   Attempt {attempt+1}...\")\n",
    "            \n",
    "            # 1. Generate Code (Feedback is empty on first try)\n",
    "            raw_response = agent.generate(feedback)\n",
    "            filepath, _ = save_code(raw_response, filename)\n",
    "            \n",
    "            # 2. Execute\n",
    "            result = execute_script(filepath)\n",
    "            \n",
    "            # 3. Validation Logic\n",
    "            if result and result.returncode == 0:\n",
    "                print(f\"   ‚úÖ Success.\")\n",
    "                return True, result.stdout\n",
    "            \n",
    "            # 4. ERROR HANDLING (Self-Correction)\n",
    "            else:\n",
    "                print(f\"   ‚ùå Failed.\")\n",
    "                \n",
    "                # Capture the specific error\n",
    "                if result:\n",
    "                    # Get the last 20 lines of the traceback (most useful part)\n",
    "                    error_log = result.stderr.strip()\n",
    "                    tail_error = \"\\n\".join(error_log.splitlines()[-20:])\n",
    "                    print(f\"      Err: {tail_error}\")\n",
    "                    \n",
    "                    # UPDATE FEEDBACK FOR NEXT LOOP\n",
    "                    feedback = (\n",
    "                        f\"Previous code crashed with this error:\\n\"\n",
    "                        f\"```text\\n{tail_error}\\n```\\n\"\n",
    "                        \"Fix the code to handle this error. Do not make the same mistake.\"\n",
    "                    )\n",
    "                else:\n",
    "                    feedback = \"The code timed out after 120 seconds. Optimize it or check for infinite loops.\"\n",
    "        \n",
    "        return False, \"Failed after max retries\"\n",
    "\n",
    "    def execute_script(filepath):\n",
    "        print(f\"‚ö° Executing {filepath}...\")\n",
    "        filename = os.path.basename(filepath)\n",
    "        \n",
    "        try:\n",
    "            # CAPTURE both stdout (normal output) and stderr (crash errors)\n",
    "            result = subprocess.run(\n",
    "                [PYTHON_EXEC, filename], \n",
    "                cwd=WORKSPACE_DIR,       \n",
    "                capture_output=True,     # <--- Critical: Captures the output\n",
    "                text=True,               # <--- Critical: Decodes to string\n",
    "                timeout=120 \n",
    "            )\n",
    "            return result\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"   ‚ùå Timeout Expired!\")\n",
    "            return None\n",
    "\n",
    "    def start(self):\n",
    "        print(f\"üöÄ STARTING JOB: {self.specs['name']}\")\n",
    "        \n",
    "        # Phase 1: Smart Data Search\n",
    "        success, output = self.run_phase(\"Data Prep (Smart Search)\", self.data_agent, \"01_data_prep.py\")\n",
    "        if not success: \n",
    "            print(\"‚ùå Data Phase Failed. Stopping.\")\n",
    "            return\n",
    "\n",
    "        # Parse Data Files from Output\n",
    "        for line in output.splitlines():\n",
    "            if \"DATA_PROCESSED:\" in line:\n",
    "                try:\n",
    "                    parts = line.split(\"DATA_PROCESSED:\")[1].strip().split()\n",
    "                    if len(parts) >= 2:\n",
    "                        self.model_agent.set_data_files(parts[0], parts[1])\n",
    "                        print(f\"   üìÇ Identified Data Files: {parts[0]}, {parts[1]}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è Parsing warning: {e}\")\n",
    "\n",
    "        # Phase 2: Training\n",
    "        success, output = self.run_phase(\"Model Training\", self.model_agent, \"02_train_model.py\")\n",
    "        if not success: return\n",
    "\n",
    "        # Result Parsing\n",
    "        for line in output.splitlines():\n",
    "            if \"FINAL_METRIC\" in line:\n",
    "                print(f\"\\nüèÜ {line.strip()}\")\n",
    "                break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # --- RUN AGENT ---\n",
    "    my_challenge = {\n",
    "        \"name\": \"Titanic\",\n",
    "        \"requirements\": \"Predict survival (0/1). Train on train.csv, predict on test.csv.\",\n",
    "        \n",
    "        \"data_description\": \"\"\"\n",
    "        The dataset contains the following columns:\n",
    "        - PassengerId: Unique ID (Drop this for training)\n",
    "        - Survived: Target variable (0 = No, 1 = Yes)\n",
    "        - Pclass: Ticket class (1, 2, 3). Treat as Ordinal.\n",
    "        - Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')\n",
    "        - Sex: 'male'/'female'. (Map to 0/1)\n",
    "        - Age: Numeric. Contains missing values (Fill with Median by Title).\n",
    "        - Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')\n",
    "        - Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode).\n",
    "        \"\"\",\n",
    "        \n",
    "        \"target_metric\": \"Accuracy\",\n",
    "        \"output_expectations\": \"submission.csv with PassengerId, Survived\"\n",
    "    }\n",
    "    \n",
    "    manager = CopilotManager(my_challenge)\n",
    "    manager.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b832722d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING WORKFLOW: Titanic\n",
      "\n",
      "üîπ STARTED AGENT: Data Engineering\n",
      "   üìù Generatng initial code...\n",
      "   ‚ö° Executing 01_data_pipeline.py...\n",
      "   ‚ùå Execution Failed.\n",
      "      [Bug Log]: Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/contest_workspace/01_data_pipeline.py\", line 33, in <module>\n",
      "    test_df['Title'] = test_df['Name'].apply(extract_title) if test_file else None\n",
      "TypeError: 'NoneType' object does not support item assignment\n",
      "   üîß Fixing bug (Attempt 1/3)...\n",
      "   ‚ö° Executing 01_data_pipeline.py...\n",
      "   ‚ùå Execution Failed.\n",
      "      [Bug Log]: Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/contest_workspace/01_data_pipeline.py\", line 32, in <module>\n",
      "    test_df['Sex'] = sex_encoder.transform(test_df['Sex']) if test_file else None\n",
      "TypeError: 'NoneType' object does not support item assignment\n",
      "   üîß Fixing bug (Attempt 2/3)...\n",
      "   ‚ö° Executing 01_data_pipeline.py...\n",
      "   ‚ùå Execution Failed.\n",
      "      [Bug Log]: Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/contest_workspace/01_data_pipeline.py\", line 54, in <module>\n",
      "    train_df = pd.concat([train_df, pd.DataFrame(embarked_encoded.toarray(), columns=embarked_encoder.get_feature_names(['Embarked']))], axis=1)\n",
      "AttributeError: 'OneHotEncoder' object has no attribute 'get_feature_names'. Did you mean: 'get_feature_names_out'?\n",
      "   üîß Fixing bug (Attempt 3/3)...\n",
      "   üß† (Agent is thinking...)\r"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL_NAME = \"llama3\"  # Ensure you have this model: `ollama pull llama3`\n",
    "WORKSPACE_DIR = \"./contest_workspace\"\n",
    "PYTHON_EXEC = sys.executable \n",
    "\n",
    "# ROOT DATA PATH (Adjust this to your actual path)\n",
    "ALL_DATA_PATH = \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets\" \n",
    "\n",
    "# Setup Directories\n",
    "os.makedirs(WORKSPACE_DIR, exist_ok=True)\n",
    "os.makedirs(ALL_DATA_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- 1. LLM COMMUNICATION LAYER ---\n",
    "def call_llm(system_prompt, user_prompt):\n",
    "    \"\"\"Sends request to Ollama with a strict JSON payload.\"\"\"\n",
    "    full_prompt = f\"System: {system_prompt}\\nUser: {user_prompt}\"\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME, \n",
    "        \"prompt\": full_prompt, \n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.2, # Low temp for precise coding\n",
    "            \"num_ctx\": 16384,    # Large context for code + error logs\n",
    "            \"stop\": [\"User:\", \"System:\", \"```python\\n\\n\"] # Stop tokens\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        print(\"   üß† (Agent is thinking...)\", end=\"\\r\")\n",
    "        response = requests.post(OLLAMA_URL, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json().get('response', '')\n",
    "    except Exception as e:\n",
    "        return f\"LLM_ERROR: {str(e)}\"\n",
    "\n",
    "def save_code_to_file(code_text, filename):\n",
    "    \"\"\"Extracts Python block and saves to file.\"\"\"\n",
    "    clean_code = code_text\n",
    "    # Extract code between markdown ticks\n",
    "    if \"```python\" in code_text:\n",
    "        clean_code = code_text.split(\"```python\")[1].split(\"```\")[0]\n",
    "    elif \"```\" in code_text:\n",
    "        clean_code = code_text.split(\"```\")[1].split(\"```\")[0]\n",
    "    \n",
    "    clean_code = clean_code.strip()\n",
    "    \n",
    "    # Remove accidental starting text\n",
    "    lines = clean_code.splitlines()\n",
    "    if lines and lines[0].lower().startswith(\"python\"):\n",
    "        clean_code = \"\\n\".join(lines[1:])\n",
    "\n",
    "    filepath = os.path.join(WORKSPACE_DIR, filename)\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(clean_code)\n",
    "    return filepath, clean_code\n",
    "\n",
    "def execute_code(filepath):\n",
    "    \"\"\"Executes the script and returns (Success_Bool, Output/Error).\"\"\"\n",
    "    print(f\"   ‚ö° Executing {os.path.basename(filepath)}...\")\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [PYTHON_EXEC, os.path.basename(filepath)], \n",
    "            cwd=WORKSPACE_DIR,       \n",
    "            capture_output=True,     \n",
    "            text=True,               \n",
    "            timeout=120 \n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            return True, result.stdout\n",
    "        else:\n",
    "            return False, result.stderr\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return False, \"TIMEOUT: Script took longer than 120 seconds.\"\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "\n",
    "# --- 2. AGENT DEFINITIONS ---\n",
    "\n",
    "class BaseAgent:\n",
    "    \"\"\"Parent class capable of Generating and Fixing code.\"\"\"\n",
    "    def __init__(self, role, system_instruction):\n",
    "        self.role = role\n",
    "        self.system = system_instruction\n",
    "\n",
    "    def generate_initial_code(self):\n",
    "        \"\"\"Step 1: Create code from scratch.\"\"\"\n",
    "        prompt = f\"Write the Python code for the {self.role} task based on your System Instructions.\"\n",
    "        return call_llm(self.system, prompt)\n",
    "\n",
    "    def fix_code(self, broken_code, error_log):\n",
    "        \"\"\"Step 2: Self-Reflection & Fix.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        ### SELF-REFLECTION & FIX TASK\n",
    "        Your previous code failed to execute.\n",
    "        \n",
    "        **1. THE BROKEN CODE:**\n",
    "        ```python\n",
    "        {broken_code}\n",
    "        ```\n",
    "        \n",
    "        **2. THE ERROR LOG:**\n",
    "        {error_log}\n",
    "        \n",
    "        **INSTRUCTION:**\n",
    "        - Analyze the error log carefully.\n",
    "        - Rewrite the code to fix the specific bug.\n",
    "        - Return the COMPLETE corrected Python script.\n",
    "        \"\"\"\n",
    "        return call_llm(self.system, prompt)\n",
    "\n",
    "\n",
    "class DataAgent(BaseAgent):\n",
    "    def __init__(self, specs):\n",
    "        system = (\n",
    "            f\"You are an Expert Data Engineer. Contest: '{specs['name']}'.\\n\"\n",
    "            f\"ROOT PATH: '{os.path.abspath(ALL_DATA_PATH)}'.\\n\"\n",
    "            f\"DATA SPECS: {specs['data_description']}.\\n\\n\"\n",
    "            \"TASK: Write a robust Python script to Find, Load, and Preprocess data.\\n\"\n",
    "            \"STRICT RULES:\\n\"\n",
    "            \"1. USE `os.walk` to find the folder matching the Contest Name.\\n\"\n",
    "            \"2. CHECK file count using `glob`. IF 1 file -> Split 80/20. IF 2 files -> Load separately.\\n\"\n",
    "            \"3. PREPROCESS: Handle missing values and categorical encoding.\\n\"\n",
    "            \"4. OUTPUT: Save 'processed_train.csv' and 'processed_test.csv'.\\n\"\n",
    "            \"5. PRINT 'DATA_READY: processed_train.csv processed_test.csv' at the very end.\\n\"\n",
    "            \"OUTPUT FORMAT: Single valid ```python block.\"\n",
    "        )\n",
    "        super().__init__(\"Data Engineering\", system)\n",
    "\n",
    "\n",
    "class ModelAgent(BaseAgent):\n",
    "    def __init__(self, specs):\n",
    "        self.specs = specs\n",
    "        # Placeholder system prompt, will be updated when data is ready\n",
    "        self.train_file = \"processed_train.csv\"\n",
    "        self.test_file = \"processed_test.csv\"\n",
    "        self._update_system()\n",
    "\n",
    "    def _update_system(self):\n",
    "        system = (\n",
    "            f\"You are a Kaggle Grandmaster. Goal: Maximize {self.specs['target_metric']}.\\n\"\n",
    "            f\"INPUT FILES: {self.train_file}, {self.test_file}\\n\"\n",
    "            \"TASK: Write Python code to Train a Model and Generate Submission.\\n\"\n",
    "            \"STEPS:\\n\"\n",
    "            \"1. Load the processed CSV files.\\n\"\n",
    "            \"2. Train a robust model (RandomForest/XGBoost/LightGBM).\\n\"\n",
    "            \"3. Evaluate on internal validation set.\\n\"\n",
    "            \"4. Generate 'submission.csv'.\\n\"\n",
    "            \"5. PRINT 'FINAL_SCORE: <score>' at the end.\\n\"\n",
    "            \"OUTPUT FORMAT: Single valid ```python block.\"\n",
    "        )\n",
    "        super().__init__(\"Model Training\", system)\n",
    "    \n",
    "    def set_data_files(self, train, test):\n",
    "        self.train_file = train\n",
    "        self.test_file = test\n",
    "        self._update_system()\n",
    "\n",
    "\n",
    "# --- 3. THE MANAGER (WORKFLOW ORCHESTRATOR) ---\n",
    "\n",
    "class CopilotManager:\n",
    "    def __init__(self, specs):\n",
    "        self.specs = specs\n",
    "        self.data_agent = DataAgent(specs)\n",
    "        self.model_agent = ModelAgent(specs)\n",
    "\n",
    "    def run_agent_cycle(self, agent, filename, max_retries=3):\n",
    "        \"\"\"\n",
    "        The Core Loop: Gen -> Exec -> Fix -> Retry\n",
    "        \"\"\"\n",
    "        print(f\"\\nüîπ STARTED AGENT: {agent.role}\")\n",
    "        \n",
    "        current_code = \"\"\n",
    "        last_error = \"\"\n",
    "\n",
    "        for attempt in range(max_retries + 1):\n",
    "            # --- STEP A: GENERATE / FIX ---\n",
    "            if attempt == 0:\n",
    "                print(\"   üìù Generatng initial code...\")\n",
    "                raw_response = agent.generate_initial_code()\n",
    "            else:\n",
    "                print(f\"   üîß Fixing bug (Attempt {attempt}/{max_retries})...\")\n",
    "                # Feedback Loop: Send broken code + error back to agent\n",
    "                raw_response = agent.fix_code(current_code, last_error)\n",
    "\n",
    "            # --- STEP B: SAVE & PREPARE ---\n",
    "            filepath, clean_code = save_code_to_file(raw_response, filename)\n",
    "            current_code = clean_code # Update memory\n",
    "\n",
    "            # --- STEP C: EXECUTE ---\n",
    "            success, output = execute_code(filepath)\n",
    "\n",
    "            # --- STEP D: EVALUATE ---\n",
    "            if success:\n",
    "                print(\"   ‚úÖ Execution Successful.\")\n",
    "                return True, output\n",
    "            else:\n",
    "                print(\"   ‚ùå Execution Failed.\")\n",
    "                # Capture error for the next 'Fix' loop\n",
    "                last_error = output.strip()\n",
    "                # Print short preview of error\n",
    "                error_preview = \"\\n\".join(last_error.splitlines()[-10:])\n",
    "                print(f\"      [Bug Log]: {error_preview}\")\n",
    "\n",
    "        print(\"   üíÄ Max retries reached. Moving on...\")\n",
    "        return False, last_error\n",
    "\n",
    "    def start_workflow(self):\n",
    "        print(f\"üöÄ STARTING WORKFLOW: {self.specs['name']}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # ==========================================\n",
    "        # 1. DATA AGENT PHASE\n",
    "        # ==========================================\n",
    "        success, output = self.run_agent_cycle(self.data_agent, \"01_data_pipeline.py\")\n",
    "        \n",
    "        if not success:\n",
    "            print(\"üõë Data Agent failed. Workflow stopped.\")\n",
    "            return\n",
    "\n",
    "        # Parse Output to find file names\n",
    "        train_file, test_file = None, None\n",
    "        for line in output.splitlines():\n",
    "            if \"DATA_READY:\" in line:\n",
    "                try:\n",
    "                    parts = line.split(\"DATA_READY:\")[1].strip().split()\n",
    "                    train_file, test_file = parts[0], parts[1]\n",
    "                    print(f\"   üìÇ Passed to Model Agent: {train_file}, {test_file}\")\n",
    "                except:\n",
    "                    print(\"   ‚ö†Ô∏è Could not parse file names, using defaults.\")\n",
    "\n",
    "        # ==========================================\n",
    "        # 2. MODEL AGENT PHASE\n",
    "        # ==========================================\n",
    "        if train_file and test_file:\n",
    "            self.model_agent.set_data_files(train_file, test_file)\n",
    "\n",
    "        success, output = self.run_agent_cycle(self.model_agent, \"02_model_pipeline.py\")\n",
    "\n",
    "        if success:\n",
    "            for line in output.splitlines():\n",
    "                if \"FINAL_SCORE:\" in line:\n",
    "                    print(f\"\\nüèÜ {line.strip()}\")\n",
    "        \n",
    "        print(f\"\\nüèÅ Workflow Finished in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "\n",
    "# --- 4. EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Requirement Spec\n",
    "    my_challenge = {\n",
    "        \"name\": \"Titanic\",\n",
    "        \"requirements\": \"Predict survival (0/1). Train on train.csv, predict on test.csv.\",\n",
    "        \n",
    "        \"data_description\": \"\"\"\n",
    "        The dataset contains the following columns:\n",
    "        - PassengerId: Unique ID (Drop this for training)\n",
    "        - Survived: Target variable (0 = No, 1 = Yes)\n",
    "        - Pclass: Ticket class (1, 2, 3). Treat as Ordinal.\n",
    "        - Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')\n",
    "        - Sex: 'male'/'female'. (Map to 0/1)\n",
    "        - Age: Numeric. Contains missing values (Fill with Median by Title).\n",
    "        - Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')\n",
    "        - Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode).\n",
    "        \"\"\",\n",
    "        \n",
    "        \"target_metric\": \"Accuracy\",\n",
    "        \"output_expectations\": \"submission.csv with PassengerId, Survived\"\n",
    "    }\n",
    "\n",
    "    # Run\n",
    "    manager = CopilotManager(my_challenge)\n",
    "    manager.start_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4054faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL_NAME = \"llama3\"\n",
    "WORKSPACE_DIR = \"./contest_workspace\"\n",
    "PYTHON_EXEC = sys.executable\n",
    "\n",
    "# 1. ROOT DATA FOLDER (Agent searches inside here)\n",
    "ALL_DATA_PATH = \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets\" \n",
    "\n",
    "os.makedirs(WORKSPACE_DIR, exist_ok=True)\n",
    "os.makedirs(ALL_DATA_PATH, exist_ok=True)\n",
    "\n",
    "# # --- UTILS ---\n",
    "# def call_llm(system_prompt, user_prompt):\n",
    "#     full_prompt = f\"System: {system_prompt}\\nUser: {user_prompt}\"\n",
    "#     payload = {\n",
    "#         \"model\": MODEL_NAME, \"prompt\": full_prompt, \"stream\": False,\n",
    "#         \"options\": {\"temperature\": 0.2, \"num_ctx\": 8192}\n",
    "#     }\n",
    "#     try:\n",
    "#         print(\"   (Agent is thinking...)\", end=\"\\r\")\n",
    "#         response = requests.post(OLLAMA_URL, json=payload)\n",
    "#         response.raise_for_status()\n",
    "#         return response.json()['response']\n",
    "#     except Exception as e:\n",
    "#         return f\"LLM_ERROR: {str(e)}\"\n",
    "\n",
    "# --- UTILS ---\n",
    "def call_llm(message):\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME, \"prompt\": message, \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0.2, \"num_ctx\": 8192}\n",
    "    }\n",
    "    try:\n",
    "        print(\"   (Agent is thinking...)\", end=\"\\r\")\n",
    "        response = requests.post(OLLAMA_URL, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()['response']\n",
    "    except Exception as e:\n",
    "        return f\"LLM_ERROR: {str(e)}\"\n",
    "\n",
    "def save_code(code_text, filename):\n",
    "    clean_code = code_text\n",
    "    if \"```python\" in code_text:\n",
    "        clean_code = code_text.split(\"```python\")[1].split(\"```\")[0]\n",
    "    elif \"```\" in code_text:\n",
    "        clean_code = code_text.split(\"```\")[1].split(\"```\")[0]\n",
    "    clean_code = clean_code.strip()\n",
    "\n",
    "    lines = clean_code.splitlines()\n",
    "    if lines and lines[0].strip().lower() == \"python\":\n",
    "        clean_code = \"\\n\".join(lines[1:]).strip()\n",
    "\n",
    "    filepath = os.path.join(WORKSPACE_DIR, filename)\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(clean_code)\n",
    "    return filepath, clean_code\n",
    "\n",
    "\n",
    "def execute_script(filepath):\n",
    "    print(f\"‚ö° Executing {filepath}...\")\n",
    "    \n",
    "    # FIX: Extract just the filename ('01_data_prep.py') \n",
    "    # because cwd is already set to WORKSPACE_DIR\n",
    "    filename = os.path.basename(filepath)\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [PYTHON_EXEC, filename], # <--- Changed from 'filepath' to 'filename'\n",
    "            cwd=WORKSPACE_DIR,       # Execution happens inside the workspace folder\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=120 \n",
    "        )\n",
    "        return result\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- AGENTS ---\n",
    "\n",
    "class DataAgent:\n",
    "    def __init__(self, specs):\n",
    "        self.money = {}\n",
    "        self.specs = specs\n",
    "\n",
    "        self.system = (\n",
    "            f\"You are an Autonomous Data Engineer. Contest: '{specs['name']}'.\\n\"\n",
    "            f\"ROOT SEARCH PATH: '{os.path.abspath(ALL_DATA_PATH)}'.\\n\"\n",
    "            f\"DATA DESCRIPTION: {specs['data_description']}.\\n\\n\"\n",
    "            \n",
    "            \"TASK: Write a Python script to Find, Inspect, and Prepare data.\\n\"\n",
    "            \"LOGIC FLOW:\\n\"\n",
    "            \"1. SEARCH: Find the subfolder inside ROOT SEARCH PATH that matches the Contest Name.\\n\"\n",
    "            \"2. INSPECT: List all files in that folder.\\n\"\n",
    "            \"3. DECIDE STRATEGY:\\n\"\n",
    "            \"   - CASE A: If you find separate 'train' and 'test' files -> Load them directly.\\n\"\n",
    "            \"   - CASE B: If you find 'train' and 'test' and 'val' -> Merge 'val' into 'train' or ignore it, then Load train/test.\\n\"\n",
    "            \"   - CASE C: check in folder, if only one file csv => load this as data files.\\n\"\n",
    "            \"4. PREPARE: Handle missing values and Encode categoricals (OneHot/Label).\\n\"\n",
    "            \"5. DETECT: Check label and data, label can based on data_description, rename this col into 'target' \"\n",
    "            \"6. OUTPUT: Save 'processed_train.csv' and 'processed_test.csv' to the current directory.\\n\"\n",
    "            \"7. PRINT: 'DATA_PROCESSED: processed_train.csv processed_test.csv' at the end.\\n\"\n",
    "            \"OUTPUT: ONLY valid Python code.\"\n",
    "        )\n",
    "\n",
    "        self.agent_profile = \"\"\"You are the world's best data scientist of an automated machine learning project (AutoML) that can find the most relevant datasets,run useful preprocessing, perform suitable data augmentation, and make meaningful visulaization to comprehensively understand the data based on the user requirements. You have the following main responsibilities to complete.\n",
    "        1. Retrieve a dataset from the user or search for the dataset based on the user instruction or find the subfolder inside ROOT SEARCH PATH that matches the Contest Name.\n",
    "        2. Perform data preprocessing based on the user instruction or best practice based on the given tasks.\n",
    "        3. Perform data augmentation as neccesary.\n",
    "        4. Extract useful information and underlying characteristics of the dataset.\"\"\"\n",
    "\n",
    "    def understand_plan(self, plan):\n",
    "        summary_prompt = f\"\"\"As a proficient data scientist, summarize the following plan given by the senior AutoML project manager according to the user's requirements and your expertise in data science.\n",
    "        \n",
    "        # User's Requirements\n",
    "            Contest: '{self.specs['name']}'.\\n\n",
    "            ROOT SEARCH PATH: '{os.path.abspath(ALL_DATA_PATH)}'.\\n\n",
    "            DATA DESCRIPTION: {self.specs['data_description']}.\\n\\n\n",
    "        \n",
    "        # Project Plan\n",
    "        {plan}\n",
    "        \n",
    "        The summary of the plan should enable you to fulfill your responsibilities as the answers to the following questions by focusing on the data manipulation and analysis.\n",
    "        1. How to retrieve or collect the dataset(s)?\n",
    "        2. How to preprocess the retrieved dataset(s)?\n",
    "        3. How to efficiently augment the dataset(s)?\n",
    "        4. How to extract and understand the underlying characteristics of the dataset(s)?\n",
    "        \n",
    "        Note that you should not perform data visualization because you cannot see it. Make sure that another data scientist can exectly reproduce the results based on your summary.\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.agent_profile},\n",
    "            {\"role\": \"user\", \"content\": summary_prompt},\n",
    "        ]\n",
    "\n",
    "        retry = 0\n",
    "        while retry < 10:\n",
    "            try:\n",
    "                res = call_llm(self.agent_profile, summary_prompt)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(\"system\", e)\n",
    "                retry += 1\n",
    "                continue\n",
    "\n",
    "    # def generate(self, feedback=\"\"):\n",
    "    #     prompt = \"Generate the data search, inspection, and preparation script.\"\n",
    "    #     if feedback:\n",
    "    #         prompt += f\"\\n\\n‚ö†Ô∏è PREVIOUS RUN FAILED. FIX ERROR:\\n{feedback}\\n\"\n",
    "    #         prompt += \"Review the code logic carefully. Ensure variables (like train_df) are defined before use in all branches. Output ONLY valid Python code.\"\n",
    "    #     return call_llm(self.system, prompt)\n",
    "\n",
    "    def execute_plan(self, specs, plan, data_path, pid):\n",
    "        print(self.agent_type, \"I am working with the given plan!\", pid)\n",
    "        data_plan = self.understand_plan(plan)\n",
    "\n",
    "        # Check whether the given source is accessible before running the execution --> reduce FileNotFound error\n",
    "        # modality-based extraction ?\n",
    "\n",
    "        exec_prompt = f\"\"\"As a proficient data scientist, your task is to explain **detailed** steps for data manipulation and analysis parts by executing the following machine learning development plan.\n",
    "        \n",
    "        # Plan\n",
    "        {data_plan}\n",
    "        \n",
    "        # Potential Source of Dataset\n",
    "        {specs['data_description']}\n",
    "        \n",
    "        Make sure that your explanation follows these instructions:\n",
    "        - All of your explanation must be self-contained without using any placeholder to ensure that other data scientists can exactly reproduce all the steps, but do not include any code.\n",
    "        - Include how and where to retrieve or collect the data.\n",
    "        - Include how to preprocess the data and which tools or libraries are used for the preprocessing.\n",
    "        - Include how to do the data augmentation with details and names.\n",
    "        - Include how to extract and understand the characteristics of the data.\n",
    "        - Include reasons why each step in your explanations is essential to effectively complete the plan.        \n",
    "        Note that you should not perform data visualization because you cannot see it. Make sure to focus only on the data part as it is your expertise. Do not conduct or perform anything regarding modeling or training.\n",
    "        After complete the explanations, explicitly specify the (expected) outcomes and results both quantitative and qualitative of your explanations.\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": agent_profile},\n",
    "            {\"role\": \"user\", \"content\": exec_prompt},\n",
    "        ]\n",
    "\n",
    "        retry = 0\n",
    "        while retry < 10:\n",
    "            try:\n",
    "                res = call_llm(self.agent_profile, exec_prompt)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(\"system\", e)\n",
    "                retry += 1\n",
    "                continue\n",
    "\n",
    "        # Data LLaMA summarizes the given plan for optimizing data relevant processes\n",
    "        action_result = res.choices[0].message.content.strip()\n",
    "        self.money[f\"Data_Plan_Execution_{pid}\"] = res.usage.to_dict(mode=\"json\")\n",
    "\n",
    "        print(self.agent_type, \"I have done with my execution!\", pid)\n",
    "        return action_result\n",
    "\n",
    "class ModelAgent:\n",
    "    def __init__(self, specs):\n",
    "        self.target_metric = specs['target_metric']\n",
    "        self.train_file = \"processed_train.csv\"\n",
    "        self.test_file = \"processed_test.csv\"\n",
    "        self.update_system_prompt()\n",
    "\n",
    "        # self.system = (\n",
    "        #     f\"You are an Autonomous ML Researcher. Metric: {self.target_metric}.\\n\"\n",
    "        #     \"TASK: Train a model on 'processed_train.csv'.\\n\"\n",
    "        #     \"REQUIREMENTS:\\n\"\n",
    "        #     \"1. Load 'processed_train.csv'.\\n\"\n",
    "        #     \"2. Train a model (RandomForest/XGBoost).\\n\"\n",
    "        #     \"3. Internal Validation: Split processed_train 80/20 to calculate metric.\\n\"\n",
    "        #     \"4. PRINT FORMAT: 'FINAL_METRIC: <number>'.\\n\"\n",
    "        #     \"5. Prediction: Load 'processed_test.csv', predict, and save 'submission.csv'.\\n\"\n",
    "        #     \"OUTPUT: ONLY valid Python code.\"\n",
    "        # )\n",
    "\n",
    "    def update_system_prompt(self):\n",
    "        self.system = (\n",
    "            f\"You are an Autonomous ML Researcher. Metric: {self.target_metric}.\\n\"\n",
    "            f\"TASK: Train a model on '{self.train_file}'.\\n\"\n",
    "            \"REQUIREMENTS:\\n\"\n",
    "            f\"1. Load '{self.train_file}'.\\n\"\n",
    "            \"2. Train a model (RandomForest/XGBoost).\\n\"\n",
    "            f\"3. Internal Validation: Split {self.train_file} 80/20 to calculate metric.\\n\"\n",
    "            \"4. PRINT FORMAT: 'FINAL_METRIC: <number>'.\\n\"\n",
    "            f\"5. Prediction: Load '{self.test_file}', predict, and save 'submission.csv'.\\n\"\n",
    "            \"OUTPUT: ONLY valid Python code.\"\n",
    "        )\n",
    "\n",
    "    # def generate(self, feedback=\"\"):\n",
    "    #     prompt = f\"Generate training script optimizing for {self.target_metric}.\"\n",
    "    #     if feedback:\n",
    "    #         prompt += f\"\\nFIX ERROR: {feedback}\"\n",
    "    #     return call_llm(self.system, prompt)\n",
    "\n",
    "    def set_data_files(self, train_file, test_file):\n",
    "        self.train_file = train_file\n",
    "        self.test_file = test_file\n",
    "        self.update_system_prompt()\n",
    "\n",
    "    def generate(self, feedback=\"\"):\n",
    "        prompt = f\"Generate training script optimizing for {self.target_metric}.\"\n",
    "        if feedback:\n",
    "            prompt += f\"\\n\\n‚ö†Ô∏è PREVIOUS RUN FAILED. FIX ERROR:\\n{feedback}\\n\"\n",
    "            prompt += \"Review the code logic carefully. Output ONLY valid Python code.\"\n",
    "        return call_llm(self.system, prompt)\n",
    "\n",
    "\n",
    "# --- MANAGER ---\n",
    "\n",
    "class AutonomousManager:\n",
    "    def __init__(self, specs):\n",
    "        self.specs = specs\n",
    "        self.data_agent = DataAgent(specs)\n",
    "        self.model_agent = ModelAgent(specs)\n",
    "\n",
    "    # def run_phase(self, phase_name, agent, filename, max_retries=3):\n",
    "    #     print(f\"\\nüîµ PHASE: {phase_name}\")\n",
    "    #     feedback = \"\"\n",
    "        \n",
    "    #     for attempt in range(max_retries):\n",
    "    #         print(f\"   Attempt {attempt+1}...\")\n",
    "    #         raw_response = agent.generate(feedback)\n",
    "    #         filepath, _ = save_code(raw_response, filename)\n",
    "            \n",
    "    #         result = execute_script(filepath)\n",
    "            \n",
    "    #         if result and result.returncode == 0:\n",
    "    #             print(f\"   ‚úÖ Success.\")\n",
    "    #             return True, result.stdout\n",
    "    #         else:\n",
    "    #             print(f\"   ‚ùå Failed.\")\n",
    "    #             err = result.stderr.strip() if result else \"Timeout\"\n",
    "    #             print(f\"      Err: {err[-200:]}\")\n",
    "    #             feedback = f\"Runtime Error: {err}\"\n",
    "        \n",
    "    #     return False, \"Failed\"\n",
    "\n",
    "    def start(self):\n",
    "        print(f\"üöÄ STARTING JOB: {self.specs['name']}\")\n",
    "        \n",
    "        # Phase 1: Smart Data Search\n",
    "        success, output = self.run_phase(\"Data Prep (Smart Search)\", self.data_agent, \"01_data_prep.py\")\n",
    "        if not success: return\n",
    "\n",
    "        # Parse Data Files\n",
    "        for line in output.split('\\n'):\n",
    "            if \"DATA_PROCESSED:\" in line:\n",
    "                parts = line.split(\"DATA_PROCESSED:\")[1].strip().split()\n",
    "                if len(parts) >= 2:\n",
    "                    self.model_agent.set_data_files(parts[0], parts[1])\n",
    "                    print(f\"   üìÇ Data Files: {parts[0]}, {parts[1]}\")\n",
    "\n",
    "\n",
    "        # Phase 2: Training\n",
    "        success, output = self.run_phase(\"Model Training\", self.model_agent, \"02_train_model.py\")\n",
    "        if not success: return\n",
    "\n",
    "        # Result Parsing\n",
    "        for line in output.split('\\n'):\n",
    "            if \"FINAL_METRIC\" in line:\n",
    "                print(f\"\\nüèÜ {line.strip()}\")\n",
    "                break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # --- RUN AGENT ---\n",
    "    my_challenge = {\n",
    "        \"name\": \"Titanic\",\n",
    "        \n",
    "        \"requirements\": \"Predict survival (0/1). Train on train.csv, predict on test.csv.\",\n",
    "        \n",
    "        # NEW FIELD: The Agent will read this to decide how to code\n",
    "        \"data_description\": \"\"\"\n",
    "        The dataset contains the following columns:\n",
    "        - PassengerId: Unique ID (Drop this for training)\n",
    "        - Survived: Target variable (0 = No, 1 = Yes)\n",
    "        - Pclass: Ticket class (1, 2, 3). Treat as Ordinal.\n",
    "        - Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')\n",
    "        - Sex: 'male'/'female'. (Map to 0/1)\n",
    "        - Age: Numeric. Contains missing values (Fill with Median by Title).\n",
    "        - Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')\n",
    "        - Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode).\n",
    "        \"\"\",\n",
    "        \n",
    "        \"target_metric\": \"Accuracy\",\n",
    "        \"output_expectations\": \"submission.csv with PassengerId, Survived\"\n",
    "    }\n",
    "    \n",
    "    manager = AutonomousManager(my_challenge)\n",
    "    manager.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d349ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "from num2words import num2words\n",
    "from multiprocessing import current_process\n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "\n",
    "\n",
    "from minions_agent.manager_agent import *\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL_NAME = \"llama3\"\n",
    "WORKSPACE_DIR = \"./contest_workspace\"\n",
    "PYTHON_EXEC = sys.executable\n",
    "\n",
    "\n",
    "# 1. ROOT DATA FOLDER (Agent searches inside here)\n",
    "ALL_DATA_PATH = \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets\" \n",
    "\n",
    "os.makedirs(WORKSPACE_DIR, exist_ok=True)\n",
    "os.makedirs(ALL_DATA_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_code(code_text, filename):\n",
    "    clean_code = code_text\n",
    "    if \"```python\" in code_text:\n",
    "        clean_code = code_text.split(\"```python\")[1].split(\"```\")[0]\n",
    "    elif \"```\" in code_text:\n",
    "        clean_code = code_text.split(\"```\")[1].split(\"```\")[0]\n",
    "    clean_code = clean_code.strip()\n",
    "\n",
    "    lines = clean_code.splitlines()\n",
    "    if lines and lines[0].strip().lower() == \"python\":\n",
    "        clean_code = \"\\n\".join(lines[1:]).strip()\n",
    "\n",
    "    filepath = os.path.join(WORKSPACE_DIR, filename)\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(clean_code)\n",
    "    return filepath, clean_code\n",
    "\n",
    "\n",
    "def execute_script(filepath):\n",
    "    print(f\"‚ö° Executing {filepath}...\")\n",
    "    \n",
    "    # FIX: Extract just the filename ('01_data_prep.py') \n",
    "    # because cwd is already set to WORKSPACE_DIR\n",
    "    filename = os.path.basename(filepath)\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [PYTHON_EXEC, filename], # <--- Changed from 'filepath' to 'filename'\n",
    "            cwd=WORKSPACE_DIR,       # Execution happens inside the workspace folder\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=120 \n",
    "        )\n",
    "        return result\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- RUN AGENT ---\n",
    "my_challenge = {\n",
    "    \"name\": \"Titanic\",\n",
    "    \n",
    "    \"requirements\": \"Predict survival (0/1)\",\n",
    "    \n",
    "    # NEW FIELD: The Agent will read this to decide how to code\n",
    "    \"data_description\": \"\"\"\n",
    "    The dataset contains the following columns:\n",
    "    - PassengerId: Unique ID (Drop this for training)\n",
    "    - Survived: Target variable (0 = No, 1 = Yes)\n",
    "    - Pclass: Ticket class (1, 2, 3). Treat as Ordinal.\n",
    "    - Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')\n",
    "    - Sex: 'male'/'female'. (Map to 0/1)\n",
    "    - Age: Numeric. Contains missing values (Fill with Median by Title).\n",
    "    - Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')\n",
    "    - Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode).\n",
    "    \"\"\",\n",
    "    \n",
    "    \"target_metric\": \"Accuracy\",\n",
    "    \"output_expectations\": \"submission.csv with PassengerId, Survived\"\n",
    "}\n",
    "\n",
    "manager = AutonomousManager(my_challenge, ALL_DATA_PATH)\n",
    "# manager.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa33813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY_PROMPT_GENCODE .)\n",
      "\n",
      "As the project manager, please carefully read and understand the following instructions suggested by data scientists and machine learning engineers. Then, select the best solution for the given user's requirements.\n",
      "        \n",
      "        - Instructions from Data Scientists\n",
      "        **AutoML Project Plan for Titanic Survival Prediction**\n",
      "\n",
      "**Project Overview**\n",
      "The objective of this project is to develop a machine learning model that predicts the survival rate of passengers on the Titanic based on the provided dataset.\n",
      "\n",
      "**Data Loading Instructions**\n",
      "\n",
      "1. **Folder Search Logic**: Use `os.walk` to search for folders in `/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets`. Look for folders containing the string \"Titanic\" and save it as `target_folder`.\n",
      "2. **File Loading Logic**:\n",
      "\t* List all .csv files in `target_folder`.\n",
      "\t* IF there is only one file, load it and split it 80/20 into `train_df` and `test_df`.\n",
      "\t* ELIF there are multiple files, identify the file with \"train\" in the name as `train_df` and the file with \"test\" in the name as `test_df`.\n",
      "3. **Output**: Save the processed dataframes as 'train.csv' and 'test.csv'.\n",
      "\n",
      "**Data Preprocessing**\n",
      "\n",
      "1. **Drop PassengerId**: Drop the PassengerId column, as it is not relevant for training.\n",
      "2. **Ordinal Encoding**: Treat Pclass as an ordinal feature.\n",
      "3. **Title Extraction**: Extract the title (Mr/Mrs) from the Name column to create a new feature 'Title'.\n",
      "4. **Sex Mapping**: Map Sex to 0/1 values.\n",
      "5. **Age Imputation**: Fill missing Age values with the median value by Title.\n",
      "6. **Cabin Deck Extraction**: Extract the first letter of Cabin as 'Deck' and fill missing values with 'Unknown'.\n",
      "7. **One-Hot Encoding**: One-hot encode Embarked.\n",
      "\n",
      "**Model Development**\n",
      "\n",
      "1. **Feature Selection**: Select relevant features based on correlation analysis and feature importance scores.\n",
      "2. **Hyperparameter Tuning**: Perform grid search or random search to tune hyperparameters for the chosen model.\n",
      "3. **Model Training**: Train a suitable machine learning model (e.g., logistic regression, decision tree, random forest) using the preprocessed data.\n",
      "\n",
      "**Evaluation**\n",
      "\n",
      "1. **Model Evaluation Metrics**: Use accuracy, precision, recall, F1-score, and AUC-ROC to evaluate the performance of the trained model.\n",
      "2. **Hyperparameter Tuning**: Perform hyperparameter tuning for the chosen model using the evaluation metrics as a guide.\n",
      "\n",
      "**Deployment**\n",
      "\n",
      "1. **Model Deployment**: Deploy the trained model in a suitable environment (e.g., Flask API, TensorFlow Serving).\n",
      "2. **Model Monitoring**: Monitor the performance of the deployed model and retrain it periodically to maintain its accuracy.\n",
      "\n",
      "**Timeline**\n",
      "\n",
      "* Data loading and preprocessing: 2 days\n",
      "* Model development and hyperparameter tuning: 3-4 days\n",
      "* Evaluation and deployment: 1-2 days\n",
      "\n",
      "Total project duration: approximately 7-9 days\n",
      "\n",
      "This plan is designed for AI agents to execute, providing a detailed end-to-end process from data retrieval to model training and evaluation.\n",
      "\n",
      "        If there is no predefined data split or the data scientists suggest the data split other than train 70%, validation 20%, and test 10%, please use 70%, 20%, and 10% instead for consistency across different tasks. This is the retrievable data path: /Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets.\n",
      "        You should exclude every suggestion related to data visualization as you will be unable to see it.\n",
      "        \n",
      "        # - Instructions from Machine Learning Engineers\n",
      "        # model_plan_for_execution                    \n",
      "        \n",
      "        - User's Requirements\n",
      "        Predict survival (0/1)\n",
      "        \n",
      "    The dataset contains the following columns:\n",
      "    - PassengerId: Unique ID (Drop this for training)\n",
      "    - Survived: Target variable (0 = No, 1 = Yes)\n",
      "    - Pclass: Ticket class (1, 2, 3). Treat as Ordinal.\n",
      "    - Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')\n",
      "    - Sex: 'male'/'female'. (Map to 0/1)\n",
      "    - Age: Numeric. Contains missing values (Fill with Median by Title).\n",
      "    - Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')\n",
      "    - Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode).\n",
      "    \n",
      "        \n",
      "        Note that you must select only ONE promising solution (i.e., one data processing pipeline and one model from the top-three models) based on the above suggestions.\n",
      "        After choosing the best solution, give detailed instructions and guidelines for MLOps engineers who will write the code based on your instructions. Do not write the code by yourself. Since PyTorch is preferred for implementing deep learning and neural networks models, please guide the MLOPs engineers accordingly.\n",
      "        Make sure your instructions are sufficient with all essential information (e.g., complete path for dataset source and model location) for any MLOps or ML engineers to enable them to write the codes using existing libraries and frameworks correctly.\n",
      "operation I am implementing the following instruction\n",
      "____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "PROMPT GEN CODE \n",
      "\n",
      "Carefully read the following instructions to write Python code for Predict survival (0/1) task.\n",
      "                As the project manager, please carefully read and understand the following instructions suggested by data scientists and machine learning engineers. Then, select the best solution for the given user's requirements.\n",
      "        \n",
      "        - Instructions from Data Scientists\n",
      "        **AutoML Project Plan for Titanic Survival Prediction**\n",
      "\n",
      "**Project Overview**\n",
      "The objective of this project is to develop a machine learning model that predicts the survival rate of passengers on the Titanic based on the provided dataset.\n",
      "\n",
      "**Data Loading Instructions**\n",
      "\n",
      "1. **Folder Search Logic**: Use `os.walk` to search for folders in `/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets`. Look for folders containing the string \"Titanic\" and save it as `target_folder`.\n",
      "2. **File Loading Logic**:\n",
      "\t* List all .csv files in `target_folder`.\n",
      "\t* IF there is only one file, load it and split it 80/20 into `train_df` and `test_df`.\n",
      "\t* ELIF there are multiple files, identify the file with \"train\" in the name as `train_df` and the file with \"test\" in the name as `test_df`.\n",
      "3. **Output**: Save the processed dataframes as 'train.csv' and 'test.csv'.\n",
      "\n",
      "**Data Preprocessing**\n",
      "\n",
      "1. **Drop PassengerId**: Drop the PassengerId column, as it is not relevant for training.\n",
      "2. **Ordinal Encoding**: Treat Pclass as an ordinal feature.\n",
      "3. **Title Extraction**: Extract the title (Mr/Mrs) from the Name column to create a new feature 'Title'.\n",
      "4. **Sex Mapping**: Map Sex to 0/1 values.\n",
      "5. **Age Imputation**: Fill missing Age values with the median value by Title.\n",
      "6. **Cabin Deck Extraction**: Extract the first letter of Cabin as 'Deck' and fill missing values with 'Unknown'.\n",
      "7. **One-Hot Encoding**: One-hot encode Embarked.\n",
      "\n",
      "**Model Development**\n",
      "\n",
      "1. **Feature Selection**: Select relevant features based on correlation analysis and feature importance scores.\n",
      "2. **Hyperparameter Tuning**: Perform grid search or random search to tune hyperparameters for the chosen model.\n",
      "3. **Model Training**: Train a suitable machine learning model (e.g., logistic regression, decision tree, random forest) using the preprocessed data.\n",
      "\n",
      "**Evaluation**\n",
      "\n",
      "1. **Model Evaluation Metrics**: Use accuracy, precision, recall, F1-score, and AUC-ROC to evaluate the performance of the trained model.\n",
      "2. **Hyperparameter Tuning**: Perform hyperparameter tuning for the chosen model using the evaluation metrics as a guide.\n",
      "\n",
      "**Deployment**\n",
      "\n",
      "1. **Model Deployment**: Deploy the trained model in a suitable environment (e.g., Flask API, TensorFlow Serving).\n",
      "2. **Model Monitoring**: Monitor the performance of the deployed model and retrain it periodically to maintain its accuracy.\n",
      "\n",
      "**Timeline**\n",
      "\n",
      "* Data loading and preprocessing: 2 days\n",
      "* Model development and hyperparameter tuning: 3-4 days\n",
      "* Evaluation and deployment: 1-2 days\n",
      "\n",
      "Total project duration: approximately 7-9 days\n",
      "\n",
      "This plan is designed for AI agents to execute, providing a detailed end-to-end process from data retrieval to model training and evaluation.\n",
      "\n",
      "        If there is no predefined data split or the data scientists suggest the data split other than train 70%, validation 20%, and test 10%, please use 70%, 20%, and 10% instead for consistency across different tasks. This is the retrievable data path: /Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets.\n",
      "        You should exclude every suggestion related to data visualization as you will be unable to see it.\n",
      "        \n",
      "        # - Instructions from Machine Learning Engineers\n",
      "        # model_plan_for_execution                    \n",
      "        \n",
      "        - User's Requirements\n",
      "        Predict survival (0/1)\n",
      "        \n",
      "    The dataset contains the following columns:\n",
      "    - PassengerId: Unique ID (Drop this for training)\n",
      "    - Survived: Target variable (0 = No, 1 = Yes)\n",
      "    - Pclass: Ticket class (1, 2, 3). Treat as Ordinal.\n",
      "    - Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')\n",
      "    - Sex: 'male'/'female'. (Map to 0/1)\n",
      "    - Age: Numeric. Contains missing values (Fill with Median by Title).\n",
      "    - Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')\n",
      "    - Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode).\n",
      "    \n",
      "        \n",
      "        Note that you must select only ONE promising solution (i.e., one data processing pipeline and one model from the top-three models) based on the above suggestions.\n",
      "        After choosing the best solution, give detailed instructions and guidelines for MLOps engineers who will write the code based on your instructions. Do not write the code by yourself. Since PyTorch is preferred for implementing deep learning and neural networks models, please guide the MLOPs engineers accordingly.\n",
      "        Make sure your instructions are sufficient with all essential information (e.g., complete path for dataset source and model location) for any MLOps or ML engineers to enable them to write the codes using existing libraries and frameworks correctly.\n",
      "                \n",
      "                # Previously Written Code\n",
      "                ```python\n",
      "                \n",
      "                ```\n",
      "                \n",
      "                # Error from the Previously Written Code\n",
      "                Nothing. This is your first attempt.\n",
      "                \n",
      "\n",
      "                Note that you need to write the python code for the entire machine learning pipeline (from data retrieval to model deployment via Gradio). If saving model is required, you must save the trained model to \"./agent_workspace/trained_models\" directory.\n",
      "                Start the python code with \"```python\". Please ensure the completeness of the code so that it can be run without additional modifications.\n",
      "                If there is any error from the previous attempt, please carefully fix it first.\n",
      "operation I got this error (itr #0): The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 25, in <module>\n",
      "    train_df.to_csv('train.csv', index=False)\n",
      "NameError: name 'train_df' is not defined\n",
      "\n",
      "____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "PROMPT GEN CODE \n",
      "\n",
      "Carefully read the following instructions to write Python code for Predict survival (0/1) task.\n",
      "                As the project manager, please carefully read and understand the following instructions suggested by data scientists and machine learning engineers. Then, select the best solution for the given user's requirements.\n",
      "        \n",
      "        - Instructions from Data Scientists\n",
      "        **AutoML Project Plan for Titanic Survival Prediction**\n",
      "\n",
      "**Project Overview**\n",
      "The objective of this project is to develop a machine learning model that predicts the survival rate of passengers on the Titanic based on the provided dataset.\n",
      "\n",
      "**Data Loading Instructions**\n",
      "\n",
      "1. **Folder Search Logic**: Use `os.walk` to search for folders in `/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets`. Look for folders containing the string \"Titanic\" and save it as `target_folder`.\n",
      "2. **File Loading Logic**:\n",
      "\t* List all .csv files in `target_folder`.\n",
      "\t* IF there is only one file, load it and split it 80/20 into `train_df` and `test_df`.\n",
      "\t* ELIF there are multiple files, identify the file with \"train\" in the name as `train_df` and the file with \"test\" in the name as `test_df`.\n",
      "3. **Output**: Save the processed dataframes as 'train.csv' and 'test.csv'.\n",
      "\n",
      "**Data Preprocessing**\n",
      "\n",
      "1. **Drop PassengerId**: Drop the PassengerId column, as it is not relevant for training.\n",
      "2. **Ordinal Encoding**: Treat Pclass as an ordinal feature.\n",
      "3. **Title Extraction**: Extract the title (Mr/Mrs) from the Name column to create a new feature 'Title'.\n",
      "4. **Sex Mapping**: Map Sex to 0/1 values.\n",
      "5. **Age Imputation**: Fill missing Age values with the median value by Title.\n",
      "6. **Cabin Deck Extraction**: Extract the first letter of Cabin as 'Deck' and fill missing values with 'Unknown'.\n",
      "7. **One-Hot Encoding**: One-hot encode Embarked.\n",
      "\n",
      "**Model Development**\n",
      "\n",
      "1. **Feature Selection**: Select relevant features based on correlation analysis and feature importance scores.\n",
      "2. **Hyperparameter Tuning**: Perform grid search or random search to tune hyperparameters for the chosen model.\n",
      "3. **Model Training**: Train a suitable machine learning model (e.g., logistic regression, decision tree, random forest) using the preprocessed data.\n",
      "\n",
      "**Evaluation**\n",
      "\n",
      "1. **Model Evaluation Metrics**: Use accuracy, precision, recall, F1-score, and AUC-ROC to evaluate the performance of the trained model.\n",
      "2. **Hyperparameter Tuning**: Perform hyperparameter tuning for the chosen model using the evaluation metrics as a guide.\n",
      "\n",
      "**Deployment**\n",
      "\n",
      "1. **Model Deployment**: Deploy the trained model in a suitable environment (e.g., Flask API, TensorFlow Serving).\n",
      "2. **Model Monitoring**: Monitor the performance of the deployed model and retrain it periodically to maintain its accuracy.\n",
      "\n",
      "**Timeline**\n",
      "\n",
      "* Data loading and preprocessing: 2 days\n",
      "* Model development and hyperparameter tuning: 3-4 days\n",
      "* Evaluation and deployment: 1-2 days\n",
      "\n",
      "Total project duration: approximately 7-9 days\n",
      "\n",
      "This plan is designed for AI agents to execute, providing a detailed end-to-end process from data retrieval to model training and evaluation.\n",
      "\n",
      "        If there is no predefined data split or the data scientists suggest the data split other than train 70%, validation 20%, and test 10%, please use 70%, 20%, and 10% instead for consistency across different tasks. This is the retrievable data path: /Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets.\n",
      "        You should exclude every suggestion related to data visualization as you will be unable to see it.\n",
      "        \n",
      "        # - Instructions from Machine Learning Engineers\n",
      "        # model_plan_for_execution                    \n",
      "        \n",
      "        - User's Requirements\n",
      "        Predict survival (0/1)\n",
      "        \n",
      "    The dataset contains the following columns:\n",
      "    - PassengerId: Unique ID (Drop this for training)\n",
      "    - Survived: Target variable (0 = No, 1 = Yes)\n",
      "    - Pclass: Ticket class (1, 2, 3). Treat as Ordinal.\n",
      "    - Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')\n",
      "    - Sex: 'male'/'female'. (Map to 0/1)\n",
      "    - Age: Numeric. Contains missing values (Fill with Median by Title).\n",
      "    - Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')\n",
      "    - Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode).\n",
      "    \n",
      "        \n",
      "        Note that you must select only ONE promising solution (i.e., one data processing pipeline and one model from the top-three models) based on the above suggestions.\n",
      "        After choosing the best solution, give detailed instructions and guidelines for MLOps engineers who will write the code based on your instructions. Do not write the code by yourself. Since PyTorch is preferred for implementing deep learning and neural networks models, please guide the MLOPs engineers accordingly.\n",
      "        Make sure your instructions are sufficient with all essential information (e.g., complete path for dataset source and model location) for any MLOps or ML engineers to enable them to write the codes using existing libraries and frameworks correctly.\n",
      "                \n",
      "                # Previously Written Code\n",
      "                ```python\n",
      "                \n",
      "import os\n",
      "target_folder = '/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets'\n",
      "for root, dirs, files in os.walk(target_folder):\n",
      "    if 'Titanic' in dirs:\n",
      "        target_folder = os.path.join(root, 'Titanic')\n",
      "        break\n",
      "\n",
      "train_files = [file for file in os.listdir(target_folder) if file.endswith('.csv') and 'train' in file]\n",
      "test_files = [file for file in os.listdir(target_folder) if file.endswith('.csv') and 'test' in file]\n",
      "\n",
      "if len(train_files) == 1:\n",
      "    train_df = pd.read_csv(os.path.join(target_folder, train_files[0]))\n",
      "    test_df = pd.read_csv(os.path.join(target_folder, test_files[0]))\n",
      "else:\n",
      "    for file in train_files:\n",
      "        if 'train' in file:\n",
      "            train_df = pd.read_csv(os.path.join(target_folder, file))\n",
      "            break\n",
      "    for file in test_files:\n",
      "        if 'test' in file:\n",
      "            test_df = pd.read_csv(os.path.join(target_folder, file))\n",
      "            break\n",
      "\n",
      "train_df.to_csv('train.csv', index=False)\n",
      "test_df.to_csv('test.csv', index=False)\n",
      "\n",
      "                ```\n",
      "                \n",
      "                # Error from the Previously Written Code\n",
      "                The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 25, in <module>\n",
      "    train_df.to_csv('train.csv', index=False)\n",
      "NameError: name 'train_df' is not defined\n",
      "\n",
      "                \n",
      "\n",
      "                Note that you need to write the python code for the entire machine learning pipeline (from data retrieval to model deployment via Gradio). If saving model is required, you must save the trained model to \"./agent_workspace/trained_models\" directory.\n",
      "                Start the python code with \"```python\". Please ensure the completeness of the code so that it can be run without additional modifications.\n",
      "                If there is any error from the previous attempt, please carefully fix it first.\n",
      "operation I got this error (itr #1): The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 13, in <module>\n",
      "    train_df = pd.read_csv(os.path.join(target_folder, train_files[0]))\n",
      "NameError: name 'pd' is not defined. Did you mean: 'id'?\n",
      "\n",
      "____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "PROMPT GEN CODE \n",
      "\n",
      "Carefully read the following instructions to write Python code for Predict survival (0/1) task.\n",
      "                As the project manager, please carefully read and understand the following instructions suggested by data scientists and machine learning engineers. Then, select the best solution for the given user's requirements.\n",
      "        \n",
      "        - Instructions from Data Scientists\n",
      "        **AutoML Project Plan for Titanic Survival Prediction**\n",
      "\n",
      "**Project Overview**\n",
      "The objective of this project is to develop a machine learning model that predicts the survival rate of passengers on the Titanic based on the provided dataset.\n",
      "\n",
      "**Data Loading Instructions**\n",
      "\n",
      "1. **Folder Search Logic**: Use `os.walk` to search for folders in `/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets`. Look for folders containing the string \"Titanic\" and save it as `target_folder`.\n",
      "2. **File Loading Logic**:\n",
      "\t* List all .csv files in `target_folder`.\n",
      "\t* IF there is only one file, load it and split it 80/20 into `train_df` and `test_df`.\n",
      "\t* ELIF there are multiple files, identify the file with \"train\" in the name as `train_df` and the file with \"test\" in the name as `test_df`.\n",
      "3. **Output**: Save the processed dataframes as 'train.csv' and 'test.csv'.\n",
      "\n",
      "**Data Preprocessing**\n",
      "\n",
      "1. **Drop PassengerId**: Drop the PassengerId column, as it is not relevant for training.\n",
      "2. **Ordinal Encoding**: Treat Pclass as an ordinal feature.\n",
      "3. **Title Extraction**: Extract the title (Mr/Mrs) from the Name column to create a new feature 'Title'.\n",
      "4. **Sex Mapping**: Map Sex to 0/1 values.\n",
      "5. **Age Imputation**: Fill missing Age values with the median value by Title.\n",
      "6. **Cabin Deck Extraction**: Extract the first letter of Cabin as 'Deck' and fill missing values with 'Unknown'.\n",
      "7. **One-Hot Encoding**: One-hot encode Embarked.\n",
      "\n",
      "**Model Development**\n",
      "\n",
      "1. **Feature Selection**: Select relevant features based on correlation analysis and feature importance scores.\n",
      "2. **Hyperparameter Tuning**: Perform grid search or random search to tune hyperparameters for the chosen model.\n",
      "3. **Model Training**: Train a suitable machine learning model (e.g., logistic regression, decision tree, random forest) using the preprocessed data.\n",
      "\n",
      "**Evaluation**\n",
      "\n",
      "1. **Model Evaluation Metrics**: Use accuracy, precision, recall, F1-score, and AUC-ROC to evaluate the performance of the trained model.\n",
      "2. **Hyperparameter Tuning**: Perform hyperparameter tuning for the chosen model using the evaluation metrics as a guide.\n",
      "\n",
      "**Deployment**\n",
      "\n",
      "1. **Model Deployment**: Deploy the trained model in a suitable environment (e.g., Flask API, TensorFlow Serving).\n",
      "2. **Model Monitoring**: Monitor the performance of the deployed model and retrain it periodically to maintain its accuracy.\n",
      "\n",
      "**Timeline**\n",
      "\n",
      "* Data loading and preprocessing: 2 days\n",
      "* Model development and hyperparameter tuning: 3-4 days\n",
      "* Evaluation and deployment: 1-2 days\n",
      "\n",
      "Total project duration: approximately 7-9 days\n",
      "\n",
      "This plan is designed for AI agents to execute, providing a detailed end-to-end process from data retrieval to model training and evaluation.\n",
      "\n",
      "        If there is no predefined data split or the data scientists suggest the data split other than train 70%, validation 20%, and test 10%, please use 70%, 20%, and 10% instead for consistency across different tasks. This is the retrievable data path: /Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets.\n",
      "        You should exclude every suggestion related to data visualization as you will be unable to see it.\n",
      "        \n",
      "        # - Instructions from Machine Learning Engineers\n",
      "        # model_plan_for_execution                    \n",
      "        \n",
      "        - User's Requirements\n",
      "        Predict survival (0/1)\n",
      "        \n",
      "    The dataset contains the following columns:\n",
      "    - PassengerId: Unique ID (Drop this for training)\n",
      "    - Survived: Target variable (0 = No, 1 = Yes)\n",
      "    - Pclass: Ticket class (1, 2, 3). Treat as Ordinal.\n",
      "    - Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')\n",
      "    - Sex: 'male'/'female'. (Map to 0/1)\n",
      "    - Age: Numeric. Contains missing values (Fill with Median by Title).\n",
      "    - Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')\n",
      "    - Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode).\n",
      "    \n",
      "        \n",
      "        Note that you must select only ONE promising solution (i.e., one data processing pipeline and one model from the top-three models) based on the above suggestions.\n",
      "        After choosing the best solution, give detailed instructions and guidelines for MLOps engineers who will write the code based on your instructions. Do not write the code by yourself. Since PyTorch is preferred for implementing deep learning and neural networks models, please guide the MLOPs engineers accordingly.\n",
      "        Make sure your instructions are sufficient with all essential information (e.g., complete path for dataset source and model location) for any MLOps or ML engineers to enable them to write the codes using existing libraries and frameworks correctly.\n",
      "                \n",
      "                # Previously Written Code\n",
      "                ```python\n",
      "                \n",
      "import os\n",
      "target_folder = '/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets'\n",
      "for root, dirs, files in os.walk(target_folder):\n",
      "    if 'Titanic' in dirs:\n",
      "        target_folder = os.path.join(root, 'Titanic')\n",
      "        break\n",
      "\n",
      "train_files = [file for file in os.listdir(target_folder) if file.endswith('.csv') and ('train' in file or 'test' not in file)]\n",
      "test_files = [file for file in os.listdir(target_folder) if file.endswith('.csv') and 'test' in file]\n",
      "\n",
      "if len(train_files) == 1:\n",
      "    train_df = pd.read_csv(os.path.join(target_folder, train_files[0]))\n",
      "    test_df = pd.read_csv(os.path.join(target_folder, test_files[0]))\n",
      "else:\n",
      "    for file in train_files:\n",
      "        if 'train' in file:\n",
      "            train_df = pd.read_csv(os.path.join(target_folder, file))\n",
      "            break\n",
      "    for file in test_files:\n",
      "        if 'test' in file:\n",
      "            test_df = pd.read_csv(os.path.join(target_folder, file))\n",
      "            break\n",
      "\n",
      "train_df.to_csv('train.csv', index=False)\n",
      "test_df.to_csv('test.csv', index=False)\n",
      "\n",
      "                ```\n",
      "                \n",
      "                # Error from the Previously Written Code\n",
      "                The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 13, in <module>\n",
      "    train_df = pd.read_csv(os.path.join(target_folder, train_files[0]))\n",
      "NameError: name 'pd' is not defined. Did you mean: 'id'?\n",
      "\n",
      "                \n",
      "\n",
      "                Note that you need to write the python code for the entire machine learning pipeline (from data retrieval to model deployment via Gradio). If saving model is required, you must save the trained model to \"./agent_workspace/trained_models\" directory.\n",
      "                Start the python code with \"```python\". Please ensure the completeness of the code so that it can be run without additional modifications.\n",
      "                If there is any error from the previous attempt, please carefully fix it first.\n",
      "operation I got this error (itr #2): The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 13, in <module>\n",
      "    train_df = pd.read_csv(os.path.join(target_folder, train_files[0]))\n",
      "NameError: name 'pd' is not defined. Did you mean: 'id'?\n",
      "\n",
      "____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "PROMPT GEN CODE \n",
      "\n",
      "Carefully read the following instructions to write Python code for Predict survival (0/1) task.\n",
      "                As the project manager, please carefully read and understand the following instructions suggested by data scientists and machine learning engineers. Then, select the best solution for the given user's requirements.\n",
      "        \n",
      "        - Instructions from Data Scientists\n",
      "        **AutoML Project Plan for Titanic Survival Prediction**\n",
      "\n",
      "**Project Overview**\n",
      "The objective of this project is to develop a machine learning model that predicts the survival rate of passengers on the Titanic based on the provided dataset.\n",
      "\n",
      "**Data Loading Instructions**\n",
      "\n",
      "1. **Folder Search Logic**: Use `os.walk` to search for folders in `/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets`. Look for folders containing the string \"Titanic\" and save it as `target_folder`.\n",
      "2. **File Loading Logic**:\n",
      "\t* List all .csv files in `target_folder`.\n",
      "\t* IF there is only one file, load it and split it 80/20 into `train_df` and `test_df`.\n",
      "\t* ELIF there are multiple files, identify the file with \"train\" in the name as `train_df` and the file with \"test\" in the name as `test_df`.\n",
      "3. **Output**: Save the processed dataframes as 'train.csv' and 'test.csv'.\n",
      "\n",
      "**Data Preprocessing**\n",
      "\n",
      "1. **Drop PassengerId**: Drop the PassengerId column, as it is not relevant for training.\n",
      "2. **Ordinal Encoding**: Treat Pclass as an ordinal feature.\n",
      "3. **Title Extraction**: Extract the title (Mr/Mrs) from the Name column to create a new feature 'Title'.\n",
      "4. **Sex Mapping**: Map Sex to 0/1 values.\n",
      "5. **Age Imputation**: Fill missing Age values with the median value by Title.\n",
      "6. **Cabin Deck Extraction**: Extract the first letter of Cabin as 'Deck' and fill missing values with 'Unknown'.\n",
      "7. **One-Hot Encoding**: One-hot encode Embarked.\n",
      "\n",
      "**Model Development**\n",
      "\n",
      "1. **Feature Selection**: Select relevant features based on correlation analysis and feature importance scores.\n",
      "2. **Hyperparameter Tuning**: Perform grid search or random search to tune hyperparameters for the chosen model.\n",
      "3. **Model Training**: Train a suitable machine learning model (e.g., logistic regression, decision tree, random forest) using the preprocessed data.\n",
      "\n",
      "**Evaluation**\n",
      "\n",
      "1. **Model Evaluation Metrics**: Use accuracy, precision, recall, F1-score, and AUC-ROC to evaluate the performance of the trained model.\n",
      "2. **Hyperparameter Tuning**: Perform hyperparameter tuning for the chosen model using the evaluation metrics as a guide.\n",
      "\n",
      "**Deployment**\n",
      "\n",
      "1. **Model Deployment**: Deploy the trained model in a suitable environment (e.g., Flask API, TensorFlow Serving).\n",
      "2. **Model Monitoring**: Monitor the performance of the deployed model and retrain it periodically to maintain its accuracy.\n",
      "\n",
      "**Timeline**\n",
      "\n",
      "* Data loading and preprocessing: 2 days\n",
      "* Model development and hyperparameter tuning: 3-4 days\n",
      "* Evaluation and deployment: 1-2 days\n",
      "\n",
      "Total project duration: approximately 7-9 days\n",
      "\n",
      "This plan is designed for AI agents to execute, providing a detailed end-to-end process from data retrieval to model training and evaluation.\n",
      "\n",
      "        If there is no predefined data split or the data scientists suggest the data split other than train 70%, validation 20%, and test 10%, please use 70%, 20%, and 10% instead for consistency across different tasks. This is the retrievable data path: /Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets.\n",
      "        You should exclude every suggestion related to data visualization as you will be unable to see it.\n",
      "        \n",
      "        # - Instructions from Machine Learning Engineers\n",
      "        # model_plan_for_execution                    \n",
      "        \n",
      "        - User's Requirements\n",
      "        Predict survival (0/1)\n",
      "        \n",
      "    The dataset contains the following columns:\n",
      "    - PassengerId: Unique ID (Drop this for training)\n",
      "    - Survived: Target variable (0 = No, 1 = Yes)\n",
      "    - Pclass: Ticket class (1, 2, 3). Treat as Ordinal.\n",
      "    - Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')\n",
      "    - Sex: 'male'/'female'. (Map to 0/1)\n",
      "    - Age: Numeric. Contains missing values (Fill with Median by Title).\n",
      "    - Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')\n",
      "    - Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode).\n",
      "    \n",
      "        \n",
      "        Note that you must select only ONE promising solution (i.e., one data processing pipeline and one model from the top-three models) based on the above suggestions.\n",
      "        After choosing the best solution, give detailed instructions and guidelines for MLOps engineers who will write the code based on your instructions. Do not write the code by yourself. Since PyTorch is preferred for implementing deep learning and neural networks models, please guide the MLOPs engineers accordingly.\n",
      "        Make sure your instructions are sufficient with all essential information (e.g., complete path for dataset source and model location) for any MLOps or ML engineers to enable them to write the codes using existing libraries and frameworks correctly.\n",
      "                \n",
      "                # Previously Written Code\n",
      "                ```python\n",
      "                \n",
      "import os\n",
      "target_folder = '/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets'\n",
      "for root, dirs, files in os.walk(target_folder):\n",
      "    if 'Titanic' in dirs:\n",
      "        target_folder = os.path.join(root, 'Titanic')\n",
      "        break\n",
      "\n",
      "train_files = [file for file in os.listdir(target_folder) if file.endswith('.csv') and ('train' in file or 'test' not in file)]\n",
      "test_files = [file for file in os.listdir(target_folder) if file.endswith('.csv') and 'test' in file]\n",
      "\n",
      "if len(train_files) == 1:\n",
      "    train_df = pd.read_csv(os.path.join(target_folder, train_files[0]))\n",
      "    test_df = pd.read_csv(os.path.join(target_folder, test_files[0]))\n",
      "else:\n",
      "    for file in train_files:\n",
      "        if 'train' in file:\n",
      "            train_df = pd.read_csv(os.path.join(target_folder, file))\n",
      "            break\n",
      "    for file in test_files:\n",
      "        if 'test' in file:\n",
      "            test_df = pd.read_csv(os.path.join(target_folder, file))\n",
      "            break\n",
      "\n",
      "train_df.to_csv('train.csv', index=False)\n",
      "test_df.to_csv('test.csv', index=False)\n",
      "\n",
      "                ```\n",
      "                \n",
      "                # Error from the Previously Written Code\n",
      "                The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 13, in <module>\n",
      "    train_df = pd.read_csv(os.path.join(target_folder, train_files[0]))\n",
      "NameError: name 'pd' is not defined. Did you mean: 'id'?\n",
      "\n",
      "                \n",
      "\n",
      "                Note that you need to write the python code for the entire machine learning pipeline (from data retrieval to model deployment via Gradio). If saving model is required, you must save the trained model to \"./agent_workspace/trained_models\" directory.\n",
      "                Start the python code with \"```python\". Please ensure the completeness of the code so that it can be run without additional modifications.\n",
      "                If there is any error from the previous attempt, please carefully fix it first.\n",
      "operation I got this error (itr #3): The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 13, in <module>\n",
      "    train_df = pd.read_csv(os.path.join(target_folder, train_files[0]))\n",
      "NameError: name 'pd' is not defined. Did you mean: 'id'?\n",
      "\n",
      "____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "PROMPT GEN CODE \n",
      "\n",
      "Carefully read the following instructions to write Python code for Predict survival (0/1) task.\n",
      "                As the project manager, please carefully read and understand the following instructions suggested by data scientists and machine learning engineers. Then, select the best solution for the given user's requirements.\n",
      "        \n",
      "        - Instructions from Data Scientists\n",
      "        **AutoML Project Plan for Titanic Survival Prediction**\n",
      "\n",
      "**Project Overview**\n",
      "The objective of this project is to develop a machine learning model that predicts the survival rate of passengers on the Titanic based on the provided dataset.\n",
      "\n",
      "**Data Loading Instructions**\n",
      "\n",
      "1. **Folder Search Logic**: Use `os.walk` to search for folders in `/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets`. Look for folders containing the string \"Titanic\" and save it as `target_folder`.\n",
      "2. **File Loading Logic**:\n",
      "\t* List all .csv files in `target_folder`.\n",
      "\t* IF there is only one file, load it and split it 80/20 into `train_df` and `test_df`.\n",
      "\t* ELIF there are multiple files, identify the file with \"train\" in the name as `train_df` and the file with \"test\" in the name as `test_df`.\n",
      "3. **Output**: Save the processed dataframes as 'train.csv' and 'test.csv'.\n",
      "\n",
      "**Data Preprocessing**\n",
      "\n",
      "1. **Drop PassengerId**: Drop the PassengerId column, as it is not relevant for training.\n",
      "2. **Ordinal Encoding**: Treat Pclass as an ordinal feature.\n",
      "3. **Title Extraction**: Extract the title (Mr/Mrs) from the Name column to create a new feature 'Title'.\n",
      "4. **Sex Mapping**: Map Sex to 0/1 values.\n",
      "5. **Age Imputation**: Fill missing Age values with the median value by Title.\n",
      "6. **Cabin Deck Extraction**: Extract the first letter of Cabin as 'Deck' and fill missing values with 'Unknown'.\n",
      "7. **One-Hot Encoding**: One-hot encode Embarked.\n",
      "\n",
      "**Model Development**\n",
      "\n",
      "1. **Feature Selection**: Select relevant features based on correlation analysis and feature importance scores.\n",
      "2. **Hyperparameter Tuning**: Perform grid search or random search to tune hyperparameters for the chosen model.\n",
      "3. **Model Training**: Train a suitable machine learning model (e.g., logistic regression, decision tree, random forest) using the preprocessed data.\n",
      "\n",
      "**Evaluation**\n",
      "\n",
      "1. **Model Evaluation Metrics**: Use accuracy, precision, recall, F1-score, and AUC-ROC to evaluate the performance of the trained model.\n",
      "2. **Hyperparameter Tuning**: Perform hyperparameter tuning for the chosen model using the evaluation metrics as a guide.\n",
      "\n",
      "**Deployment**\n",
      "\n",
      "1. **Model Deployment**: Deploy the trained model in a suitable environment (e.g., Flask API, TensorFlow Serving).\n",
      "2. **Model Monitoring**: Monitor the performance of the deployed model and retrain it periodically to maintain its accuracy.\n",
      "\n",
      "**Timeline**\n",
      "\n",
      "* Data loading and preprocessing: 2 days\n",
      "* Model development and hyperparameter tuning: 3-4 days\n",
      "* Evaluation and deployment: 1-2 days\n",
      "\n",
      "Total project duration: approximately 7-9 days\n",
      "\n",
      "This plan is designed for AI agents to execute, providing a detailed end-to-end process from data retrieval to model training and evaluation.\n",
      "\n",
      "        If there is no predefined data split or the data scientists suggest the data split other than train 70%, validation 20%, and test 10%, please use 70%, 20%, and 10% instead for consistency across different tasks. This is the retrievable data path: /Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets.\n",
      "        You should exclude every suggestion related to data visualization as you will be unable to see it.\n",
      "        \n",
      "        # - Instructions from Machine Learning Engineers\n",
      "        # model_plan_for_execution                    \n",
      "        \n",
      "        - User's Requirements\n",
      "        Predict survival (0/1)\n",
      "        \n",
      "    The dataset contains the following columns:\n",
      "    - PassengerId: Unique ID (Drop this for training)\n",
      "    - Survived: Target variable (0 = No, 1 = Yes)\n",
      "    - Pclass: Ticket class (1, 2, 3). Treat as Ordinal.\n",
      "    - Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')\n",
      "    - Sex: 'male'/'female'. (Map to 0/1)\n",
      "    - Age: Numeric. Contains missing values (Fill with Median by Title).\n",
      "    - Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')\n",
      "    - Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode).\n",
      "    \n",
      "        \n",
      "        Note that you must select only ONE promising solution (i.e., one data processing pipeline and one model from the top-three models) based on the above suggestions.\n",
      "        After choosing the best solution, give detailed instructions and guidelines for MLOps engineers who will write the code based on your instructions. Do not write the code by yourself. Since PyTorch is preferred for implementing deep learning and neural networks models, please guide the MLOPs engineers accordingly.\n",
      "        Make sure your instructions are sufficient with all essential information (e.g., complete path for dataset source and model location) for any MLOps or ML engineers to enable them to write the codes using existing libraries and frameworks correctly.\n",
      "                \n",
      "                # Previously Written Code\n",
      "                ```python\n",
      "                \n",
      "import os\n",
      "target_folder = '/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets'\n",
      "for root, dirs, files in os.walk(target_folder):\n",
      "    if 'Titanic' in dirs:\n",
      "        target_folder = os.path.join(root, 'Titanic')\n",
      "        break\n",
      "\n",
      "train_files = [file for file in os.listdir(target_folder) if file.endswith('.csv') and ('train' in file or 'test' not in file)]\n",
      "test_files = [file for file in os.listdir(target_folder) if file.endswith('.csv') and 'test' in file]\n",
      "\n",
      "if len(train_files) == 1:\n",
      "    train_df = pd.read_csv(os.path.join(target_folder, train_files[0]))\n",
      "    test_df = pd.read_csv(os.path.join(target_folder, test_files[0]))\n",
      "else:\n",
      "    for file in train_files:\n",
      "        if 'train' in file:\n",
      "            train_df = pd.read_csv(os.path.join(target_folder, file))\n",
      "            break\n",
      "    for file in test_files:\n",
      "        if 'test' in file:\n",
      "            test_df = pd.read_csv(os.path.join(target_folder, file))\n",
      "            break\n",
      "\n",
      "train_df.to_csv('train.csv', index=False)\n",
      "test_df.to_csv('test.csv', index=False)\n",
      "\n",
      "                ```\n",
      "                \n",
      "                # Error from the Previously Written Code\n",
      "                The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 13, in <module>\n",
      "    train_df = pd.read_csv(os.path.join(target_folder, train_files[0]))\n",
      "NameError: name 'pd' is not defined. Did you mean: 'id'?\n",
      "\n",
      "                \n",
      "\n",
      "                Note that you need to write the python code for the entire machine learning pipeline (from data retrieval to model deployment via Gradio). If saving model is required, you must save the trained model to \"./agent_workspace/trained_models\" directory.\n",
      "                Start the python code with \"```python\". Please ensure the completeness of the code so that it can be run without additional modifications.\n",
      "                If there is any error from the previous attempt, please carefully fix it first.\n",
      "operation I got this error (itr #4): The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 16, in <module>\n",
      "    test_df = pd.read_csv(os.path.join(target_folder, test_files[0]))\n",
      "IndexError: list index out of range\n",
      "\n",
      "operation I executed the given plan and got the follow results:\n",
      "\n",
      "The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 16, in <module>\n",
      "    test_df = pd.read_csv(os.path.join(target_folder, test_files[0]))\n",
      "IndexError: list index out of range\n",
      "\n",
      "system /0_p1_full, <<< END CODING, TIME USED: 1627.7910277843475 SECS >>>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rcode': 1,\n",
       " 'action_result': 'The script has been executed. Here is the output:\\nTraceback (most recent call last):\\n  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 16, in <module>\\n    test_df = pd.read_csv(os.path.join(target_folder, test_files[0]))\\nIndexError: list index out of range\\n',\n",
       " 'code': \"\\nimport os\\nimport pandas as pd\\n\\ntarget_folder = '/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets'\\nfor root, dirs, files in os.walk(target_folder):\\n    if 'Titanic' in dirs:\\n        target_folder = os.path.join(root, 'Titanic')\\n        break\\n\\ntrain_files = [file for file in os.listdir(target_folder) if file.endswith('.csv') and ('train' in file or 'test' not in file)]\\ntest_files = [file for file in os.listdir(target_folder) if file.endswith('.csv') and 'test' in file]\\n\\nif len(train_files) == 1:\\n    train_df = pd.read_csv(os.path.join(target_folder, train_files[0]))\\n    test_df = pd.read_csv(os.path.join(target_folder, test_files[0]))\\nelse:\\n    for file in train_files:\\n        if 'train' in file:\\n            train_df = pd.read_csv(os.path.join(target_folder, file))\\n            break\\n    for file in test_files:\\n        if 'test' in file:\\n            test_df = pd.read_csv(os.path.join(target_folder, file))\\n            break\\n\\n# Drop PassengerId column\\ntrain_df.drop('PassengerId', axis=1, inplace=True)\\ntest_df.drop('PassengerId', axis=1, inplace=True)\\n\\n# Ordinal Encoding for Pclass\\ntrain_df['Pclass'] = train_df['Pclass'].astype('category')\\ntest_df['Pclass'] = test_df['Pclass'].astype('category')\\n\\n# Extract Title from Name column\\ntrain_df['Title'] = train_df['Name'].apply(lambda x: x.split(',')[1].split('.')[0])\\ntest_df['Title'] = test_df['Name'].apply(lambda x: x.split(',')[1].split('.')[0])\\n\\n# Map Sex to 0/1 values\\ntrain_df['Sex'] = train_df['Sex'].map({'male': 0, 'female': 1})\\ntest_df['Sex'] = test_df['Sex'].map({'male': 0, 'female': 1})\\n\\n# Fill missing Age values with median by Title\\nmedian_age_by_title = train_df.groupby('Title')['Age'].transform('median')\\ntrain_df.loc[train_df['Age'].isna(), 'Age'] = median_age_by_title\\ntest_df.loc[test_df['Age'].isna(), 'Age'] = median_age_by_title\\n\\n# Extract first letter of Cabin as 'Deck' and fill missing values with 'Unknown'\\ntrain_df['Cabin Deck'] = train_df['Cabin'].apply(lambda x: x[0] if pd.notna(x) else 'Unknown')\\ntest_df['Cabin Deck'] = test_df['Cabin'].apply(lambda x: x[0] if pd.notna(x) else 'Unknown')\\n\\n# One-Hot Encoding for Embarked\\ntrain_df = pd.get_dummies(train_df, columns=['Embarked'])\\ntest_df = pd.get_dummies(test_df, columns=['Embarked'])\\n\\n# Split data into training and validation sets (70%, 20%, 10%)\\nfrom sklearn.model_selection import train_test_split\\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\\n\\n# Save the preprocessed dataframes as 'train.csv' and 'val.csv'\\ntrain_df.to_csv('train.csv', index=False)\\nval_df.to_csv('val.csv', index=False)\\n\",\n",
       " 'error_logs': ['The script has been executed. Here is the output:\\nTraceback (most recent call last):\\n  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 25, in <module>\\n    train_df.to_csv(\\'train.csv\\', index=False)\\nNameError: name \\'train_df\\' is not defined\\n',\n",
       "  'The script has been executed. Here is the output:\\nTraceback (most recent call last):\\n  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 13, in <module>\\n    train_df = pd.read_csv(os.path.join(target_folder, train_files[0]))\\nNameError: name \\'pd\\' is not defined. Did you mean: \\'id\\'?\\n',\n",
       "  'The script has been executed. Here is the output:\\nTraceback (most recent call last):\\n  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 13, in <module>\\n    train_df = pd.read_csv(os.path.join(target_folder, train_files[0]))\\nNameError: name \\'pd\\' is not defined. Did you mean: \\'id\\'?\\n',\n",
       "  'The script has been executed. Here is the output:\\nTraceback (most recent call last):\\n  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 13, in <module>\\n    train_df = pd.read_csv(os.path.join(target_folder, train_files[0]))\\nNameError: name \\'pd\\' is not defined. Did you mean: \\'id\\'?\\n',\n",
       "  'The script has been executed. Here is the output:\\nTraceback (most recent call last):\\n  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 16, in <module>\\n    test_df = pd.read_csv(os.path.join(target_folder, test_files[0]))\\nIndexError: list index out of range\\n']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_results = [{}]\n",
    "action_results[0]['DATA'] = manager.make_plans()[0]\n",
    "action_results[0]['pass'] = True\n",
    "\n",
    "gen_code = manager.generate_code(action_results=action_results)\n",
    "gen_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93e31d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   (Agent is thinking...)\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'**End-to-End Plan for AutoML Project: Titanic Survival Prediction**\\n\\n**Step 1: Data Retrieval and Preprocessing**\\n\\n* Use `os.walk` to search for folders in `/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets`.\\n* Find the folder that contains the string \"Titanic\" and save it as `target_folder`.\\n* List all `.csv` files in `target_folder`.\\n* If there is only one file, load it and split it 80/20 into `train_df` and `test_df`. Otherwise:\\n\\t+ Identify the file with \"train\" in the name -> `train_df`.\\n\\t+ Identify the file with \"test\" in the name -> `test_df`.\\n\\n**Step 2: Data Cleaning and Feature Engineering**\\n\\n* Drop the `PassengerId` column as it\\'s not relevant for training.\\n* Treat `Pclass` as an ordinal feature.\\n* Extract the title from `Name` and create a new feature `Title`.\\n* Map `Sex` to numerical values (0/1).\\n* Fill missing values in `Age` with median values by `Title`.\\n* Extract the first letter of `Cabin` as \\'Deck\\' and fill missing values with \\'Unknown\\'.\\n* One-hot encode `Embarked`.\\n\\n**Step 3: Data Splitting and Modeling**\\n\\n* Split the preprocessed data into training (`train_df`) and testing sets (`test_df`).\\n* Train a suitable machine learning model (e.g., logistic regression, decision tree, random forest) on `train_df`.\\n* Use hyperparameter tuning (e.g., grid search, random search) to optimize model performance.\\n* Evaluate the trained model on `test_df`.\\n\\n**Step 4: Model Evaluation and Selection**\\n\\n* Calculate relevant evaluation metrics (e.g., accuracy, F1-score, AUC-ROC).\\n* Compare the performance of different models using these metrics.\\n* Select the best-performing model based on the evaluation metrics.\\n\\n**Step 5: Model Deployment and Monitoring**\\n\\n* Deploy the selected model in a suitable environment (e.g., Flask API, TensorFlow Serving).\\n* Monitor the model\\'s performance using real-world data or online platforms (e.g., Kaggle).\\n\\n**Additional Considerations**\\n\\n* Ensure that all steps are executed in an automated manner using AI agents.\\n* Use state-of-the-art libraries and tools for data preprocessing, modeling, and deployment (e.g., Pandas, NumPy, scikit-learn, TensorFlow).\\n* Continuously monitor the model\\'s performance and retrain it as needed to maintain its accuracy.\\n\\nThis plan provides a comprehensive end-to-end process for developing an AutoML project that predicts survival based on the Titanic dataset.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.make_plans()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76259f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY_PROMPT_GENCODE \n",
      "\n",
      "As the project manager, please carefully read and understand the following instructions suggested by data scientists and machine learning engineers. Then, select the best solution for the given user's requirements.\n",
      "        \n",
      "        - Instructions from Data Scientists\n",
      "        You are an Autonomous Data Engineer. Contest: 'Titanic'.\n",
      "ROOT SEARCH PATH: '/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets'.\n",
      "DATA DESCRIPTION:The dataset contains the following columns:        - PassengerId: Unique ID (Drop this for training)       - Survived: Target variable (0 = No, 1 = Yes)- Pclass: Ticket class (1, 2, 3). Treat as Ordinal.- Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')- Sex: 'male'/'female'. (Map to 0/1)- Age: Numeric. Contains missing values (Fill with Median by Title).- Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')- Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode)..\n",
      "TASK: Write a Python script to Find, Inspect, and Prepare data.\n",
      "LOGIC FLOW:\n",
      "1. SEARCH: Find the subfolder inside ROOT SEARCH PATH that matches the Contest Name.\n",
      "2. INSPECT: List all files in that folder.\n",
      "3. DECIDE STRATEGY:\n",
      "   - CASE A: If you find separate 'train' and 'test' files -> Load them directly.\n",
      "   - CASE B: If you find 'train' and 'test' and 'val' -> Merge 'val' into 'train' or ignore it, then Load train/test.\n",
      "   - CASE C: check in folder, if only one file csv => load this as data files.\n",
      "4. PREPARE: Handle missing values and Encode categoricals (OneHot/Label).\n",
      "5. DETECT: Check label and data, label can based on data_description, rename this col into 'target' 6. OUTPUT: Save 'processed_train.csv' and 'processed_test.csv' to the current directory.\n",
      "7. PRINT: 'DATA_PROCESSED: processed_train.csv processed_test.csv' at the end.\n",
      "OUTPUT: ONLY valid Python code.\n",
      "\n",
      "        If there is no predefined data split or the data scientists suggest the data split other than train 70%, validation 20%, and test 10%, please use 70%, 20%, and 10% instead for consistency across different tasks. This is the retrievable data path: /Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets.\n",
      "        You should exclude every suggestion related to data visualization as you will be unable to see it.\n",
      "        \n",
      "        # - Instructions from Machine Learning Engineers\n",
      "        # model_plan_for_execution                    \n",
      "        \n",
      "        - User's Requirements\n",
      "        Predict survival (0/1)\n",
      "        \n",
      "    The dataset contains the following columns:\n",
      "    - PassengerId: Unique ID (Drop this for training)\n",
      "    - Survived: Target variable (0 = No, 1 = Yes)\n",
      "    - Pclass: Ticket class (1, 2, 3). Treat as Ordinal.\n",
      "    - Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')\n",
      "    - Sex: 'male'/'female'. (Map to 0/1)\n",
      "    - Age: Numeric. Contains missing values (Fill with Median by Title).\n",
      "    - Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')\n",
      "    - Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode).\n",
      "    \n",
      "        \n",
      "        Note that you must select only ONE promising solution (i.e., one data processing pipeline and one model from the top-three models) based on the above suggestions.\n",
      "        After choosing the best solution, give detailed instructions and guidelines for MLOps engineers who will write the code based on your instructions. Do not write the code by yourself. Since PyTorch is preferred for implementing deep learning and neural networks models, please guide the MLOPs engineers accordingly.\n",
      "        Make sure your instructions are sufficient with all essential information (e.g., complete path for dataset source and model location) for any MLOps or ML engineers to enable them to write the codes using existing libraries and frameworks correctly.\n",
      "operation I am implementing the following instruction\n",
      "____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "PROMPT GEN CODE \n",
      "\n",
      "Carefully read the following instructions to write Python code for Predict survival (0/1) task.\n",
      "                You are an Autonomous Data Engineer. Contest: 'Titanic'.\n",
      "ROOT SEARCH PATH: '/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets'.\n",
      "DATA DESCRIPTION:The dataset contains the following columns:        - PassengerId: Unique ID (Drop this for training)       - Survived: Target variable (0 = No, 1 = Yes)- Pclass: Ticket class (1, 2, 3). Treat as Ordinal.- Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')- Sex: 'male'/'female'. (Map to 0/1)- Age: Numeric. Contains missing values (Fill with Median by Title).- Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')- Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode)..\n",
      "TASK: Write a Python script to Find, Inspect, and Prepare data.\n",
      "LOGIC FLOW:\n",
      "1. SEARCH: Find the subfolder inside ROOT SEARCH PATH that matches the Contest Name.\n",
      "2. INSPECT: List all files in that folder.\n",
      "3. DECIDE STRATEGY:\n",
      "   - CASE A: If you find separate 'train' and 'test' files -> Load them directly.\n",
      "   - CASE B: If you find 'train' and 'test' and 'val' -> Merge 'val' into 'train' or ignore it, then Load train/test.\n",
      "   - CASE C: check in folder, if only one file csv => load this as data files.\n",
      "4. PREPARE: Handle missing values and Encode categoricals (OneHot/Label).\n",
      "5. DETECT: Check label and data, label can based on data_description, rename this col into 'target' 6. OUTPUT: Save 'processed_train.csv' and 'processed_test.csv' to the current directory.\n",
      "7. PRINT: 'DATA_PROCESSED: processed_train.csv processed_test.csv' at the end.\n",
      "OUTPUT: ONLY valid Python code.\n",
      "\n",
      "                \n",
      "                # Previously Written Code\n",
      "                ```python\n",
      "                \n",
      "                ```\n",
      "                \n",
      "                # Error from the Previously Written Code\n",
      "                Nothing. This is your first attempt.\n",
      "                \n",
      "\n",
      "                Note that you need to write the python code for the data processing part. If saving model is required, you must save the trained model to \"./agent_workspace/trained_models\" directory.\n",
      "                Start the python code with \"```python\". Please ensure the completeness of the code so that it can be run without additional modifications.\n",
      "                If there is any error from the previous attempt, please carefully fix it first.\n",
      "operation ===== Retry: 1 =====\n",
      "operation Executioin error occurs: list index out of range\n",
      "____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "PROMPT GEN CODE \n",
      "\n",
      "Carefully read the following instructions to write Python code for Predict survival (0/1) task.\n",
      "                You are an Autonomous Data Engineer. Contest: 'Titanic'.\n",
      "ROOT SEARCH PATH: '/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets'.\n",
      "DATA DESCRIPTION:The dataset contains the following columns:        - PassengerId: Unique ID (Drop this for training)       - Survived: Target variable (0 = No, 1 = Yes)- Pclass: Ticket class (1, 2, 3). Treat as Ordinal.- Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')- Sex: 'male'/'female'. (Map to 0/1)- Age: Numeric. Contains missing values (Fill with Median by Title).- Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')- Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode)..\n",
      "TASK: Write a Python script to Find, Inspect, and Prepare data.\n",
      "LOGIC FLOW:\n",
      "1. SEARCH: Find the subfolder inside ROOT SEARCH PATH that matches the Contest Name.\n",
      "2. INSPECT: List all files in that folder.\n",
      "3. DECIDE STRATEGY:\n",
      "   - CASE A: If you find separate 'train' and 'test' files -> Load them directly.\n",
      "   - CASE B: If you find 'train' and 'test' and 'val' -> Merge 'val' into 'train' or ignore it, then Load train/test.\n",
      "   - CASE C: check in folder, if only one file csv => load this as data files.\n",
      "4. PREPARE: Handle missing values and Encode categoricals (OneHot/Label).\n",
      "5. DETECT: Check label and data, label can based on data_description, rename this col into 'target' 6. OUTPUT: Save 'processed_train.csv' and 'processed_test.csv' to the current directory.\n",
      "7. PRINT: 'DATA_PROCESSED: processed_train.csv processed_test.csv' at the end.\n",
      "OUTPUT: ONLY valid Python code.\n",
      "\n",
      "                \n",
      "                # Previously Written Code\n",
      "                ```python\n",
      "                \n",
      "                ```\n",
      "                \n",
      "                # Error from the Previously Written Code\n",
      "                Nothing. This is your first attempt.\n",
      "                \n",
      "\n",
      "                Note that you need to write the python code for the data processing part. If saving model is required, you must save the trained model to \"./agent_workspace/trained_models\" directory.\n",
      "                Start the python code with \"```python\". Please ensure the completeness of the code so that it can be run without additional modifications.\n",
      "                If there is any error from the previous attempt, please carefully fix it first.\n",
      "operation I got this error (itr #1): The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 42, in <module>\n",
      "    df['Embarked'] = pd.get_dummies(df['Embarked'])\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/pandas/core/frame.py\", line 3645, in __setitem__\n",
      "    self._set_item_frame_value(key, value)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/pandas/core/frame.py\", line 3775, in _set_item_frame_value\n",
      "    raise ValueError(\"Columns must be same length as key\")\n",
      "ValueError: Columns must be same length as key\n",
      "\n",
      "____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "PROMPT GEN CODE \n",
      "\n",
      "Carefully read the following instructions to write Python code for Predict survival (0/1) task.\n",
      "                You are an Autonomous Data Engineer. Contest: 'Titanic'.\n",
      "ROOT SEARCH PATH: '/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets'.\n",
      "DATA DESCRIPTION:The dataset contains the following columns:        - PassengerId: Unique ID (Drop this for training)       - Survived: Target variable (0 = No, 1 = Yes)- Pclass: Ticket class (1, 2, 3). Treat as Ordinal.- Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')- Sex: 'male'/'female'. (Map to 0/1)- Age: Numeric. Contains missing values (Fill with Median by Title).- Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')- Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode)..\n",
      "TASK: Write a Python script to Find, Inspect, and Prepare data.\n",
      "LOGIC FLOW:\n",
      "1. SEARCH: Find the subfolder inside ROOT SEARCH PATH that matches the Contest Name.\n",
      "2. INSPECT: List all files in that folder.\n",
      "3. DECIDE STRATEGY:\n",
      "   - CASE A: If you find separate 'train' and 'test' files -> Load them directly.\n",
      "   - CASE B: If you find 'train' and 'test' and 'val' -> Merge 'val' into 'train' or ignore it, then Load train/test.\n",
      "   - CASE C: check in folder, if only one file csv => load this as data files.\n",
      "4. PREPARE: Handle missing values and Encode categoricals (OneHot/Label).\n",
      "5. DETECT: Check label and data, label can based on data_description, rename this col into 'target' 6. OUTPUT: Save 'processed_train.csv' and 'processed_test.csv' to the current directory.\n",
      "7. PRINT: 'DATA_PROCESSED: processed_train.csv processed_test.csv' at the end.\n",
      "OUTPUT: ONLY valid Python code.\n",
      "\n",
      "                \n",
      "                # Previously Written Code\n",
      "                ```python\n",
      "                \n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "# Set the root search path\n",
      "root_search_path = '/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets'\n",
      "\n",
      "# Find the subfolder inside ROOT SEARCH PATH that matches the Contest Name\n",
      "contest_name = 'Titanic'\n",
      "contest_folder = [f for f in os.listdir(root_search_path) if contest_name in f][0]\n",
      "\n",
      "# List all files in that folder\n",
      "files_in_contest_folder = os.listdir(os.path.join(root_search_path, contest_folder))\n",
      "\n",
      "# Decide strategy based on file names\n",
      "if any('train.csv' in f and 'test.csv' in f for f in files_in_contest_folder):\n",
      "    # CASE A: Load train and test files directly\n",
      "    train_file = [f for f in files_in_contest_folder if 'train.csv' in f][0]\n",
      "    test_file = [f for f in files_in_contest_folder if 'test.csv' in f][0]\n",
      "    train_df = pd.read_csv(os.path.join(root_search_path, contest_folder, train_file))\n",
      "    test_df = pd.read_csv(os.path.join(root_search_path, contest_folder, test_file))\n",
      "elif any('train.csv' in f and 'val.csv' in f for f in files_in_contest_folder):\n",
      "    # CASE B: Merge val into train or ignore it, then load train/test\n",
      "    if 'val.csv' in [f for f in files_in_contest_folder]:\n",
      "        val_file = [f for f in files_in_contest_folder if 'val.csv' in f][0]\n",
      "        train_df = pd.read_csv(os.path.join(root_search_path, contest_folder, 'train.csv'))\n",
      "        test_df = pd.read_csv(os.path.join(root_search_path, contest_folder, 'test.csv'))\n",
      "    else:\n",
      "        train_file = [f for f in files_in_contest_folder if 'train.csv' in f][0]\n",
      "        test_file = [f for f in files_in_contest_folder if 'test.csv' in f][0]\n",
      "        train_df = pd.read_csv(os.path.join(root_search_path, contest_folder, train_file))\n",
      "        test_df = pd.read_csv(os.path.join(root_search_path, contest_folder, test_file))\n",
      "else:\n",
      "    # CASE C: Load single csv file as data files\n",
      "    data_file = [f for f in files_in_contest_folder if 'csv' in f][0]\n",
      "    df = pd.read_csv(os.path.join(root_search_path, contest_folder, data_file))\n",
      "\n",
      "# Prepare data\n",
      "df['Title'] = df['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip().title())\n",
      "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
      "df['Cabin'] = df['Cabin'].str[0]\n",
      "df['Embarked'] = pd.get_dummies(df['Embarked'])\n",
      "\n",
      "# Detect label and data\n",
      "target_col = 'Survived'\n",
      "df.rename(columns={target_col: target_col}, inplace=True)\n",
      "\n",
      "# Output\n",
      "train_df.to_csv('processed_train.csv', index=False)\n",
      "test_df.to_csv('processed_test.csv', index=False)\n",
      "\n",
      "print('DATA_PROCESSED: processed_train.csv processed_test.csv')\n",
      "\n",
      "                ```\n",
      "                \n",
      "                # Error from the Previously Written Code\n",
      "                The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 42, in <module>\n",
      "    df['Embarked'] = pd.get_dummies(df['Embarked'])\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/pandas/core/frame.py\", line 3645, in __setitem__\n",
      "    self._set_item_frame_value(key, value)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/pandas/core/frame.py\", line 3775, in _set_item_frame_value\n",
      "    raise ValueError(\"Columns must be same length as key\")\n",
      "ValueError: Columns must be same length as key\n",
      "\n",
      "                \n",
      "\n",
      "                Note that you need to write the python code for the data processing part. If saving model is required, you must save the trained model to \"./agent_workspace/trained_models\" directory.\n",
      "                Start the python code with \"```python\". Please ensure the completeness of the code so that it can be run without additional modifications.\n",
      "                If there is any error from the previous attempt, please carefully fix it first.\n",
      "operation I got this error (itr #2): The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 43, in <module>\n",
      "    df['Embarked'] = pd.get_dummies(df['Embarked'])\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/pandas/core/frame.py\", line 3645, in __setitem__\n",
      "    self._set_item_frame_value(key, value)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/pandas/core/frame.py\", line 3775, in _set_item_frame_value\n",
      "    raise ValueError(\"Columns must be same length as key\")\n",
      "ValueError: Columns must be same length as key\n",
      "\n",
      "____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "PROMPT GEN CODE \n",
      "\n",
      "Carefully read the following instructions to write Python code for Predict survival (0/1) task.\n",
      "                You are an Autonomous Data Engineer. Contest: 'Titanic'.\n",
      "ROOT SEARCH PATH: '/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets'.\n",
      "DATA DESCRIPTION:The dataset contains the following columns:        - PassengerId: Unique ID (Drop this for training)       - Survived: Target variable (0 = No, 1 = Yes)- Pclass: Ticket class (1, 2, 3). Treat as Ordinal.- Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')- Sex: 'male'/'female'. (Map to 0/1)- Age: Numeric. Contains missing values (Fill with Median by Title).- Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')- Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode)..\n",
      "TASK: Write a Python script to Find, Inspect, and Prepare data.\n",
      "LOGIC FLOW:\n",
      "1. SEARCH: Find the subfolder inside ROOT SEARCH PATH that matches the Contest Name.\n",
      "2. INSPECT: List all files in that folder.\n",
      "3. DECIDE STRATEGY:\n",
      "   - CASE A: If you find separate 'train' and 'test' files -> Load them directly.\n",
      "   - CASE B: If you find 'train' and 'test' and 'val' -> Merge 'val' into 'train' or ignore it, then Load train/test.\n",
      "   - CASE C: check in folder, if only one file csv => load this as data files.\n",
      "4. PREPARE: Handle missing values and Encode categoricals (OneHot/Label).\n",
      "5. DETECT: Check label and data, label can based on data_description, rename this col into 'target' 6. OUTPUT: Save 'processed_train.csv' and 'processed_test.csv' to the current directory.\n",
      "7. PRINT: 'DATA_PROCESSED: processed_train.csv processed_test.csv' at the end.\n",
      "OUTPUT: ONLY valid Python code.\n",
      "\n",
      "                \n",
      "                # Previously Written Code\n",
      "                ```python\n",
      "                \n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "# Set the root search path\n",
      "root_search_path = '/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets'\n",
      "\n",
      "# Find the subfolder inside ROOT SEARCH PATH that matches the Contest Name\n",
      "contest_name = 'Titanic'\n",
      "contest_folder = [f for f in os.listdir(root_search_path) if contest_name in f][0]\n",
      "\n",
      "# List all files in that folder\n",
      "files_in_contest_folder = os.listdir(os.path.join(root_search_path, contest_folder))\n",
      "\n",
      "# Decide strategy based on file names\n",
      "if any('train.csv' in f and 'test.csv' in f for f in files_in_contest_folder):\n",
      "    # CASE A: Load train and test files directly\n",
      "    train_file = [f for f in files_in_contest_folder if 'train.csv' in f][0]\n",
      "    test_file = [f for f in files_in_contest_folder if 'test.csv' in f][0]\n",
      "    train_df = pd.read_csv(os.path.join(root_search_path, contest_folder, train_file))\n",
      "    test_df = pd.read_csv(os.path.join(root_search_path, contest_folder, test_file))\n",
      "elif any('train.csv' in f and 'val.csv' in f for f in files_in_contest_folder):\n",
      "    # CASE B: Merge val into train or ignore it, then load train/test\n",
      "    if 'val.csv' in [f for f in files_in_contest_folder]:\n",
      "        val_file = [f for f in files_in_contest_folder if 'val.csv' in f][0]\n",
      "        train_df = pd.concat([pd.read_csv(os.path.join(root_search_path, contest_folder, 'train.csv')), \n",
      "                              pd.read_csv(os.path.join(root_search_path, contest_folder, val_file))])\n",
      "        test_df = pd.read_csv(os.path.join(root_search_path, contest_folder, 'test.csv'))\n",
      "    else:\n",
      "        train_file = [f for f in files_in_contest_folder if 'train.csv' in f][0]\n",
      "        test_file = [f for f in files_in_contest_folder if 'test.csv' in f][0]\n",
      "        train_df = pd.read_csv(os.path.join(root_search_path, contest_folder, train_file))\n",
      "        test_df = pd.read_csv(os.path.join(root_search_path, contest_folder, test_file))\n",
      "else:\n",
      "    # CASE C: Load single csv file as data files\n",
      "    data_file = [f for f in files_in_contest_folder if 'csv' in f][0]\n",
      "    df = pd.read_csv(os.path.join(root_search_path, contest_folder, data_file))\n",
      "\n",
      "# Prepare data\n",
      "df['Title'] = df['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip().title())\n",
      "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
      "df['Cabin'] = df['Cabin'].str[0]\n",
      "df['Embarked'] = pd.get_dummies(df['Embarked'])\n",
      "\n",
      "# Detect label and data\n",
      "target_col = 'Survived'\n",
      "df.rename(columns={target_col: target_col}, inplace=True)\n",
      "\n",
      "# Output\n",
      "train_df.to_csv('processed_train.csv', index=False)\n",
      "test_df.to_csv('processed_test.csv', index=False)\n",
      "\n",
      "print('DATA_PROCESSED: processed_train.csv processed_test.csv')\n",
      "\n",
      "                ```\n",
      "                \n",
      "                # Error from the Previously Written Code\n",
      "                The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 43, in <module>\n",
      "    df['Embarked'] = pd.get_dummies(df['Embarked'])\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/pandas/core/frame.py\", line 3645, in __setitem__\n",
      "    self._set_item_frame_value(key, value)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/pandas/core/frame.py\", line 3775, in _set_item_frame_value\n",
      "    raise ValueError(\"Columns must be same length as key\")\n",
      "ValueError: Columns must be same length as key\n",
      "\n",
      "                \n",
      "\n",
      "                Note that you need to write the python code for the data processing part. If saving model is required, you must save the trained model to \"./agent_workspace/trained_models\" directory.\n",
      "                Start the python code with \"```python\". Please ensure the completeness of the code so that it can be run without additional modifications.\n",
      "                If there is any error from the previous attempt, please carefully fix it first.\n",
      "operation I got this error (itr #3): The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 43, in <module>\n",
      "    df['Embarked'] = pd.get_dummies(df['Embarked'], drop_first=True)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/pandas/core/frame.py\", line 3645, in __setitem__\n",
      "    self._set_item_frame_value(key, value)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/pandas/core/frame.py\", line 3775, in _set_item_frame_value\n",
      "    raise ValueError(\"Columns must be same length as key\")\n",
      "ValueError: Columns must be same length as key\n",
      "\n",
      "____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "PROMPT GEN CODE \n",
      "\n",
      "Carefully read the following instructions to write Python code for Predict survival (0/1) task.\n",
      "                You are an Autonomous Data Engineer. Contest: 'Titanic'.\n",
      "ROOT SEARCH PATH: '/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets'.\n",
      "DATA DESCRIPTION:The dataset contains the following columns:        - PassengerId: Unique ID (Drop this for training)       - Survived: Target variable (0 = No, 1 = Yes)- Pclass: Ticket class (1, 2, 3). Treat as Ordinal.- Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')- Sex: 'male'/'female'. (Map to 0/1)- Age: Numeric. Contains missing values (Fill with Median by Title).- Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')- Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode)..\n",
      "TASK: Write a Python script to Find, Inspect, and Prepare data.\n",
      "LOGIC FLOW:\n",
      "1. SEARCH: Find the subfolder inside ROOT SEARCH PATH that matches the Contest Name.\n",
      "2. INSPECT: List all files in that folder.\n",
      "3. DECIDE STRATEGY:\n",
      "   - CASE A: If you find separate 'train' and 'test' files -> Load them directly.\n",
      "   - CASE B: If you find 'train' and 'test' and 'val' -> Merge 'val' into 'train' or ignore it, then Load train/test.\n",
      "   - CASE C: check in folder, if only one file csv => load this as data files.\n",
      "4. PREPARE: Handle missing values and Encode categoricals (OneHot/Label).\n",
      "5. DETECT: Check label and data, label can based on data_description, rename this col into 'target' 6. OUTPUT: Save 'processed_train.csv' and 'processed_test.csv' to the current directory.\n",
      "7. PRINT: 'DATA_PROCESSED: processed_train.csv processed_test.csv' at the end.\n",
      "OUTPUT: ONLY valid Python code.\n",
      "\n",
      "                \n",
      "                # Previously Written Code\n",
      "                ```python\n",
      "                \n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "# Set the root search path\n",
      "root_search_path = '/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets'\n",
      "\n",
      "# Find the subfolder inside ROOT SEARCH PATH that matches the Contest Name\n",
      "contest_name = 'Titanic'\n",
      "contest_folder = [f for f in os.listdir(root_search_path) if contest_name in f][0]\n",
      "\n",
      "# List all files in that folder\n",
      "files_in_contest_folder = os.listdir(os.path.join(root_search_path, contest_folder))\n",
      "\n",
      "# Decide strategy based on file names\n",
      "if any('train.csv' in f and 'test.csv' in f for f in files_in_contest_folder):\n",
      "    # CASE A: Load train and test files directly\n",
      "    train_file = [f for f in files_in_contest_folder if 'train.csv' in f][0]\n",
      "    test_file = [f for f in files_in_contest_folder if 'test.csv' in f][0]\n",
      "    train_df = pd.read_csv(os.path.join(root_search_path, contest_folder, train_file))\n",
      "    test_df = pd.read_csv(os.path.join(root_search_path, contest_folder, test_file))\n",
      "elif any('train.csv' in f and 'val.csv' in f for f in files_in_contest_folder):\n",
      "    # CASE B: Merge val into train or ignore it, then load train/test\n",
      "    if 'val.csv' in [f for f in files_in_contest_folder]:\n",
      "        val_file = [f for f in files_in_contest_folder if 'val.csv' in f][0]\n",
      "        train_df = pd.concat([pd.read_csv(os.path.join(root_search_path, contest_folder, 'train.csv')), \n",
      "                              pd.read_csv(os.path.join(root_search_path, contest_folder, val_file))])\n",
      "        test_df = pd.read_csv(os.path.join(root_search_path, contest_folder, 'test.csv'))\n",
      "    else:\n",
      "        train_file = [f for f in files_in_contest_folder if 'train.csv' in f][0]\n",
      "        test_file = [f for f in files_in_contest_folder if 'test.csv' in f][0]\n",
      "        train_df = pd.read_csv(os.path.join(root_search_path, contest_folder, train_file))\n",
      "        test_df = pd.read_csv(os.path.join(root_search_path, contest_folder, test_file))\n",
      "else:\n",
      "    # CASE C: Load single csv file as data files\n",
      "    data_file = [f for f in files_in_contest_folder if 'csv' in f][0]\n",
      "    df = pd.read_csv(os.path.join(root_search_path, contest_folder, data_file))\n",
      "\n",
      "# Prepare data\n",
      "df['Title'] = df['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip().title())\n",
      "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
      "df['Cabin'] = df['Cabin'].str[0]\n",
      "df['Embarked'] = pd.get_dummies(df['Embarked'], drop_first=True)\n",
      "\n",
      "# Detect label and data\n",
      "target_col = 'Survived'\n",
      "df.rename(columns={target_col: target_col}, inplace=True)\n",
      "\n",
      "# Output\n",
      "train_df.to_csv('processed_train.csv', index=False)\n",
      "test_df.to_csv('processed_test.csv', index=False)\n",
      "\n",
      "print('DATA_PROCESSED: processed_train.csv processed_test.csv')\n",
      "\n",
      "\n",
      "                ```\n",
      "                \n",
      "                # Error from the Previously Written Code\n",
      "                The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 43, in <module>\n",
      "    df['Embarked'] = pd.get_dummies(df['Embarked'], drop_first=True)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/pandas/core/frame.py\", line 3645, in __setitem__\n",
      "    self._set_item_frame_value(key, value)\n",
      "  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/pandas/core/frame.py\", line 3775, in _set_item_frame_value\n",
      "    raise ValueError(\"Columns must be same length as key\")\n",
      "ValueError: Columns must be same length as key\n",
      "\n",
      "                \n",
      "\n",
      "                Note that you need to write the python code for the data processing part. If saving model is required, you must save the trained model to \"./agent_workspace/trained_models\" directory.\n",
      "                Start the python code with \"```python\". Please ensure the completeness of the code so that it can be run without additional modifications.\n",
      "                If there is any error from the previous attempt, please carefully fix it first.\n",
      "operation I got this error (itr #4): The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 52, in <module>\n",
      "    train_df.to_csv('processed_train.csv', index=False)\n",
      "NameError: name 'train_df' is not defined\n",
      "\n",
      "operation I executed the given plan and got the follow results:\n",
      "\n",
      "The script has been executed. Here is the output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 52, in <module>\n",
      "    train_df.to_csv('processed_train.csv', index=False)\n",
      "NameError: name 'train_df' is not defined\n",
      "\n",
      "system /0_p1_full, <<< END CODING, TIME USED: 378.1722490787506 SECS >>>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rcode': 1,\n",
       " 'action_result': 'The script has been executed. Here is the output:\\nTraceback (most recent call last):\\n  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 52, in <module>\\n    train_df.to_csv(\\'processed_train.csv\\', index=False)\\nNameError: name \\'train_df\\' is not defined\\n',\n",
       " 'code': \"\\nimport os\\nimport pandas as pd\\n\\n# Set the root search path\\nroot_search_path = '/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/all_datasets'\\n\\n# Find the subfolder inside ROOT SEARCH PATH that matches the Contest Name\\ncontest_name = 'Titanic'\\ncontest_folder = [f for f in os.listdir(root_search_path) if contest_name in f][0]\\n\\n# List all files in that folder\\nfiles_in_contest_folder = os.listdir(os.path.join(root_search_path, contest_folder))\\n\\n# Decide strategy based on file names\\nif any('train.csv' in f and 'test.csv' in f for f in files_in_contest_folder):\\n    # CASE A: Load train and test files directly\\n    train_file = [f for f in files_in_contest_folder if 'train.csv' in f][0]\\n    test_file = [f for f in files_in_contest_folder if 'test.csv' in f][0]\\n    train_df = pd.read_csv(os.path.join(root_search_path, contest_folder, train_file))\\n    test_df = pd.read_csv(os.path.join(root_search_path, contest_folder, test_file))\\nelif any('train.csv' in f and 'val.csv' in f for f in files_in_contest_folder):\\n    # CASE B: Merge val into train or ignore it, then load train/test\\n    if 'val.csv' in [f for f in files_in_contest_folder]:\\n        val_file = [f for f in files_in_contest_folder if 'val.csv' in f][0]\\n        train_df = pd.concat([pd.read_csv(os.path.join(root_search_path, contest_folder, 'train.csv')), \\n                              pd.read_csv(os.path.join(root_search_path, contest_folder, val_file))])\\n        test_df = pd.read_csv(os.path.join(root_search_path, contest_folder, 'test.csv'))\\n    else:\\n        train_file = [f for f in files_in_contest_folder if 'train.csv' in f][0]\\n        test_file = [f for f in files_in_contest_folder if 'test.csv' in f][0]\\n        train_df = pd.read_csv(os.path.join(root_search_path, contest_folder, train_file))\\n        test_df = pd.read_csv(os.path.join(root_search_path, contest_folder, test_file))\\nelse:\\n    # CASE C: Load single csv file as data files\\n    data_file = [f for f in files_in_contest_folder if 'csv' in f][0]\\n    df = pd.read_csv(os.path.join(root_search_path, contest_folder, data_file))\\n\\n# Prepare data\\ndf['Title'] = df['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip().title())\\ndf['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\\ndf['Cabin'] = df['Cabin'].str[0]\\ndf['Embarked'] = df['Embarked'].fillna('Unknown')\\nembarked_dummies = pd.get_dummies(df['Embarked'], drop_first=True)\\ndf = pd.concat([df, embarked_dummies], axis=1)\\n\\n# Detect label and data\\ntarget_col = 'Survived'\\ndf.rename(columns={target_col: target_col}, inplace=True)\\n\\n# Output\\ntrain_df.to_csv('processed_train.csv', index=False)\\ntest_df.to_csv('processed_test.csv', index=False)\\n\\nprint('DATA_PROCESSED: processed_train.csv processed_test.csv')\\n\",\n",
       " 'error_logs': ['The script has been executed. Here is the output:\\nTraceback (most recent call last):\\n  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 42, in <module>\\n    df[\\'Embarked\\'] = pd.get_dummies(df[\\'Embarked\\'])\\n  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/pandas/core/frame.py\", line 3645, in __setitem__\\n    self._set_item_frame_value(key, value)\\n  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/pandas/core/frame.py\", line 3775, in _set_item_frame_value\\n    raise ValueError(\"Columns must be same length as key\")\\nValueError: Columns must be same length as key\\n',\n",
       "  'The script has been executed. Here is the output:\\nTraceback (most recent call last):\\n  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 43, in <module>\\n    df[\\'Embarked\\'] = pd.get_dummies(df[\\'Embarked\\'])\\n  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/pandas/core/frame.py\", line 3645, in __setitem__\\n    self._set_item_frame_value(key, value)\\n  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/pandas/core/frame.py\", line 3775, in _set_item_frame_value\\n    raise ValueError(\"Columns must be same length as key\")\\nValueError: Columns must be same length as key\\n',\n",
       "  'The script has been executed. Here is the output:\\nTraceback (most recent call last):\\n  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 43, in <module>\\n    df[\\'Embarked\\'] = pd.get_dummies(df[\\'Embarked\\'], drop_first=True)\\n  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/pandas/core/frame.py\", line 3645, in __setitem__\\n    self._set_item_frame_value(key, value)\\n  File \"/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/pandas/core/frame.py\", line 3775, in _set_item_frame_value\\n    raise ValueError(\"Columns must be same length as key\")\\nValueError: Columns must be same length as key\\n',\n",
       "  'The script has been executed. Here is the output:\\nTraceback (most recent call last):\\n  File \"/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/./agent_workspace_minions/0_p1_full.py\", line 52, in <module>\\n    train_df.to_csv(\\'processed_train.csv\\', index=False)\\nNameError: name \\'train_df\\' is not defined\\n']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [{}]\n",
    "\n",
    "a_system = (\n",
    "            f\"You are an Autonomous Data Engineer. Contest: 'Titanic'.\\n\"\n",
    "            f\"ROOT SEARCH PATH: '{ALL_DATA_PATH}'.\\n\"\n",
    "            f\"DATA DESCRIPTION:\" \n",
    "            \"The dataset contains the following columns:\"\n",
    "            \"        - PassengerId: Unique ID (Drop this for training)\"\n",
    "            \"       - Survived: Target variable (0 = No, 1 = Yes)\"\n",
    "                    \"- Pclass: Ticket class (1, 2, 3). Treat as Ordinal.\"\n",
    "                    \"- Name: Passenger name. (Extract Title like Mr/Mrs to create new feature 'Title')\"\n",
    "                    \"- Sex: 'male'/'female'. (Map to 0/1)\"\n",
    "                    \"- Age: Numeric. Contains missing values (Fill with Median by Title).\"\n",
    "                    \"- Cabin: Cabin number. Many missing. (Extract first letter as 'Deck', fill missing with 'Unknown')\"\n",
    "                    \"- Embarked: Port of Embarkation (C, Q, S). (One-Hot Encode).\"\n",
    "            \".\\n\"\n",
    "            \n",
    "            \"TASK: Write a Python script to Find, Inspect, and Prepare data.\\n\"\n",
    "            \"LOGIC FLOW:\\n\"\n",
    "            \"1. SEARCH: Find the subfolder inside ROOT SEARCH PATH that matches the Contest Name.\\n\"\n",
    "            \"2. INSPECT: List all files in that folder.\\n\"\n",
    "            \"3. DECIDE STRATEGY:\\n\"\n",
    "            \"   - CASE A: If you find separate 'train' and 'test' files -> Load them directly.\\n\"\n",
    "            \"   - CASE B: If you find 'train' and 'test' and 'val' -> Merge 'val' into 'train' or ignore it, then Load train/test.\\n\"\n",
    "            \"   - CASE C: check in folder, if only one file csv => load this as data files.\\n\"\n",
    "            \"4. PREPARE: Handle missing values and Encode categoricals (OneHot/Label).\\n\"\n",
    "            \"5. DETECT: Check label and data, label can based on data_description, rename this col into 'target' \"\n",
    "            \"6. OUTPUT: Save 'processed_train.csv' and 'processed_test.csv' to the current directory.\\n\"\n",
    "            \"7. PRINT: 'DATA_PROCESSED: processed_train.csv processed_test.csv' at the end.\\n\"\n",
    "            \"OUTPUT: ONLY valid Python code.\"\n",
    ")\n",
    "\n",
    "a[0]['DATA'] = a_system\n",
    "a[0][\"pass\"] = True \n",
    "\n",
    "gen_code = manager.generate_code(action_results=a)\n",
    "gen_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad5da33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, result in enumerate(verification_result):\n",
    "            rs[i][\"pass\"] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1b0f4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_type Saved a pass plan: ./plan_path/plan_0.json\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "plan_path = './plan_path'\n",
    "os.makedirs(plan_path, exist_ok=True)\n",
    "for i, action in enumerate(rs):\n",
    "    if action[\"pass\"]:\n",
    "        # save pass plan\n",
    "        filename = f\"{plan_path}/plan_{i}.json\"\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(action, f)\n",
    "            print(\n",
    "                \"agent_type\",\n",
    "                f\"Saved a pass plan: {plan_path}/plan_{i}.json\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5799b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_solution(self, selected_solution):\n",
    "    with open(f\"prompt_pool/{self.task}.py\") as file:\n",
    "        template_code = file.read()        \n",
    "    # code-based execution\n",
    "    self.code_path = f\"/{uid}_p{self.n_plans}_{'full' if self.full_pipeline else ''}\"\n",
    "    ops_llama = OperationAgent(\n",
    "        specs==self.specs,\n",
    "        code_path=self.code_path,\n",
    "        device=self.device,\n",
    "    )\n",
    "    ops_result = ops_llama.implement_solution(\n",
    "        code_instructions=selected_solution, \n",
    "        full_pipeline=self.full_pipeline, \n",
    "        code=template_code\n",
    "    )\n",
    "    self.money['Operation'] = ops_llama.money\n",
    "    return ops_result\n",
    "\n",
    "\n",
    "def generate_code(self):\n",
    "\n",
    "    \"\"\"\n",
    "    Purpose: one\n",
    "    \"\"\"\n",
    "    # Code Execution stage\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data_plan_for_execution = \"\"\n",
    "    model_plan_for_execution = \"\"\n",
    "    for action in self.action_results:\n",
    "        if action[\"pass\"]:\n",
    "            data_plan_for_execution = (\n",
    "                data_plan_for_execution + action[\"data\"] + \"\\n\"\n",
    "            )\n",
    "            # model_plan_for_execution = (\n",
    "            #     model_plan_for_execution + action[\"model\"] + \"\\n\"\n",
    "            # )\n",
    "\n",
    "    # Summarize the passed plan for operation llama to write and execute the code\n",
    "    upload_path = (\n",
    "        f\"This is the retrievable data path: {self.data_path}.\"\n",
    "        if self.data_path\n",
    "        else \"\"\n",
    "    )\n",
    "    summary_prompt = f\"\"\"As the project manager, please carefully read and understand the following instructions suggested by data scientists and machine learning engineers. Then, select the best solution for the given user's requirements.\n",
    "    \n",
    "    - Instructions from Data Scientists\n",
    "    {data_plan_for_execution}\n",
    "    If there is no predefined data split or the data scientists suggest the data split other than train 70%, validation 20%, and test 10%, please use 70%, 20%, and 10% instead for consistency across different tasks. {upload_path}\n",
    "    You should exclude every suggestion related to data visualization as you will be unable to see it.\n",
    "    \n",
    "    # - Instructions from Machine Learning Engineers\n",
    "    # model_plan_for_execution                    \n",
    "    \n",
    "    - User's Requirements\n",
    "    {self.specs['requirements']}\n",
    "    {self.specs['data_description']}\n",
    "    \n",
    "    Note that you must select only ONE promising solution (i.e., one data processing pipeline and one model from the top-{num2words(self.n_candidates)} models) based on the above suggestions.\n",
    "    After choosing the best solution, give detailed instructions and guidelines for MLOps engineers who will write the code based on your instructions. Do not write the code by yourself. Since PyTorch is preferred for implementing deep learning and neural networks models, please guide the MLOPs engineers accordingly.\n",
    "    Make sure your instructions are sufficient with all essential information (e.g., complete path for dataset source and model location) for any MLOps or ML engineers to enable them to write the codes using existing libraries and frameworks correctly.\"\"\"\n",
    "    \n",
    "    self.implementation_result = self.implement_solution(summary_prompt)\n",
    "    print_message('system', f'{self.code_path}, <<< END CODING, TIME USED: {time.time() - init_time} SECS >>>')\n",
    "    self.timer['implementation'] = time.time() - start_time\n",
    "    \n",
    "    self.n_attempts += 1\n",
    "    return self.implementation_result\n",
    "\n",
    "\n",
    "ge_co = manager"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hanoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
